<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Marcin Płodzień">

<title>Automatic differentiation – lectures_ml</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Automatic differentiation – lectures_ml">
<meta property="og:description" content="Machine learning course of the master in Quantum Sciences">
<meta property="og:site_name" content="lectures_ml">
<meta name="twitter:title" content="Automatic differentiation – lectures_ml">
<meta name="twitter:description" content="Machine learning course of the master in Quantum Sciences">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">lectures_ml</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/BorjaRequena/Neural-Network-Course"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../course/index.html">Course</a></li><li class="breadcrumb-item"><a href="../../course/deep_learning/neural_networks_from_scratch.html">Fundamentals of deep learning</a></li><li class="breadcrumb-item"><a href="../../course/deep_learning/automatic_differentiation_pytorch.html">Automatic differentiation</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../course/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction to Machine Learning</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Linear models</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/linear_models/linear_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/linear_models/polynomial_fit.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Polynomial fit</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/linear_models/logistic_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Logistic regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/linear_models/perceptron.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Perceptron</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/probabilistic_view.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A probabilistic view on machine learning</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../course/applications/applications-index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Machine learning application overview</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/applications/applications-cv.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Typical tasks in computer vision</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/applications/applications-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Typical tasks in natural language processing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/applications/applications-tabular.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Typical tasks with structured data</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Fundamentals of deep learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/deep_learning/neural_networks_from_scratch.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Neural networks from scratch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/deep_learning/pytorch_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Neural networks with PyTorch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/deep_learning/automatic_differentiation_pytorch.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Automatic differentiation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/deep_learning/multiclass_classification_problem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Multiclass classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/deep_learning/regularization_techniques.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Neural Networks regularization</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/montecarlo_integration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Monte Carlo Integration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/rbm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Restricted Boltzmann Machines</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Generative models</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/generative/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Generative models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/generative/language_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Language models</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">Automatic differentiation for quantum computing</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/quantum/qaoa.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quantum Approximate Optimization Algorithm (QAOA) from scratch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/quantum/vqe.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Variational Quantum Eigensolver (VQE) from scratch</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../homeworks/index_homework.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homework</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../homeworks/regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../homeworks/perceptron.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - Perceptron</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../homeworks/generative.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Generative models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../homeworks/ml_physics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 - ML techniques in physics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../homeworks/paper_report.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Paper report</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false">
 <span class="menu-text">Library</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lib_nbs/data_gen.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Generation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lib_nbs/losses.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Loss functions and gradients</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lib_nbs/optimizers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Optimizers</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#calculating-gradients" id="toc-calculating-gradients" class="nav-link active" data-scroll-target="#calculating-gradients"><span class="header-section-number">1</span> Calculating gradients</a></li>
  <li><a href="#computational-graph" id="toc-computational-graph" class="nav-link" data-scroll-target="#computational-graph"><span class="header-section-number">2</span> Computational graph</a>
  <ul class="collapse">
  <li><a href="#forward-function-evaluation" id="toc-forward-function-evaluation" class="nav-link" data-scroll-target="#forward-function-evaluation"><span class="header-section-number">2.1</span> Forward function evaluation</a></li>
  <li><a href="#calculating-gradients-1" id="toc-calculating-gradients-1" class="nav-link" data-scroll-target="#calculating-gradients-1"><span class="header-section-number">2.2</span> Calculating gradients</a>
  <ul class="collapse">
  <li><a href="#forward-mode-ad" id="toc-forward-mode-ad" class="nav-link" data-scroll-target="#forward-mode-ad"><span class="header-section-number">2.2.1</span> Forward-mode AD</a></li>
  <li><a href="#reverse-mode-backpropagation-ad" id="toc-reverse-mode-backpropagation-ad" class="nav-link" data-scroll-target="#reverse-mode-backpropagation-ad"><span class="header-section-number">2.2.2</span> Reverse-mode (backpropagation) AD</a></li>
  </ul></li>
  <li><a href="#calculating-gradietns-with-pytorch-torch.autograd" id="toc-calculating-gradietns-with-pytorch-torch.autograd" class="nav-link" data-scroll-target="#calculating-gradietns-with-pytorch-torch.autograd"><span class="header-section-number">2.3</span> Calculating gradietns with PyTorch: torch.autograd</a>
  <ul class="collapse">
  <li><a href="#example-1" id="toc-example-1" class="nav-link" data-scroll-target="#example-1"><span class="header-section-number">2.3.1</span> Example 1</a></li>
  <li><a href="#example-2" id="toc-example-2" class="nav-link" data-scroll-target="#example-2"><span class="header-section-number">2.3.2</span> Example 2</a></li>
  <li><a href="#example-3" id="toc-example-3" class="nav-link" data-scroll-target="#example-3"><span class="header-section-number">2.3.3</span> Example 3</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#gradients-in-a-deep-neural-network" id="toc-gradients-in-a-deep-neural-network" class="nav-link" data-scroll-target="#gradients-in-a-deep-neural-network"><span class="header-section-number">3</span> Gradients in a Deep Neural Network</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/BorjaRequena/Neural-Network-Course/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li><li><a href="https://github.com/BorjaRequena/Neural-Network-Course/blob/master/nbs/course/deep_learning/automatic_differentiation_pytorch.ipynb" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../course/index.html">Course</a></li><li class="breadcrumb-item"><a href="../../course/deep_learning/neural_networks_from_scratch.html">Fundamentals of deep learning</a></li><li class="breadcrumb-item"><a href="../../course/deep_learning/automatic_differentiation_pytorch.html">Automatic differentiation</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Automatic differentiation</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Marcin Płodzień </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<p><a href="https://githubtocolab.com/BorjaRequena/Neural-Network-Course/blob/master/nbs/course/deep_learning/automatic_differentiation_pytorch.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab"></a></p>
<section id="calculating-gradients" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Calculating gradients</h1>
<p>We can distinguish three methods for calculating (partial) derivatives of a function <span class="math inline">\(f: \mathbb{R}^m \to \mathbb{R}\)</span> in a computer program.</p>
<ol type="1">
<li>Finite differences method:</li>
</ol>
<p>Simplest approach is to use finite difference method. For a given function <span class="math inline">\(f\)</span>, the components of its gradient</p>
<p><span class="math display">\[\begin{equation}
    \nabla f = \big(\frac{\partial f}{\partial x_1},\cdots,\frac{\partial f}{\partial x_m}\big),
\end{equation}\]</span> can be approximated as <span class="math display">\[\begin{equation}
\frac{\partial f(\vec{x})}{\partial x_i} \sim \frac{ f(\vec{x} + h\vec{e}_i) - f(\vec{x})}{h},
\end{equation}\]</span> where <span class="math inline">\(\vec{e}_i \in \mathbb{R}^m\)</span> is the <span class="math inline">\(i\)</span>-th unit vector and <span class="math inline">\(h\)</span> is small step size. However, aproximating <span class="math inline">\(\nabla f\)</span> requires <span class="math inline">\(\mathcal{O}(m)\)</span> evaluations of f.&nbsp;Additionally round-off errors due to floating-point arithmetic dominate the errors as <span class="math inline">\(h\to 0\)</span>.</p>
<ol start="2" type="1">
<li>Symbolic differentiation</li>
</ol>
<p>Calculating derivatives is done by automated manipulation of mathematical expressions allowing obtain explicit results (with help of computer algebra systems such us Mathematica, Maple, Maxima). For example for two given functions <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(g(x)\)</span> derivative of their product is calculated explicitly following</p>
<p><span class="math display">\[\begin{equation}
\frac{d}{dx}(f(x)g(x)) = \frac{df(x)}{x}g(x) + f(x)\frac{dg(x)}{dx}   
\end{equation}\]</span></p>
<p>The benefit of symbolic expression is that results are interpretable and allow find analytical solutions. However, symbolic derivatives generated through symbolic differentation typically do not allow for efficient calculations of derivative values.</p>
<ol start="3" type="1">
<li>Automatic differentiation allows to obtain exact numerical value of the derivatives without the need of the symbolic expression - this method lies between symbolic differentiation and numerical differentiation.</li>
</ol>
<p>The logic lying behind AD is that all numerical computations are compositions of a finite set of elementary operations for which derivatives are known. By combining the derivatives of the constituent operations through the computational diagram, and applying the chain rule for derivatives, we can obtain exact numerical of the derivative of the overall expression for a given input.</p>
</section>
<section id="computational-graph" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Computational graph</h1>
<p>A computational graph is a graph-based representation of a mathematical computation. It is a way of visually representing the operations performed in a computation, and the dependencies between these operations.</p>
<section id="forward-function-evaluation" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="forward-function-evaluation"><span class="header-section-number">2.1</span> Forward function evaluation</h2>
<p>Let us consider expression <span class="math inline">\(f(x_1,x_2) = \ln x_1 + \cos x_2 - x_1 x_2\)</span>, Function <span class="math inline">\(f\)</span> is a mapping <span class="math inline">\(f: \mathbb{R}^n \to \mathbb{R}^m\)</span> with <span class="math inline">\(n = 2\)</span>, <span class="math inline">\(m=1\)</span>.</p>
<p>Following [1], we introduce the notation for the computational graph as follows:</p>
<ol type="1">
<li><p>Input variables are denoted as <span class="math inline">\(v_{1-i}\)</span>, where <span class="math inline">\(i = 1,\dots,n\)</span>.</p></li>
<li><p>Intermediate variables are denoted as <span class="math inline">\(v_i\)</span>, <span class="math inline">\(i = 1,\dots,l\)</span>.</p></li>
<li><p>Output variables are denoted as <span class="math inline">\(v_{l+i}\)</span>, <span class="math inline">\(i = 1,\dots,m\)</span>.</p></li>
</ol>
<p>The computational graph related to considered function <span class="math inline">\(f(x_1,x_2)\)</span> is</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="automatic_differentiation_pytorch_files/figure-html/445a80ab-1-computational-graph.png" class="img-fluid figure-img"></p>
<figcaption>computational-graph.png</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>Calculate value of <span class="math inline">\(f(x_1,x_2)\)</span> at <span class="math inline">\((x_1, x_2) = (2,1)\)</span> via passing the diagram from left to right: <img src="automatic_differentiation_pytorch_files/figure-html/badb30a9-1-forward-pass.png" class="img-fluid" alt="forward-pass.png"></p>
</div>
</div>
</section>
<section id="calculating-gradients-1" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="calculating-gradients-1"><span class="header-section-number">2.2</span> Calculating gradients</h2>
<p>Automatic differentiation allows us to calculate exact value of the gradient at given point. In our example, we are interested in value of <span class="math inline">\(\frac{\partial f}{\partial x_1}\)</span> at given point <span class="math inline">\((x_1, x_2) = (2,1)\)</span>. This can be obtain in two modes.</p>
<section id="forward-mode-ad" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="forward-mode-ad"><span class="header-section-number">2.2.1</span> Forward-mode AD</h3>
<p>Forward-mode AD is implemented by complementing each intermediate variable <span class="math inline">\(v_i\)</span> with a derivative: <span class="math display">\[\begin{equation}
\dot{v}_i = \frac{\partial v_i}{\partial x_1},
\end{equation}\]</span> and by applying chain rule for differentiation we can obtain desired gradient. Derivativeas are propagated forward in sync with the function evaluation.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>Calculate value of <span class="math inline">\(\frac{\partial f}{\partial x_1} = \dot{v}_5\)</span> at <span class="math inline">\((x_1, x_2) = (2,1)\)</span> through passing the diagram: <img src="automatic_differentiation_pytorch_files/figure-html/7e3ee704-1-forward-mode-AD.png" class="img-fluid" alt="forward-mode-AD.png"></p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Dual numbers and forward-mode AD
</div>
</div>
<div class="callout-body-container callout-body">
<p>In practive, forward-mode is implemented by extending the algebra of real numbers via introducing <span class="math inline">\(\textit{dual}\)</span> numbers, defined as <span class="math display">\[\begin{equation}
\tilde{z}_1 = a_1 +\epsilon b_1,
\end{equation}\]</span> where <span class="math inline">\(a,b \in \mathbb{R}\)</span>, and <span class="math inline">\(\epsilon^2 = 0\)</span>. Next, addition and multiplication of dual numbers is defined as:</p>
<ol type="1">
<li><p>Addition: <span class="math inline">\(z_1 + z_2 = (a_1 + a_2) + \epsilon(b_1 + b_2)\)</span></p></li>
<li><p>Multiplication: <span class="math inline">\(z_1z_2 = a_1a_2 + + a_1b_2\epsilon +b_1a_2\epsilon + b_1b_2\epsilon^2 = a_1a_2 + \epsilon(a_1b_2+a_2b_1)\)</span></p></li>
</ol>
<p>Next, when we consider Taylor series expansion around <span class="math inline">\(\epsilon\)</span>, we have</p>
<p><span class="math display">\[\begin{equation}
f(z) = f(a+\epsilon) = f(a) + f'(a)\epsilon + \frac{1}{2}f''(a)\epsilon^2 + \dots,
\end{equation}\]</span> we see that this simplifies to <span class="math display">\[\begin{equation}
f(a+\epsilon) = f(a) + \epsilon f'(a),
\end{equation}\]</span> which means that operations on dual number <span class="math inline">\(a\)</span> automatically provides numerical value for <span class="math inline">\(f(a)\)</span> and derivative <span class="math inline">\(f'(a)\)</span>.</p>
<p>In numerical implementation, dual numbers are handled by operator overloading where all mathematical operators are working appropriately on the new algebra of dual numbers.</p>
</div>
</div>
</section>
<section id="reverse-mode-backpropagation-ad" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="reverse-mode-backpropagation-ad"><span class="header-section-number">2.2.2</span> Reverse-mode (backpropagation) AD</h3>
<p>In a reverse mode we calculate gradients backwards. Let’s have a look at our computational graph once more time:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="automatic_differentiation_pytorch_files/figure-html/a1f99636-1-computational-graph.png" class="img-fluid figure-img"></p>
<figcaption>computational-graph.png</figcaption>
</figure>
</div>
<p>We are interested in calculating derivative of <span class="math inline">\(y\)</span> with respect to <span class="math inline">\(v_i\)</span>, i.e.&nbsp;$ $. For a computational graph we can write the chain rule as <span class="math display">\[\begin{equation}
\frac{\partial y_j}{\partial v_i} = \frac{\partial y_j}{\partial v_k}\frac{\partial v_k}{\partial v_i},
\end{equation}\]</span> where <span class="math inline">\(v_k\)</span> is a parent of a <span class="math inline">\(v_i\)</span> in a computational graph. When <span class="math inline">\(v_i\)</span> has more than one parent we sum up the chain rule as: <span class="math display">\[\begin{equation}
\frac{\partial y_j}{\partial v_i} = \sum_{p\in parents(i)} \frac{\partial y_j}{\partial v_p}\frac{\partial v_p}{\partial v_i}.
\end{equation}\]</span> In the literature the above expression is called as <span class="math inline">\(\textit{adjont}\)</span> and denoted as <span class="math display">\[\begin{equation}
\bar{v}_i = \frac{\partial y_i}{\partial v_i}.
\end{equation}\]</span></p>
<p>Next, we can rewrite the adjont in term of the adjonts of the parents, i.e. <span class="math display">\[\begin{equation}
\bar{v}_i = \sum_{p\in \text{parents(i)}} \bar{v}_p \frac{\partial v_p}{\partial v_i}
\end{equation}\]</span> which gives us a recursive algorithm node <span class="math inline">\(y\)</span> with setting starting point as <span class="math inline">\(\bar{y} = 1\)</span>.</p>
<p>Let’s write parents of each node in our example: <span class="math display">\[\begin{equation}
\begin{split}
\text{parents}(i=5) &amp;\to  y \\
\text{parents}(i=4) &amp;\to \{v_5\} \\
\text{parents}(i=3) &amp;\to \{v_5\} \\
\text{parents}(i=2) &amp;\to \{v_4\} \\
\text{parents}(i=1) &amp;\to \{v_4\} \\
\text{parents}(i=0) &amp;\to \{v_2, v_3\} \\
\text{parents}(i=-1) &amp;\to \{v_1, v_2\}\\
\end{split}
\end{equation}\]</span></p>
<p>Now we can write adjonts: <span class="math display">\[\begin{equation}
\begin{split}
  \bar{v}_5 &amp; = \bar{y} \\
  \bar{v}_4 &amp; = \bar{v}_5\frac{\partial v_5}{\partial v_4}\\
  \bar{v}_3 &amp; = \bar{v}_5\frac{\partial v_5}{\partial v_3}\\
  \bar{v}_2 &amp; = \bar{v}_4\frac{\partial v_4}{\partial v_2}\\
  \bar{v}_1 &amp; = \bar{v}_4\frac{\partial v_4}{\partial v_1}\\
  \bar{v}_0 &amp; = \bar{v}_2\frac{\partial v_2}{\partial v_0} + \bar{v}_3\frac{\partial v_3}{\partial v_0} \\
  \bar{v}_{-1} &amp; = \bar{v}_1\frac{\partial v_1}{\partial v_{-1}} + \bar{v}_2\frac{\partial v_2}{\partial v_{-1}} \\
\end{split}
\end{equation}\]</span></p>
<p>Finally, we notice that <span class="math display">\[\begin{equation}
\begin{split}
\bar{v}_0    &amp; = \bar{x}_2 = \frac{\partial y}{\partial x_2}\\
\bar{v}_{-1} &amp; = \bar{x}_1 = \frac{\partial y}{\partial x_1}.
\end{split}
\end{equation}\]</span></p>
<p>In other words, with the single backward pass we have both <span class="math inline">\(\frac{\partial y}{\partial x_1}\)</span> and <span class="math inline">\(\frac{\partial y}{\partial x_2}\)</span> (in forward mode we can obtain <span class="math inline">\(\frac{\partial y}{\partial x_1}\)</span> in one pass).</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>Calculate value of <span class="math inline">\(\frac{\partial f}{\partial x_1} = \dot{v}_5\)</span> at <span class="math inline">\((x_1, x_2) = (2,1)\)</span> through passing the diagram: <img src="automatic_differentiation_pytorch_files/figure-html/fef59115-1-backward-pass-AD.png" class="img-fluid" alt="backward-pass-AD.png"></p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
How to choose between forward-mode and reverse-mode?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let’s consider function <span class="math inline">\(f:\mathbb{R}^m \to \mathbb{R}^n\)</span></p>
<ol type="1">
<li><p>If <span class="math inline">\(m \ll n\)</span>, i.e.&nbsp;number of inputs is much smaller than number of outputs, from computational point of view it is more faborable to use forward-mode automatic differentiation.</p></li>
<li><p>If <span class="math inline">\(m \gg n\)</span>, i.e.&nbsp;number of inputs is much larger than number of outputs (and this is the case of neural networks), from computational point of view it is more faborable to use backward-mode automatic differentiation.</p></li>
</ol>
</div>
</div>
</section>
</section>
<section id="calculating-gradietns-with-pytorch-torch.autograd" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="calculating-gradietns-with-pytorch-torch.autograd"><span class="header-section-number">2.3</span> Calculating gradietns with PyTorch: torch.autograd</h2>
<p>torch.autograd is PyTorch’s automatic differentiation engine that helps in neural network training.</p>
<section id="example-1" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="example-1"><span class="header-section-number">2.3.1</span> Example 1</h3>
<p>To compute the gradient of a scalar function <span class="math inline">\(f\)</span> with respect to a single variable <span class="math inline">\(x\)</span>, we can use PyTorch’s autograd module. For example:</p>
<div id="22995754" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a tensor with requires_grad set to True</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([<span class="fl">4.0</span>], requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a scalar function f</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x):</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the gradient of f with respect to x</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> f(x)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>y.backward()</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># The gradient of f with respect to x is stored in x.grad</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x.grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([8.])</code></pre>
</div>
</div>
</section>
<section id="example-2" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="example-2"><span class="header-section-number">2.3.2</span> Example 2</h3>
<p>To compute the gradient of a function with respect to multiple variables, we can pass a tensor with requires_grad set to True to the function and then use the backward method on the resulting tensor.</p>
<div id="47a310e7" class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create tensors with requires_grad set to True</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([<span class="fl">1.0</span>], requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.tensor([<span class="fl">1.0</span>], requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> torch.tensor([<span class="fl">1.0</span>], requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function that takes two variables as input and returns their sum</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x, y, z):</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.log(torch.sin(x) <span class="op">+</span> torch.tanh(y<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>z</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the gradient of f with respect to x and y</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> f(x, y, z)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>g.backward()</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co"># The gradients of f with respect to x and y are stored in x.grad and y.grad</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x.grad)   </span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y.grad)   </span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(z.grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([0.3370])
tensor([0.5240])
tensor([-0.4719])</code></pre>
</div>
</div>
</section>
<section id="example-3" class="level3" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="example-3"><span class="header-section-number">2.3.3</span> Example 3</h3>
<p>Automatic differentiation can be used in more complicated problems.</p>
<p>Let us consider eigenproblem for the double well potential modeled as a quantum harmonic oscillator with the barrier modeled as a gaussian profile:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
\hat{H}\psi_n(x) &amp; = E_n(x)\psi_n(x) \\
\hat{H} &amp; = -\frac{1}{2}\frac{\partial^2}{\partial x^2} + \frac{1}{2}x^2 + \kappa e^{-x^2/2}
\end{split}
\end{equation}\]</span></p>
<p>We are interested in change of the ground state energy <span class="math inline">\(E_{0}\)</span> as a function of <span class="math inline">\(\kappa\)</span> parameter. To tackle this problem, we have to first calculate eigenstates of the considered Hamiltonian.</p>
<p>Let us consider equaly distributed set of points <span class="math inline">\(x_i\)</span> lying in interval <span class="math inline">\([-L/2, L/2]\)</span> with <span class="math inline">\(\Delta x = \frac{L}{N_x}\)</span>, where <span class="math inline">\(N_x\)</span> is a number of discretization points.</p>
<p>First, we have to, i.e.&nbsp;solve the eigenproblem of the form <span class="math display">\[\begin{equation}
\bar{H}\vec{\psi}_n = E_n \vec{\psi}_n,
\end{equation}\]</span> where <span class="math inline">\(\bar{H}\)</span> is <span class="math inline">\(N_x\times N_x\)</span> matrix representation of Hamiltonian <span class="math inline">\(\hat{H}\)</span> in a discretized space. After constructing the matrix <span class="math inline">\(\bar{H}\)</span>, we can diagonalize it, and find eigenvectors <span class="math inline">\(\vec{\psi}_n\)</span>, and corresponding eigenvalues <span class="math inline">\(E_n\)</span>. Note, <span class="math inline">\(n\in \{0,N_x\}\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Discrete second-order derivative and Hamiltonian matrix representation
</div>
</div>
<div class="callout-body-container callout-body">
<p>To construct the matrix representation of <span class="math inline">\(\hat{H}\)</span>, we have to implement discrete second order derivative: <span class="math display">\[\begin{equation}
\frac{\partial^2 \psi(x)}{\partial x^2}\big|_{x} \approx \frac{\psi_{i+1} - 2\psi_i + \psi_{i-1}}{\Delta x^2}
\end{equation}\]</span></p>
<p>The matrix representation of the kinetic part of the Hamiltonian <span class="math inline">\(\hat{H}\)</span> is</p>
<p><span class="math display">\[\begin{equation}
\bar{H}_{\text{T}} = -\frac{1}{2}\frac{1}{\Delta x}\begin{bmatrix}
-2 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; . \\
1 &amp; -2 &amp; 1 &amp; 0 &amp; 0 &amp; . \\
0 &amp; 1 &amp; -2 &amp; 1 &amp; 0 &amp; . \\
0 &amp; 0 &amp; 1 &amp; -2 &amp; 1 &amp; . \\
0 &amp; 0 &amp; 0 &amp; 1 &amp; -2 &amp; 1 \\
. &amp; . &amp; . &amp; . &amp; . &amp; .
\end{bmatrix}  
\end{equation}\]</span></p>
<p>The matrix representation of the potential part of the Hamiltonian <span class="math inline">\(\hat{H}\)</span> is a diagonal matrix with elements <span class="math inline">\([\bar{H}_\text{V}]_{i,j} = \delta_{i,j} \big(\frac{1}{2}x_i^2 + \kappa e^{-x_i^2/2} \big)\)</span></p>
<p>Now, the Hamiltonian matrix representation is simply <span class="math inline">\(\bar{H} = \bar{H}_T + \bar{H}_V\)</span>.</p>
</div>
</div>
<p>Let’s import necessary libraries</p>
<div id="322d70e2" class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.linalg <span class="im">import</span> eigh</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s define function returning matrix of the considered Hamiltonian. First, let’s define system size <span class="math inline">\(L = 10\)</span>, <span class="math inline">\(N_x = 500\)</span> discretization points, function returning external potential and function returning matrix representation of the Hamiltonian:</p>
<div id="c999431e" class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>Nx <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.linspace(<span class="op">-</span>L<span class="op">/</span><span class="dv">2</span>,L<span class="op">/</span><span class="dv">2</span>,Nx)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>dx <span class="op">=</span> x[<span class="dv">1</span>]<span class="op">-</span>x[<span class="dv">0</span>]</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_potential(x,kappa):</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span><span class="op">*</span>x<span class="op">**</span>(<span class="fl">2.0</span>) <span class="op">+</span> kappa<span class="op">*</span>np.exp(<span class="op">-</span>(x)<span class="op">**</span><span class="dv">2</span><span class="op">/</span><span class="dv">2</span>) </span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_H(kappa):</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    H_T <span class="op">=</span> torch.zeros((Nx,Nx))</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    ones <span class="op">=</span> torch.ones(Nx)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    H_T <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span><span class="op">/</span>dx<span class="op">**</span><span class="dv">2</span><span class="op">*</span>( torch.diag_embed(ones[:<span class="op">-</span><span class="dv">1</span>], offset <span class="op">=</span> <span class="dv">1</span>) <span class="op">-</span> <span class="dv">2</span><span class="op">*</span>torch.diag_embed(ones, offset <span class="op">=</span> <span class="dv">0</span>) <span class="op">+</span> torch.diag_embed(ones[:<span class="op">-</span><span class="dv">1</span>], offset <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>))  </span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    H_V <span class="op">=</span> torch.diag(get_potential(x,kappa))</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    H <span class="op">=</span> H_T <span class="op">+</span> H_V</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> H</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s have a look at potential shape for different parameters <span class="math inline">\(\kappa\)</span>:</p>
<div id="2fde18f4" class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>kappa_vec <span class="op">=</span> torch.tensor([<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">10</span>])</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>N_kappa <span class="op">=</span> kappa_vec.size(<span class="dv">0</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, N_kappa, figsize<span class="op">=</span>(<span class="dv">16</span>,<span class="dv">4</span>))</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>FontSize <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> kappa_i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, N_kappa):</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    kappa <span class="op">=</span> kappa_vec[kappa_i]</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> get_potential(x,kappa)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    ax[kappa_i].plot(x,V.detach().numpy())</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    ax[kappa_i].set_title(<span class="vs">r"$\kappa = $ "</span> <span class="op">+</span> <span class="st">"</span><span class="sc">{:2.2f}</span><span class="st">"</span>.<span class="bu">format</span>(kappa.item()))</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    ax[kappa_i].set_xlabel(<span class="st">"x"</span>,fontsize<span class="op">=</span>FontSize)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="vs">r"$V(x)$"</span>,fontsize<span class="op">=</span>FontSize)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="automatic_differentiation_pytorch_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Pytorch function torch.eigh calculates eigenfunctions and eigenvalues of a given matrix: this allows us to write two functions returning ground state energy <span class="math inline">\(E_{\text{GS}}\)</span>, and ground state gap <span class="math inline">\(\Delta E\)</span>, i.e.&nbsp;energy difference between two first eigenenergies:</p>
<div id="e0973ff5" class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_energy_ground_state(kappa):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    H <span class="op">=</span> get_H(kappa)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    Energies, Vectors <span class="op">=</span> eigh(H)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    E_GS <span class="op">=</span> Energies[<span class="dv">0</span>]    </span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> E_GS</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_gap(kappa):</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    H <span class="op">=</span> get_H(kappa)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    Energies, Vectors <span class="op">=</span> eigh(H)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    gap <span class="op">=</span> Energies[<span class="dv">1</span>] <span class="op">-</span> Energies[<span class="dv">0</span>]    </span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> gap</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s see density of few first eigenstates of the Hamiltonian for given parameter <span class="math inline">\(\kappa\)</span>:</p>
<div id="54a846d7" class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">3</span>, N_kappa, figsize<span class="op">=</span>(<span class="dv">16</span>,<span class="dv">12</span>))</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>FontSize <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> kappa_i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, N_kappa):</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    kappa <span class="op">=</span> kappa_vec[kappa_i]</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> get_potential(x,kappa)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>,kappa_i].plot(x,V.detach().numpy())</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>,kappa_i].set_title(<span class="vs">r"$\kappa = $ "</span> <span class="op">+</span> <span class="st">"</span><span class="sc">{:2.2f}</span><span class="st">"</span>.<span class="bu">format</span>(kappa.item()),fontsize<span class="op">=</span>FontSize)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>,kappa_i].set_xlabel(<span class="st">"x"</span>,fontsize<span class="op">=</span>FontSize)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    H <span class="op">=</span> get_H(kappa)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    Energies, Vectors <span class="op">=</span> eigh(H)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    rho <span class="op">=</span> torch.<span class="bu">abs</span>(Vectors)<span class="op">**</span><span class="dv">2</span><span class="op">/</span>dx</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    n_max <span class="op">=</span>  <span class="dv">4</span>            <span class="co"># maximal number of eigenstates</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    E_max <span class="op">=</span> Energies[n_max] <span class="co">#</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,n_max):</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>        ax[<span class="dv">1</span>,kappa_i].plot(x,rho[:,i].detach().numpy())</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">2</span>,kappa_i].plot(Energies[<span class="dv">0</span>:n_max].detach().numpy(),<span class="st">'x'</span>)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>,kappa_i].set_xlabel(<span class="st">"-0"</span>,fontsize<span class="op">=</span>FontSize)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">2</span>,kappa_i].set_xlabel(<span class="vs">r"n"</span>,fontsize<span class="op">=</span>FontSize)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">2</span>,kappa_i].set_yticks(np.arange(<span class="dv">0</span>,E_max,<span class="fl">0.5</span>))</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">0</span>].set_ylabel(<span class="vs">r"$V(x)$"</span>,fontsize<span class="op">=</span>FontSize) </span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">0</span>].set_ylabel(<span class="vs">r"$|\psi_n(x)|^2$"</span>,fontsize<span class="op">=</span>FontSize) </span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>,<span class="dv">0</span>].set_ylabel(<span class="vs">r"$E_n$"</span>,fontsize<span class="op">=</span>FontSize) </span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="automatic_differentiation_pytorch_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Now, let’s check how ground state energy changes with <span class="math inline">\(\kappa\)</span>:</p>
<div id="19b0cb48" class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>E_gs_vs_kappa <span class="op">=</span> []</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>gap_vs_kappa <span class="op">=</span> []</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>kappa_max <span class="op">=</span> <span class="dv">11</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>kappa_vec <span class="op">=</span> np.linspace(<span class="dv">0</span>,kappa_max,<span class="dv">100</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> kappa <span class="kw">in</span> kappa_vec:</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    E_gs_vs_kappa.append([get_energy_ground_state(kappa)])</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    gap_vs_kappa.append([get_gap(kappa)])</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>FontSize <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>,figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">4</span>))</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(kappa_vec, E_gs_vs_kappa, <span class="st">'--'</span>)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(kappa_vec, gap_vs_kappa, <span class="st">'--'</span>)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="vs">r"$\kappa$"</span>,fontsize<span class="op">=</span>FontSize)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="vs">r"ground state energy $E_</span><span class="sc">{GS}</span><span class="vs">$"</span>,fontsize<span class="op">=</span>FontSize)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="vs">r"$\kappa$"</span>,fontsize<span class="op">=</span>FontSize)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="vs">r"energy gap $\Delta</span><span class="sc">{E}</span><span class="vs">$"</span>, fontsize<span class="op">=</span>FontSize)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Text(0, 0.5, 'energy gap $\\Delta{E}$')</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;Figure size 640x480 with 0 Axes&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="automatic_differentiation_pytorch_files/figure-html/cell-9-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Finally, let’s calculate derivative <span class="math inline">\(\frac{d E_{\text{GS}}(\kappa)}{d \kappa}\)</span> at given <span class="math inline">\(\kappa\)</span> using torch.autograd:</p>
<div id="565a3362" class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>kappa_fixed <span class="op">=</span> torch.tensor(<span class="fl">10.</span>, requires_grad <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>E_GS <span class="op">=</span> get_energy_ground_state(kappa_fixed)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>E_GS.backward()</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(kappa_fixed.grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(0.1249)</code></pre>
</div>
</div>
<p>Let’s plot <span class="math inline">\(\frac{d E_{\text{GS}}(\kappa)}{d \kappa}\)</span>, and <span class="math inline">\(\frac{d \Delta E_{\text{GS}}(\kappa)}{d \kappa}\)</span></p>
<div id="21e65016" class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>dEdkappa <span class="op">=</span> []</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>dgapdkappa <span class="op">=</span> []</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> kappa <span class="kw">in</span> kappa_vec:</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    kappa_fixed_1 <span class="op">=</span> torch.tensor(kappa, requires_grad <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    E_GS <span class="op">=</span> get_energy_ground_state(kappa_fixed_1)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    E_GS.backward()</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    diff <span class="op">=</span> kappa_fixed_1.grad</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    dEdkappa.append([diff.item()])</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    kappa_fixed_2 <span class="op">=</span> torch.tensor(kappa, requires_grad <span class="op">=</span> <span class="va">True</span>)    </span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    gap <span class="op">=</span> get_gap(kappa_fixed_2)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    gap.backward()</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    diff <span class="op">=</span> kappa_fixed_2.grad</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    dgapdkappa.append([diff.item()])    </span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>,figsize<span class="op">=</span>(<span class="dv">14</span>,<span class="dv">4</span>))</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(kappa_vec,dEdkappa,<span class="st">'--'</span>)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(kappa_vec,dgapdkappa,<span class="st">'--'</span>)</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>FontSize<span class="op">=</span><span class="dv">16</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="vs">r"$\kappa$"</span>,fontsize<span class="op">=</span>FontSize)</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="vs">r"$\frac</span><span class="sc">{dE}</span><span class="vs">{d\kappa}$"</span>,fontsize<span class="op">=</span>FontSize)</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="vs">r"$\kappa$"</span>,fontsize<span class="op">=</span>FontSize)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="vs">r"$\frac{d\Delta E}{d\kappa}$"</span>,fontsize<span class="op">=</span>FontSize)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Text(0, 0.5, '$\\frac{d\\Delta E}{d\\kappa}$')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="automatic_differentiation_pytorch_files/figure-html/cell-11-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="gradients-in-a-deep-neural-network" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Gradients in a Deep Neural Network</h1>
<p>We can then express the Neural Network <span class="math inline">\(K\)</span> layers as a many-level function composition</p>
<p><span class="math display">\[\begin{equation}
\vec{y} = (f_K\circ f_{K-1} \circ \dots f_1)(\vec{x}) = f_K(f_{K-1}(\cdots(f_1(\vec{x}))),
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\vec{x}\)</span> are inputs, while <span class="math inline">\(\vec{y}\)</span> are outputs (e.g.&nbsp;predicted labels), and <span class="math inline">\(j = 0,\dots,K\)</span> enumerates layers of the network. The input data flow can be decomposed in the following steps:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
  f_0 &amp; \equiv x \\
  f_1 &amp; = \sigma_1(W_0 f_0 + b_0) \\
  f_2 &amp; = \sigma_2(W_1 f_1 + b_1) \\
  \vdots\\
  f_j &amp; = \sigma_j(W_{i-1} f_{j-1} + b_{j-1})\\
  \vdots\\
  f_K &amp; = \sigma_K(W_{K-1} f_{K-1} + b_{K}).
\end{split}
\end{equation}\]</span></p>
<p>Since we’ll mostly be discussing autograd in the context of training, our output of interest will be the model’s loss function <span class="math inline">\(L\)</span>, which is a single-valued scalar function of the model’s output. This function expresses how far off our model’s prediction was from a particular input’s ideal output.</p>
<p>We try to obtain that by an iterative process of passing the training data, and updating the weights according to gradient of the loss function with respect to the training parameters <span class="math inline">\(\theta = \{W_0, b_0, W_1, b_1, \dots, A_{K-1}, b_k\}\)</span>. The gradient of the loss function with respect to the parameters set <span class="math inline">\(\theta\)</span> requires the partial derivatives with respect to <span class="math inline">\(\theta_j = \{W_j, b_j\}\)</span> of each layer <span class="math inline">\(j = 1,\dots,K\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
  \frac{\partial L}{\partial \theta_{K-1}} &amp; = \frac{\partial L}{\partial f_K}\color{blue}{ \frac{\partial f_K}{\partial \theta_{K-1}}} \\
  \frac{\partial L}{\partial \theta_{K-2}} &amp; = \frac{\partial L}{\partial f_K}\color{red}{ \frac{\partial f_K}{\partial f_{K-1}}}\color{blue}{ \frac{\partial f_{K-1}}{\partial \theta_{K-2}}} \\
  \frac{\partial L}{\partial \theta_{K-3}} &amp; = \frac{\partial L}{\partial f_K}\color{red}{ \frac{\partial f_K}{\partial f_{K-1}}}\color{red}{ \frac{\partial f_{K-1}}{\partial f_{K-2}}}\color{blue}{ \frac{\partial f_{K-2}}{\partial \theta_{K-3}}} \\
  \frac{\partial L}{\partial \theta_{K-4}} &amp; = \frac{\partial L}{\partial f_K}\color{red}{ \frac{\partial f_K}{\partial f_{K-1}}}\color{red}{ \frac{\partial f_{K-1}}{\partial f_{K-2}}}\color{red}{ \frac{\partial f_{K-2}}{\partial f_{K-3}}}\color{blue}{ \frac{\partial f_{K-3}}{\partial \theta_{K-4}}} \\
  \vdots
\end{split}
\end{equation}\]</span></p>
<p>As such, assuming we have already computed <span class="math inline">\(\frac{\partial L}{\partial \theta_{i+1}}\)</span> it can be reused to compute <span class="math inline">\(\frac{\partial L}{\partial \theta_{i}}\)</span> - which is a reverse-mode AD.</p>
<p>Autograd lies at the heart of building machine learning projects. It allows automatic calculation of gradient of the custom loss function for the Neural Network with respect to it’s parameters.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/BorjaRequena\.github\.io\/Neural-Network-Course");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/BorjaRequena/Neural-Network-Course/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li><li><a href="https://github.com/BorjaRequena/Neural-Network-Course/blob/master/nbs/course/deep_learning/automatic_differentiation_pytorch.ipynb" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div></div></footer></body></html>