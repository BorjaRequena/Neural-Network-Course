<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Marcin Płodzień and Borja Requena">

<title>Neural networks from scratch – lectures_ml</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Neural networks from scratch – lectures_ml">
<meta property="og:description" content="Machine learning course of the master in Quantum Sciences">
<meta property="og:site_name" content="lectures_ml">
<meta name="twitter:title" content="Neural networks from scratch – lectures_ml">
<meta name="twitter:description" content="Machine learning course of the master in Quantum Sciences">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">lectures_ml</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/BorjaRequena/Neural-Network-Course"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../course/index.html">Course</a></li><li class="breadcrumb-item"><a href="../../course/deep_learning/neural_networks_from_scratch.html">Fundamentals of deep learning</a></li><li class="breadcrumb-item"><a href="../../course/deep_learning/neural_networks_from_scratch.html">Neural networks from scratch</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../course/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction to Machine Learning</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Linear models</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/linear_models/linear_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/linear_models/polynomial_fit.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Polynomial fit</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/linear_models/logistic_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Logistic regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/linear_models/perceptron.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Perceptron</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/probabilistic_view.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A probabilistic view on machine learning</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../course/applications/applications-index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Machine learning application overview</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/applications/applications-cv.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Typical tasks in computer vision</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/applications/applications-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Typical tasks in natural language processing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/applications/applications-tabular.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Typical tasks with structured data</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Fundamentals of deep learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/deep_learning/neural_networks_from_scratch.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Neural networks from scratch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/deep_learning/pytorch_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Neural networks with PyTorch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/deep_learning/automatic_differentiation_pytorch.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Automatic differentiation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/deep_learning/multiclass_classification_problem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Multiclass classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/deep_learning/regularization_techniques.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Neural Networks regularization</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/montecarlo_integration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Monte Carlo Integration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/rbm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Restricted Boltzmann Machines</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Generative models</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/generative/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Generative models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/generative/language_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Language models</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">Automatic differentiation for quantum computing</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/quantum/qaoa.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quantum Approximate Optimization Algorithm (QAOA) from scratch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/quantum/vqe.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Variational Quantum Eigensolver (VQE) from scratch</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../homeworks/index_homework.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homework</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../homeworks/regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../homeworks/perceptron.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - Perceptron</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../homeworks/generative.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Generative models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../homeworks/ml_physics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 - ML techniques in physics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../homeworks/paper_report.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Paper report</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false">
 <span class="menu-text">Library</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lib_nbs/data_gen.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Generation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lib_nbs/losses.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Loss functions and gradients</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lib_nbs/optimizers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Optimizers</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sigmoid-perceptron" id="toc-sigmoid-perceptron" class="nav-link active" data-scroll-target="#sigmoid-perceptron"><span class="header-section-number">1</span> Sigmoid perceptron</a>
  <ul class="collapse">
  <li><a href="#sigmoid-perceptron-update" id="toc-sigmoid-perceptron-update" class="nav-link" data-scroll-target="#sigmoid-perceptron-update"><span class="header-section-number">1.1</span> Sigmoid perceptron update</a></li>
  </ul></li>
  <li><a href="#single-layer-neural-network" id="toc-single-layer-neural-network" class="nav-link" data-scroll-target="#single-layer-neural-network"><span class="header-section-number">2</span> Single layer Neural Network</a>
  <ul class="collapse">
  <li><a href="#network-definition" id="toc-network-definition" class="nav-link" data-scroll-target="#network-definition"><span class="header-section-number">2.1</span> Network definition</a>
  <ul class="collapse">
  <li><a href="#feed-forward-pass" id="toc-feed-forward-pass" class="nav-link" data-scroll-target="#feed-forward-pass"><span class="header-section-number">2.1.1</span> Feed-forward pass</a></li>
  <li><a href="#parameter-update" id="toc-parameter-update" class="nav-link" data-scroll-target="#parameter-update"><span class="header-section-number">2.1.2</span> Parameter update</a></li>
  </ul></li>
  <li><a href="#example-task-handwritten-digits-with-the-mnist-dataset" id="toc-example-task-handwritten-digits-with-the-mnist-dataset" class="nav-link" data-scroll-target="#example-task-handwritten-digits-with-the-mnist-dataset"><span class="header-section-number">2.2</span> Example task: handwritten digits with the MNIST dataset</a>
  <ul class="collapse">
  <li><a href="#process-the-data" id="toc-process-the-data" class="nav-link" data-scroll-target="#process-the-data"><span class="header-section-number">2.2.1</span> Process the data</a></li>
  <li><a href="#define-the-model" id="toc-define-the-model" class="nav-link" data-scroll-target="#define-the-model"><span class="header-section-number">2.2.2</span> Define the model</a></li>
  <li><a href="#train-the-model" id="toc-train-the-model" class="nav-link" data-scroll-target="#train-the-model"><span class="header-section-number">2.2.3</span> Train the model</a></li>
  </ul></li>
  <li><a href="#activation-functions" id="toc-activation-functions" class="nav-link" data-scroll-target="#activation-functions"><span class="header-section-number">2.3</span> Activation functions</a></li>
  </ul></li>
  <li><a href="#optimization-algorithms" id="toc-optimization-algorithms" class="nav-link" data-scroll-target="#optimization-algorithms"><span class="header-section-number">3</span> Optimization algorithms</a>
  <ul class="collapse">
  <li><a href="#stochastic-gradient-descent---sgd" id="toc-stochastic-gradient-descent---sgd" class="nav-link" data-scroll-target="#stochastic-gradient-descent---sgd"><span class="header-section-number">3.1</span> Stochastic gradient descent - SGD</a></li>
  <li><a href="#momentum" id="toc-momentum" class="nav-link" data-scroll-target="#momentum"><span class="header-section-number">3.2</span> Momentum</a></li>
  <li><a href="#adaptative-gradient---adagrad" id="toc-adaptative-gradient---adagrad" class="nav-link" data-scroll-target="#adaptative-gradient---adagrad"><span class="header-section-number">3.3</span> Adaptative Gradient - Adagrad</a></li>
  <li><a href="#adadelta-extension-of-the-adaptative-gradient" id="toc-adadelta-extension-of-the-adaptative-gradient" class="nav-link" data-scroll-target="#adadelta-extension-of-the-adaptative-gradient"><span class="header-section-number">3.4</span> Adadelta: extension of the Adaptative Gradient</a></li>
  <li><a href="#adaptive-moment-estimation---adam" id="toc-adaptive-moment-estimation---adam" class="nav-link" data-scroll-target="#adaptive-moment-estimation---adam"><span class="header-section-number">3.5</span> Adaptive Moment Estimation - Adam</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">4</span> References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/BorjaRequena/Neural-Network-Course/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li><li><a href="https://github.com/BorjaRequena/Neural-Network-Course/blob/master/nbs/course/deep_learning/neural_networks_from_scratch.ipynb" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../course/index.html">Course</a></li><li class="breadcrumb-item"><a href="../../course/deep_learning/neural_networks_from_scratch.html">Fundamentals of deep learning</a></li><li class="breadcrumb-item"><a href="../../course/deep_learning/neural_networks_from_scratch.html">Neural networks from scratch</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Neural networks from scratch</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Marcin Płodzień and Borja Requena </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<p><a href="https://githubtocolab.com/BorjaRequena/Neural-Network-Course/blob/master/nbs/course/deep_learning/neural_networks_from_scratch.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab"></a></p>
<section id="sigmoid-perceptron" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Sigmoid perceptron</h1>
<p>In this section we will implement a training algorithm for a single perceptron with sigmoid activation function.<img src="neural_networks_from_scratch_files/figure-html/0da02614-ce1e-4991-8a82-278a5a7ad050-1-nn_perceptron.svg" class="img-fluid" alt="nn_perceptron.svg"></p>
<p>We consider a data set of <span class="math inline">\(n\)</span> tuples <span class="math inline">\((x_i, y_i)\)</span> that we denote as <span class="math inline">\(X\in\mathbb{R}^{n\times m}\)</span> and <span class="math inline">\(Y\in\mathbb{R^n}\)</span>. Here, <span class="math inline">\(m\)</span> denotes the number of features in our samples. The perceptron consists on:</p>
<ol type="1">
<li>A linear transformation <span class="math inline">\(f: \mathbb{R}^m \mapsto \mathbb{R}^h\)</span> of the form <span class="math inline">\(z_i = x_i^T W + \mathbf{b}\,\)</span>. Here, <span class="math inline">\(W\in\mathbb{R}^{m\times h}\)</span> are the weights, and <span class="math inline">\(\mathbf{b}\in\mathbb{R}^h\)</span> are the biases.</li>
<li>A nonlinear transformation <span class="math inline">\(\hat{y}_i = \sigma(z_i)\)</span>, in this case: the sigmoid function <span class="math inline">\(\sigma(z_i) = \frac{1}{1 + e^{-z_i}}\)</span>.</li>
</ol>
<section id="sigmoid-perceptron-update" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="sigmoid-perceptron-update"><span class="header-section-number">1.1</span> Sigmoid perceptron update</h2>
<p>The training of a perceptron consists on the iterative update of its parameters, <span class="math inline">\(W\)</span> and <span class="math inline">\(\mathbf{b}\)</span>, in order to minimize the loss function <span class="math inline">\(L\)</span>. Here, we will consider the mean-squared error:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
L &amp; = \frac{1}{N}\sum_{i=1}^n L_i\\
L_i &amp; = \frac{1}{2}(y_i - \hat{y}_i)^2,
\end{split}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\hat{y}_i\)</span> is our prediction and <span class="math inline">\(y_i\)</span> the ground truth.</p>
<p>We update the weights <span class="math inline">\(W\)</span> and biases <span class="math inline">\(\mathbf{b}\)</span> with a gradient descent procedure:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
W &amp; \leftarrow W - \eta \frac{\partial L}{\partial W} = W - \frac{\eta}{n} \sum_{i=1}^N \frac{\partial L_i}{\partial W}\\
\mathbf{b} &amp; \leftarrow \mathbf{b} - \eta \frac{\partial L}{\partial \mathbf{b}} = W - \frac{\eta}{n} \sum_{i=1}^n \frac{\partial L_i}{\partial \mathbf{b}},
\end{split}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\eta\)</span> is the learning rate.</p>
<p>In this case, we can obtain analytical expressions for the gradients:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
\frac{\partial L_i}{\partial W} &amp; = (y_i - \hat{y}_i)\frac{\partial y_i}{\partial W}\\
\frac{\partial L_i}{\partial \mathbf{b}} &amp; = (y_i - \hat{y}_i)\frac{\partial y_i}{\partial \mathbf{b}}\\
y_i &amp; = \sigma(z_i) \\
z_i &amp; = x_i^T W + \mathbf{b}
\end{split}
\end{equation}\]</span></p>
<p>With the chain rule we have:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
\frac{\partial y_i}{\partial W} &amp; = \frac{\partial \sigma(z_i)}{\partial z_i}\frac{\partial z_i}{\partial W} \\
\frac{\partial y_i}{\partial \mathbf{b}} &amp; = \frac{\partial \sigma(z_i)}{\partial z_i}\frac{\partial z_i}{\partial \mathbf{b}}
\end{split}
\end{equation}\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>Show that update rule for weights <span class="math inline">\(W\)</span> and bias <span class="math inline">\(\mathbf{b}\)</span> is:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
W &amp; \leftarrow W - \frac{\eta }{n}\sum_{i=1}^n (y_i - \hat{y}_i) y_i(1-y_i)x_i\\
\mathbf{b} &amp; \leftarrow \mathbf{b} - \frac{\eta }{n}\sum_{i=1}^n (y_i - \hat{y}_i) y_i(1-y_i)
\end{split}
\end{equation}\]</span></p>
</div>
</div>
</section>
</section>
<section id="single-layer-neural-network" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Single layer Neural Network</h1>
<p>Here, we will consider a neural network with a single hidden layer and sigmoid activation functions. <img src="neural_networks_from_scratch_files/figure-html/b5b9d64c-0f76-4ac5-82b2-2e814f3e78bb-1-nn-2.svg" class="img-fluid" alt="nn-2.svg"></p>
<section id="network-definition" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="network-definition"><span class="header-section-number">2.1</span> Network definition</h2>
<p>In a neural network with a single hidden layer, we have two perceptrons: one between the input and the hidden layer, and one between the hidden layer and the output.</p>
<p>The input layer has the same size as the number of features in our data, i.e., <span class="math inline">\(m\)</span> neurons. Then, the hidden layer has <span class="math inline">\(h\)</span> neurons, and the output layer has as many neurons as classes <span class="math inline">\(c\)</span>. In a regression task, <span class="math inline">\(c=1\)</span> as we predict a single scalar. Thus, the first weigth matrix <span class="math inline">\(W_1\)</span> has shape <span class="math inline">\(m\times h\)</span> and , and the second weight matrix <span class="math inline">\(W_2\)</span> has shape <span class="math inline">\(h\times c\)</span>. In this case, we only consider biases in the hidden layer <span class="math inline">\(\mathbf{b}_1\)</span> which is a vector with <span class="math inline">\(h\)</span> entries.</p>
<section id="feed-forward-pass" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="feed-forward-pass"><span class="header-section-number">2.1.1</span> Feed-forward pass</h3>
<p>Now, let’s go through the feed-forward pass of the training data <span class="math inline">\(X\in\mathbb{R}^{n\times m}\)</span> through our network.</p>
<ol type="1">
<li>The input goes through the first linear layer <span class="math inline">\(\mathbf{h} \leftarrow X^TW_1 + \mathbf{b}_1\)</span> with shapes <span class="math inline">\([n, m] \times [m, h] = [n, h]\)</span></li>
<li>Then, we apply the activation function <span class="math inline">\(\hat{\mathbf{h}} \leftarrow \sigma(\mathbf{h})\)</span> with shape <span class="math inline">\([n, h]\)</span></li>
<li>Then, we apply the second linear layer <span class="math inline">\(\mathbf{g} \leftarrow \mathbf{h}^TW_{2}\)</span> with shapes <span class="math inline">\([n, h] \times [h, c] = [n, c]\)</span></li>
<li>Finally, we apply the activation function <span class="math inline">\(\hat{\mathbf{y}} \leftarrow \sigma(\mathbf{g})\)</span> with shape <span class="math inline">\([n, c]\)</span></li>
</ol>
</section>
<section id="parameter-update" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="parameter-update"><span class="header-section-number">2.1.2</span> Parameter update</h3>
<p>We will use the MSE loss function denoted in matrix rerpesentation as <span class="math inline">\(L = \frac{1}{2n}||Y - \hat{Y}||^2\)</span>,</p>
<p>The parameter update rule is <span class="math display">\[\begin{equation}
\begin{split}
  W_1 &amp; = W_1 - \frac{\eta}{n}\frac{\partial L}{\partial W_1} \\
  \mathbf{b}_1 &amp; = \mathbf{b}_1 - \frac{\eta}{n}\frac{\partial L}{\partial \mathbf{b}_1} \\
  W_2 &amp; = W_2 - \frac{\eta}{n}\frac{\partial L}{\partial W_2}.
\end{split}
\end{equation}\]</span></p>
<p>Let us calculate gradients of the loss function with respect to <span class="math inline">\(W_1\)</span>, <span class="math inline">\(\mathbf{b}_1\)</span> and <span class="math inline">\(W_2\)</span> using the chain rule:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
\frac{\partial L}{\partial W_{2}} &amp; = \frac{\partial L}{\partial \hat{Y}}\frac{\partial \hat{Y}}{\partial \mathbf{g}}\frac{\partial \mathbf{g}}{\partial W_{2}} \\
\frac{\partial L}{\partial W_{1}} &amp; = \frac{\partial L}{\partial \hat{Y}}\frac{\partial \hat{Y}}{\partial \mathbf{g}}\frac{\partial \mathbf{g}}{\partial \hat{\mathbf{h}}}\frac{\partial \hat{\mathbf{h}}}{\partial \mathbf{h}}\frac{\partial \mathbf{h}}{\partial W_{1}}\\
\frac{\partial L}{\partial \mathbf{b}_{1}} &amp; = \frac{\partial L}{\partial \hat{Y}}\frac{\partial \hat{Y}}{\partial \mathbf{g}}\frac{\partial \mathbf{g}}{\partial \hat{\mathbf{h}}}\frac{\partial \hat{\mathbf{h}}}{\partial \mathbf{h}}\frac{\partial \mathbf{h}}{\partial \mathbf{b}_{1}}
\end{split}
\end{equation}\]</span></p>
<p>We can write down every term:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
\frac{\partial L}{\partial \hat{Y}} &amp; = \hat{Y} - Y \\
\frac{\partial \hat{Y}}{\partial \mathbf{g}} &amp; = \hat{Y}(1-\hat{Y}) \\
\frac{\partial \mathbf{g}}{\partial W_{2}} &amp; = \hat{\mathbf{h}} \\
\frac{\partial \mathbf{g}}{\partial \hat{\mathbf{h}}} &amp; = W_{2} \\
\frac{\partial \hat{\mathbf{h}}}{\partial \mathbf{h}} &amp; = \hat{\mathbf{h}}(1-\hat{\mathbf{h}}) \\
\frac{\partial \mathbf{h}}{\partial W_1} &amp; = X\\
\frac{\partial \mathbf{h}}{\partial \mathbf{b}_1} &amp; = \mathbb{1}
\end{split}
\end{equation}\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>Show explicitly that <span class="math inline">\(Q_1\)</span>, and <span class="math inline">\(Q_2\)</span> read: <span class="math display">\[\begin{equation}
\begin{split}
Q_2 &amp; \equiv \frac{\partial L}{\partial \hat{Y}}\frac{\partial \hat{Y}}{\partial \mathbf{g}} = (\hat{Y}-Y)\hat{Y}(1-\hat{Y}) \\
Q_1 &amp; \equiv \frac{\partial L}{\partial \hat{Y}}\frac{\partial \hat{Y}}{\partial \mathbf{g}}\frac{\partial \mathbf{g}}{\partial \hat{\mathbf{h}}}\frac{\partial \hat{\mathbf{h}}}{\partial \mathbf{h}} = Q_2 W_{2}\hat{\mathbf{h}}(1-\hat{\mathbf{h}})
\end{split}
\end{equation}\]</span></p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>Show that update rules for weights <span class="math inline">\(W_1\)</span> and <span class="math inline">\(W_2\)</span> are</p>
<p><span class="math inline">\(W_2 = W_2 - \frac{\eta}{n}\hat{\mathbf{h}}^TQ_2\)</span></p>
<p><span class="math inline">\(B_1 = B_1 - \frac{\eta}{n}Q_1\)</span></p>
<p><span class="math inline">\(W_1 = W_1 - \frac{\eta}{n}X^TQ_1\)</span></p>
<p>Hint 1: Operations in <span class="math inline">\((\hat{Y}-Y)Y(1-Y)\)</span> are element-wise multiplications.</p>
<p>Hint 2: Operations in <span class="math inline">\(\hat{\mathbf{h}}(1-\hat{\mathbf{h}})\)</span> are element-wise multiplications.</p>
<p>Hint 3: The resulting weight updates must have the same dimension as the weight matrices.</p>
</div>
</div>
</section>
</section>
<section id="example-task-handwritten-digits-with-the-mnist-dataset" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="example-task-handwritten-digits-with-the-mnist-dataset"><span class="header-section-number">2.2</span> Example task: handwritten digits with the MNIST dataset</h2>
<p>We test the concepts introduced above using the MNIST dataset. MNIST stands for Modified National Institute of Standards and Technology and the dataset consists of <span class="math inline">\(28\times28\)</span> images of handwritten digits. Here, we will perform a regression task trying to predict the value of the digit from the image.</p>
<p>We will use the following architecture:</p>
<ol type="1">
<li>Input layer with <span class="math inline">\(m = 28\times28 = 784\)</span> neurons.</li>
<li>Hidden layer with <span class="math inline">\(h = 25\)</span> neurons.</li>
<li>Ouptut layer with <span class="math inline">\(c = 1\)</span> neuron.</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="neural_networks_from_scratch_files/figure-html/296fcab7-49eb-49be-99d5-90cdb9cf6b42-1-nn_MNIST.svg" class="img-fluid figure-img"></p>
<figcaption>nn_MNIST.svg</figcaption>
</figure>
</div>
<section id="process-the-data" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="process-the-data"><span class="header-section-number">2.2.1</span> Process the data</h3>
<p>We start by importing the MNIST dataset</p>
<div id="25967e8c" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>(x_train, y_train), (x_test, y_test) <span class="op">=</span> tf.keras.datasets.mnist.load_data()</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># split train and validation</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">7</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>idx_rnd <span class="op">=</span> np.random.permutation(np.arange(x_train.shape[<span class="dv">0</span>]))</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>split <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.2</span><span class="op">*</span>x_train.shape[<span class="dv">0</span>]) <span class="co"># Take 20% for validation</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>idx_val, idx_train <span class="op">=</span> idx_rnd[:split], idx_rnd[split:]</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>x_val, y_val <span class="op">=</span> x_train[idx_val], y_train[idx_val]</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>x_train, y_train <span class="op">=</span> x_train[idx_train], y_train[idx_train]</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>n_train, n_val, n_test <span class="op">=</span> x_train.shape[<span class="dv">0</span>], x_val.shape[<span class="dv">0</span>], x_test.shape[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11493376/11490434 [==============================] - 1s 0us/step</code></pre>
</div>
</div>
<p>Let’s have a look at some examples to get a better idea about the task.</p>
<div id="a7d6c081" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="dv">10</span>):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> np.where(y_train <span class="op">==</span> i)[<span class="dv">0</span>] <span class="co"># find indices of i-digit</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, k, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, k):</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        ax[j].imshow(x_train[idx[j]])</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="neural_networks_from_scratch_files/figure-html/cell-3-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="neural_networks_from_scratch_files/figure-html/cell-3-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="neural_networks_from_scratch_files/figure-html/cell-3-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="neural_networks_from_scratch_files/figure-html/cell-3-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="neural_networks_from_scratch_files/figure-html/cell-3-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="neural_networks_from_scratch_files/figure-html/cell-3-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="neural_networks_from_scratch_files/figure-html/cell-3-output-7.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="neural_networks_from_scratch_files/figure-html/cell-3-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="neural_networks_from_scratch_files/figure-html/cell-3-output-9.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="neural_networks_from_scratch_files/figure-html/cell-3-output-10.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Before putting our images through the model, we first need to pre-process the data. We will:</p>
<ol type="1">
<li>Flatten the images</li>
<li>Normalize them <span class="math inline">\(\mathbf{x}_i \to \frac{\mathbf{x}_i - \text{mean}(\mathbf{x}_i)}{\text{std}(\mathbf{x})}\)</span></li>
<li>Because output of our network comes from a simgoid activation function in the range <span class="math inline">\((0, 1)\)</span>, we will bring the image labels <span class="math inline">\(y \in \{0,1,2,\dots,9\}\)</span> to the <span class="math inline">\((0,1)\)</span> range dividing by 10.</li>
</ol>
<div id="35cd5068" class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten the images</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> x_train.reshape(n_train, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>X_val <span class="op">=</span> x_val.reshape(n_val, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> x_test.reshape(n_test, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalize the data</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> normalize(x):</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (x <span class="op">-</span> np.mean(x, axis<span class="op">=</span><span class="dv">1</span>)[:, <span class="va">None</span>])<span class="op">/</span>np.std(x, axis<span class="op">=</span><span class="dv">1</span>)[:, <span class="va">None</span>]</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> normalize(X_train)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>X_val <span class="op">=</span> normalize(X_val)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> normalize(X_test)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Bring the targets in range</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>Y_train <span class="op">=</span> y_train<span class="op">/</span><span class="fl">10.0</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>Y_val <span class="op">=</span> y_val<span class="op">/</span><span class="fl">10.0</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>Y_test <span class="op">=</span> y_test<span class="op">/</span><span class="fl">10.0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="define-the-model" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="define-the-model"><span class="header-section-number">2.2.2</span> Define the model</h3>
<p>Let’s define now the neural network parameters</p>
<div id="2923149f" class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># NN parameters</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> X_train.shape[<span class="dv">1</span>] <span class="co"># number of input neurons</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> <span class="dv">21</span>               <span class="co"># number of hidden neurons</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> <span class="dv">1</span>                <span class="co"># number of output neurons</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And define a forward and a backward function to compute the output and the gradients.</p>
<div id="f71f4da5-4c19-4b54-ab8d-9603cb2798d7" class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(x, w_1, b_1, w_2):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Forward pass through our neural network."</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> x <span class="op">@</span> w_1 <span class="op">+</span> b_1</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    h_hat <span class="op">=</span> sigmoid(h)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    g <span class="op">=</span> h_hat <span class="op">@</span> w_2</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sigmoid(g), g, h_hat, h</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(<span class="op">-</span>x))</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backward(x, y, y_pred, w_2, h_hat):</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Backward pass through our neural network."</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    Q_2 <span class="op">=</span> (y_pred<span class="op">-</span>y[:, <span class="va">None</span>])<span class="op">*</span>y_pred<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>y_pred)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    Q_1 <span class="op">=</span> Q_2 <span class="op">@</span> w_2.T<span class="op">*</span>h_hat<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>h_hat)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    grad_w_1 <span class="op">=</span> x.T <span class="op">@</span> Q_1</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    grad_b_1 <span class="op">=</span> np.<span class="bu">sum</span>(Q_1, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    grad_w_2 <span class="op">=</span> h_hat.T <span class="op">@</span> Q_2</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> grad_w_1, grad_b_1, grad_w_2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="train-the-model" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3" class="anchored" data-anchor-id="train-the-model"><span class="header-section-number">2.2.3</span> Train the model</h3>
<p>We can now train the model!</p>
<div id="d3b2ca93-1a3b-40c5-ab7c-be61c974aaf4" class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Training parameters</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>eta <span class="op">=</span> <span class="dv">50</span>      <span class="co"># learning rate</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>n_epoch <span class="op">=</span> <span class="dv">500</span> <span class="co"># training epochs</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the weights randomly</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>w_1 <span class="op">=</span> <span class="dv">1</span><span class="op">*</span>(np.random.rand(m, h) <span class="op">-</span> <span class="fl">0.5</span>)  </span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>w_2 <span class="op">=</span> <span class="dv">1</span><span class="op">*</span>(np.random.rand(h, c) <span class="op">-</span> <span class="fl">0.5</span>)  </span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>b_1 <span class="op">=</span> <span class="dv">1</span><span class="op">*</span>(np.random.rand(h)   <span class="op">-</span> <span class="fl">0.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="a9b76d77-3826-4e6c-a0fd-707a7fe721bb" class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>loss_train_vs_epoch <span class="op">=</span> []</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>loss_val_vs_epoch <span class="op">=</span> []</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(n_epoch)):</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward pass</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    Y_pred, g, h_hat, h <span class="op">=</span> forward(X_train, w_1, b_1, w_2)    </span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    loss_train <span class="op">=</span> <span class="fl">0.5</span><span class="op">*</span>np.mean((Y_pred.squeeze() <span class="op">-</span> Y_train)<span class="op">**</span><span class="dv">2</span>)    </span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    loss_train_vs_epoch.append(loss_train)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backward pass</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    grad_w_1, grad_b_1, grad_w_2 <span class="op">=</span> backward(X_train, Y_train, Y_pred, w_2, h_hat)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update parameters </span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    w_1 <span class="op">-=</span> eta<span class="op">/</span>n_train<span class="op">*</span>grad_w_1 </span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    b_1 <span class="op">-=</span> eta<span class="op">/</span>n_train<span class="op">*</span>grad_b_1</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    w_2 <span class="op">-=</span> eta<span class="op">/</span>n_train<span class="op">*</span>grad_w_2</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Validate the performance</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    Y_pred, _, _, _ <span class="op">=</span> forward(X_val, w_1, b_1, w_2)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    loss_val <span class="op">=</span> <span class="fl">0.5</span><span class="op">*</span>np.mean((Y_pred.squeeze() <span class="op">-</span> Y_val)<span class="op">**</span><span class="dv">2</span>)    </span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    loss_val_vs_epoch.append(loss_val)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"8dd0f0211ef4409881860a4551fd5152","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<div id="9a487b31-8e25-467c-9fd0-1925c5d48b79" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Train and validation loss of </span><span class="sc">{</span>loss_train<span class="sc">:.5f}</span><span class="ss"> and </span><span class="sc">{</span>loss_val<span class="sc">:.5f}</span><span class="ss">, respectively"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Train and validation loss of 0.00625 and 0.00735, respectively</code></pre>
</div>
</div>
<div id="0bf1753a" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">4</span>))</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>plt.plot(loss_val_vs_epoch, label <span class="op">=</span> <span class="st">"validation"</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>plt.plot(loss_train_vs_epoch, label <span class="op">=</span> <span class="st">"training"</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>plt.grid(alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"MNIST"</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Epoch"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Loss (MSE)"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>plt.tick_params(labelsize<span class="op">=</span><span class="dv">12</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="neural_networks_from_scratch_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Now, we can look at the performance over unseen data from the test set.</p>
<div id="0d9fe018-13b8-484e-adb9-7b26a66646e0" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>Y_pred, _, _, _ <span class="op">=</span> forward(X_test, w_1, b_1, w_2)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>loss_test <span class="op">=</span> <span class="fl">0.5</span><span class="op">*</span>np.mean((Y_pred.squeeze() <span class="op">-</span> Y_test)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The test loss is </span><span class="sc">{</span>loss_test<span class="sc">:.5f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>The test loss is 0.00699</code></pre>
</div>
</div>
<p>The model seems to generalize fairly well, as the performance is comparable to the one obtained in the training set. Indeed, looking at the training losses, we see that the model is barely overfitting as there is almost no difference between the training and validation loss. This is mainly due to the simplicity of the model that we are considering.</p>
<p>We can also pretend for a moment that this is a classification task. This is definitely not how you would frame a classification problem, but we can assign prediction intervals to the MNIST labels and see how we would do.</p>
<div id="b227da6b-6e0e-4d3d-bfc7-6bb203a15faa" class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train set prediction</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>pred_train, _, _, _ <span class="op">=</span> forward(X_train, w_1, b_1, w_2)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>pred_train <span class="op">=</span> np.around(<span class="dv">10</span><span class="op">*</span>pred_train).astype(<span class="bu">int</span>).squeeze()</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>true_train <span class="op">=</span> (<span class="dv">10</span><span class="op">*</span>Y_train).astype(<span class="bu">int</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>conf_mat_train <span class="op">=</span> get_confusion_matrix(pred_train, true_train)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>accuracy_train <span class="op">=</span> (pred_train <span class="op">==</span> true_train).mean()</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Test set prediction</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>pred_test, _, _, _ <span class="op">=</span> forward(X_test, w_1, b_1, w_2)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>pred_test <span class="op">=</span> np.around(<span class="dv">10</span><span class="op">*</span>pred_test).astype(<span class="bu">int</span>).squeeze()</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>true_test <span class="op">=</span> (<span class="dv">10</span><span class="op">*</span>Y_test).astype(<span class="bu">int</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>conf_mat_test <span class="op">=</span> get_confusion_matrix(pred_test, true_test)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>accuracy_test <span class="op">=</span> (pred_test <span class="op">==</span> true_test).mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="05b78760-feec-4fc0-b6df-cec0cf1826d1" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, sharey<span class="op">=</span><span class="va">True</span>, constrained_layout<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].imshow(conf_mat_train)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"Train | Accuracy : "</span> <span class="op">+</span> <span class="st">"</span><span class="sc">{:2.2f}</span><span class="st">"</span>.<span class="bu">format</span>(accuracy_train<span class="op">*</span><span class="dv">100</span>) <span class="op">+</span> <span class="st">"%"</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"True label"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">"Predicted label"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xticks(np.arange(<span class="dv">0</span>, <span class="dv">10</span>))</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_yticks(np.arange(<span class="dv">0</span>, <span class="dv">10</span>))</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].imshow(conf_mat_test)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"Test | Accuracy : "</span> <span class="op">+</span> <span class="st">"</span><span class="sc">{:2.2f}</span><span class="st">"</span>.<span class="bu">format</span>(accuracy_test<span class="op">*</span><span class="dv">100</span>) <span class="op">+</span> <span class="st">"%"</span>)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"True label"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xticks(np.arange(<span class="dv">0</span>, <span class="dv">10</span>))</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_yticks(np.arange(<span class="dv">0</span>, <span class="dv">10</span>))<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="neural_networks_from_scratch_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>As we can see, the accuracy matrix has quite diagonal structure! With an accuracy far beyond what we would obtain from a random guess! We see, however, that most errors occur between consecutive classes, which is mainly due to rounding errors from the imperfect regression.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>We reiterate that this is not the proper way to handle a classification task. This is just an academic experiment to get familiar with the perceptron and see that neural networks are just a bunch of affine transformations.</p>
</div>
</div>
<p>Let’s see if we can get a better understanding of the model by looking at some predictions:</p>
<div id="6c6fee9e" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> np.where(true_test <span class="op">==</span> i)[<span class="dv">0</span>] <span class="co"># find indices of i-digit</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, k, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>        title_string <span class="op">=</span> <span class="st">"</span><span class="ch">\n</span><span class="st"> P: "</span> <span class="op">+</span> <span class="st">"</span><span class="sc">{:01d}</span><span class="st">"</span>.<span class="bu">format</span>(<span class="bu">int</span>(pred_test[idx[j]]))</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>        ax[j].set_title(title_string)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>        ax[j].imshow(X_test[idx[j], :].reshape(<span class="dv">28</span>, <span class="dv">28</span>))</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># plt.show()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="neural_networks_from_scratch_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="neural_networks_from_scratch_files/figure-html/cell-14-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="neural_networks_from_scratch_files/figure-html/cell-14-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="neural_networks_from_scratch_files/figure-html/cell-14-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="neural_networks_from_scratch_files/figure-html/cell-14-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="neural_networks_from_scratch_files/figure-html/cell-14-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="neural_networks_from_scratch_files/figure-html/cell-14-output-7.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="neural_networks_from_scratch_files/figure-html/cell-14-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="neural_networks_from_scratch_files/figure-html/cell-14-output-9.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="neural_networks_from_scratch_files/figure-html/cell-14-output-10.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In this examples, we see more clearly that, indeed, most errors occur due to categories being close to each other. For instance, all the errors in the images with 6s are either 5s or 7s. This is one of the main reasons why classification problems are not framed this way, but rather we treat every class as an independent instance of the rest.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>Check training/test accuracy and confusion matrix for different numbers of training data samples.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>Set initial biases to zero, and freeze its training. Check the change in the confusion matrix and accuracy.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>Check prediction accuracy and confusion matrix for weights and bias initialization taken from: 1. Uniform distribution [-0.5,0.5] 2. Uniform distribution [0,1] 3. Normal distribution <span class="math inline">\({\cal N}(0,1)\)</span></p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>Implement function for adaptative learning rate <span class="math inline">\(\eta\)</span> with the following rule: check relative change of the training loss after 10 epochs: if it is smaller than 5%, then <span class="math inline">\(\eta_{new} = \kappa\eta_{old}\)</span>, <span class="math inline">\(\kappa&lt;1\)</span>.</p>
</div>
</div>
</section>
</section>
<section id="activation-functions" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="activation-functions"><span class="header-section-number">2.3</span> Activation functions</h2>
<p>So far we have been using softmax <span class="math inline">\(\sigma(z) = \frac{1}{1+e^{-x}}\)</span> activation function only. The other activation functions are:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="neural_networks_from_scratch_files/figure-html/2cd53b13-1-0_lo8wlkwReDcXkts0.png" class="img-fluid figure-img"></p>
<figcaption>0_lo8wlkwReDcXkts0.png</figcaption>
</figure>
</div>
<p>Loss function should be calculated accordignly to the given activation function!</p>
</section>
</section>
<section id="optimization-algorithms" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Optimization algorithms</h1>
<p>There are many different optimization algorithms that can be used to train neural networks, and choosing the proper algorithm is essential to obtain a performant and well-trained model <span class="citation" data-cites="goodfellow:2016">[@goodfellow:2016]</span>.</p>
<p>In general, optimization algorithms can be divided into two categories: first-order methods, which only use the gradient of the loss function with respect to the model’s parameters, and second-order methods, which also use the second derivative (or Hessian matrix). Second-order methods can be more computationally expensive, but they may also be more effective in certain cases.</p>
<section id="stochastic-gradient-descent---sgd" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="stochastic-gradient-descent---sgd"><span class="header-section-number">3.1</span> Stochastic gradient descent - SGD</h2>
<p>In the standard gradient descent, we compute the gradient of the cost function with respect to the parameters for the entire training dataset. In most cases, it is extremely slow and even intractable for datasets that don’t even fit in memory. It also doesn’t allow us to update our model online, i.e.&nbsp;with new examples on-the-fly.</p>
<p>In SGD gradient descent, we use mini-batches comprised of a few training samples, and the model’s parameters are updated based on the average loss across the samples in each mini-batch. This way, SGD is able to make faster progress through the training dataset, and it can also make use of vectorized operations, which can make the training process more efficient.</p>
</section>
<section id="momentum" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="momentum"><span class="header-section-number">3.2</span> Momentum</h2>
<p>Momentum optimization is an algorithm that can be used to improve SGD. It works by adding a fraction <span class="math inline">\(\gamma\)</span> of the previous parameter update to the current one, which helps the model make faster progress in the right direction and avoid getting stuck in local minima. This fraction is called the momentum coefficient, and it is a hyperparameter that can be adjusted according to the problem.</p>
<p>The momentum algorithm accumulates a history of the past gradients and continues to move in their direction:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
g_t &amp;=  \frac{\partial L(\theta_{t-1})}{\partial \theta}\\
v_t &amp;= \gamma v_{t-1} - \eta g_t \\
\theta &amp;= \theta + v_t,
\end{split}
\end{equation}\]</span> where <span class="math inline">\(t\)</span> enumerates training epoch, <span class="math inline">\(\theta\)</span> are the trainable parameters of the Neural Network, <span class="math inline">\(\gamma\)</span> is the momentum coefficient and <span class="math inline">\(\eta\)</span> is the learning rate.</p>
<p>The velocity <span class="math inline">\(v\)</span> accumulates the gradient of the loss function <span class="math inline">\(L\)</span>; the larger <span class="math inline">\(\gamma\)</span> with respect to <span class="math inline">\(\eta\)</span>, the more previous gradients affect the current direction. In the standard SGD algorithm, the update size depended on the gradient and the learning rate. With momentum, it also depends on how large and how aligned consecutive gradients are.</p>
<p>The momentum algorithm can be understood as the simulation of a particle subjected to Newtonian dynamics. The position of particle at time <span class="math inline">\(t\)</span> is given by value of the trainable parameters <span class="math inline">\(\theta(t)\)</span>. The particle experiences the net force <span class="math display">\[\begin{equation}
f(t) = \frac{\partial^2}{\partial t^2}\theta(t),
\end{equation}\]</span> which can be expressed as a set of first time-derivatives: <span class="math display">\[\begin{equation}
\begin{split}
v(t) = \frac{\partial}{\partial t}\theta(t) \\
f(t) = \frac{\partial}{\partial t}v(t)
\end{split}
\end{equation}\]</span></p>
<p>In the momentum optimization we have two forces: 1. One force is proportional to the negative gradient of the cost function <span class="math display">\[\begin{equation}
f_1 = -\frac{\partial L(\theta)}{\partial \theta}.
\end{equation}\]</span> This force pushes the particle downhill along the cost function surface.</p>
<ol start="2" type="1">
<li>Second force is proportional to <span class="math inline">\(-v(t)\)</span>. Physical intuition is that this force corresponds to viscous drag, as if the particle must push through a resistant medium.</li>
</ol>
<p>In addition to speeding up training, momentum optimization can also help the model to generalize better to new data.</p>
</section>
<section id="adaptative-gradient---adagrad" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="adaptative-gradient---adagrad"><span class="header-section-number">3.3</span> Adaptative Gradient - Adagrad</h2>
<p>Adaptative Gradient algorithm [<a href="https://jmlr.org/papers/v12/duchi11a.html">2</a>] is based on the idea of adapting the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters.</p>
<p>The AdaGrad algorithm works by accumulating the squares of the gradients for each parameter, and then scaling the learning rate for each parameter by the inverse square root of this sum. This has the effect of reducing the learning rate for parameters that have been updated frequently, and increasing the learning rate for parameters that have been updated infrequently.</p>
<p>The update rule for AdaGrad algorithm reads</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
\theta_{t+1} &amp; = \theta_t + \Delta\theta,
\end{split}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{split}
\Delta \theta &amp;= - \frac{\eta}{\sqrt{diag( \epsilon\mathbb{1} + G_t )}} \odot g_t,\\
g_t &amp;= \frac{\partial L(\theta_{t-1})}{\partial \theta}\\
G_t &amp;= \sum_{\tau = 1}^{t} g_\tau g_\tau^T.
\end{split}
\end{equation}\]</span> where <span class="math inline">\(\odot\)</span> means element-wise multiplication. The <span class="math inline">\(\epsilon \ll 0\)</span> is a regularizing parameter, preventing from division by 0.</p>
<p>Adagrad eliminates the need to manually tune the learning rate, i.e.&nbsp;initially <span class="math inline">\(\eta \ll 1\)</span>, and it is effectively adapted during training process. Algorithm can be sensitive to the choice of the initial learning rate, and it may require careful tuning to achieve good results.</p>
</section>
<section id="adadelta-extension-of-the-adaptative-gradient" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="adadelta-extension-of-the-adaptative-gradient"><span class="header-section-number">3.4</span> Adadelta: extension of the Adaptative Gradient</h2>
<p>Adadelta algorithm [<a href="https://arxiv.org/pdf/1212.5701.pdf">3</a>] is based on the idea of adapting the learning rate to the parameters, similar to AdaGrad, but it does not require the specification of a learning rate as a hyperparameter. Adadelta uses an Exponentially Weighted Moving Average (EWMA) of the squared gradients to scale the learning rate. The Exponentially Weighted Moving Average (EWMA) for <span class="math inline">\(x_t\)</span> is defined recursively as:</p>
<p><span class="math display">\[\begin{equation}
E[x]_t = \gamma E[x]_{t-1} + (1-\gamma) x_t
\end{equation}\]</span></p>
<p>In general Adadelta algorithm uses EMWA for <span class="math inline">\(g_t^2\)</span> instead <span class="math inline">\(G_t = \sum_{\tau = 1}^t g_\tau g_\tau^T\)</span>, as in Adagrad, i.e.:</p>
<p><span class="math display">\[\begin{equation}
E[g^2]_t = \gamma E[g^2]_{t-1} + (1-\gamma)g_t^2,
\end{equation}\]</span> and we update parameters as <span class="math display">\[\begin{equation}
\begin{split}
\theta_{t+1} &amp; = \theta_t + \Delta\theta_t \\
\Delta\theta_t &amp; = - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}}\odot g_t.
\end{split}
\end{equation}\]</span> Let’s introduce notation <span class="math inline">\(RMS[g]_t = \sqrt{E[g^2]_t + \epsilon}\)</span>, where <span class="math inline">\(RMS\)</span> stands for root-mean-square.</p>
<p>However, Matthew D. Zeiler, the Author of Adadelta noticed that the parameter updates <span class="math inline">\(\Delta\theta\)</span> being applied to <span class="math inline">\(\theta\)</span> shoud have matching units. Considering, that the parameter had some hypothetical units <span class="math inline">\([\theta]\)</span>, the changes to the parameter should be changes in those units as well, i.e. <span class="math display">\[\begin{equation}
[\theta] = [\Delta\theta].
\end{equation}\]</span> However, assuming the loss function is unitless, we have <span class="math display">\[\begin{equation}
[\Delta\theta] = \frac{1}{[\theta]},
\end{equation}\]</span> thus units do not match. This is the case for SGD, Momentum, or Adagrad algorithms.</p>
<p>The second order methods such as Newton’s method that use the second derivative information preserve units for the parameter updates. For function <span class="math inline">\(f(x)\)</span>, we have <span class="math display">\[\begin{equation}
\Delta x = \frac{\frac{\partial f}{\partial x}}{\frac{\partial^2 f}{\partial x^2}},
\end{equation}\]</span> thus units <span class="math inline">\([\Delta x] = [x]\)</span> are preserved. Keeping this in mind, the update rule in Adadelta algorithm is defined as: <span class="math display">\[\begin{equation}
\begin{split}
  \theta_{t+1} &amp; = \theta_t + \Delta_t\theta_t \\
  \Delta\theta_t &amp;= -\frac{RMS[\Delta\theta]_{t-1}}{RMS[g]_t}\odot g_t.
\end{split}
\end{equation}\]</span></p>
<p>This has the effect of automatically adapting the learning rate to the characteristics of the problem, which can make it easier to use than other optimization algorithms that require manual tuning of the learning rate.</p>
</section>
<section id="adaptive-moment-estimation---adam" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="adaptive-moment-estimation---adam"><span class="header-section-number">3.5</span> Adaptive Moment Estimation - Adam</h2>
<p>Adam algorithm [<a href="https://arxiv.org/pdf/1412.6980.pdf">4</a>] combines the ideas of momentum optimization and Adagrad to make more stable updates and achieve faster convergence.</p>
<p>Like momentum optimization, Adam uses an exponentially decaying average of the previous gradients to determine the direction of the update. This helps the model to make faster progress in the right direction and avoid oscillations. Like AdaGrad, Adam also scales the learning rate for each parameter based on the inverse square root of an exponentially decaying average of the squared gradients. This has the effect of reducing the learning rate for parameters that have been updated frequently, and increasing the learning rate for parameters that have been updated infrequently.</p>
<p>Adam uses Exponentially Modified Moving Average for gradients and its square:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
g_t &amp;= \frac{\partial L(\theta_{t-1})}{\partial \theta}\\
m_t &amp;= \beta_1 m_{t-1} + (1-\beta_1)g_t \\
v_t &amp;= \beta_2 v_{t-1} + (1-\beta_2)g_t^2.
\end{split}
\end{equation}\]</span></p>
<p>The update rule for the parameters reads:</p>
<p><span class="math display">\[\begin{equation}
\Delta \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon}\hat{m}_t,
\end{equation}\]</span> where <span class="math display">\[\begin{equation}
\begin{split}
\hat{m}_t &amp;= \frac{m_t}{1-\beta_1}\\
\hat{v}_t &amp;= \frac{v_t}{1-\beta_2},
\end{split}
\end{equation}\]</span> are bias-corrected first and second gradient moments estimates.</p>
<p>Authors suggest to set <span class="math inline">\(\beta_1 = 0.9\)</span>, <span class="math inline">\(\beta_2 = 0.999\)</span>, <span class="math inline">\(\eta = 10^{-8}\)</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>Implement mini-batch SGD algorithm. In the training loop feed-forward training data with fixed size batches and update gradients after each batch. Steps:</p>
<ol type="1">
<li><p>Fix batch size <span class="math inline">\(n_b\)</span></p></li>
<li><p>Calculate number of batches <span class="math inline">\(N_b = N_{train}/n_b\)</span></p></li>
<li><p>Loop over training epochs:</p>
<p>3.1. Loop over number of batches:</p>
<pre><code> 3.1.1 at each iteration randomly choose batch size training points from training data set

 3.1.2 feed-forward through network

 3.1.3 backpropagate and update weights</code></pre>
<p>3.2 Calculate loss on test dataset, and on a single batch from the training dataset</p></li>
</ol>
</div>
</div>
</section>
</section>
<section id="references" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> References</h1>
<p>[1] Goodfellow, I., Bengio, Y., Courville, A. “Deep Learning”, MIT Press (2016), https://www.deeplearningbook.org/</p>
<p>[2] Duchi, J., Hazan, E., &amp; Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12, 2121–2159 \</p>
<p>[3] Zeiler, M. D. (2012). ADADELTA: An Adaptive Learning Rate Method. Retrieved from http://arxiv.org/abs/1212.5701</p>
<p>[4] Kingma, D. P., &amp; Ba, J. L. (2015). Adam: a Method for Stochastic Optimization. International Conference on Learning Representations, 1–13. https://arxiv.org/abs/1412.6980</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/BorjaRequena\.github\.io\/Neural-Network-Course");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/BorjaRequena/Neural-Network-Course/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li><li><a href="https://github.com/BorjaRequena/Neural-Network-Course/blob/master/nbs/course/deep_learning/neural_networks_from_scratch.ipynb" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div></div></footer></body></html>