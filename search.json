[
  {
    "objectID": "lib_nbs/utils.html",
    "href": "lib_nbs/utils.html",
    "title": "Miscellaneous",
    "section": "",
    "text": "source\n\nshow_code\n\n show_code (fun)\n\nShows the source code of a function in the output of the cell.\n\ndef f(a,b):\n    '''Computes the sum $a+b$'''\n    return a+b\nshow_code(f)\n\ndef f(a,b):\n    '''Computes the sum $a+b$'''\n    return a+b"
  },
  {
    "objectID": "lib_nbs/optimizers.html",
    "href": "lib_nbs/optimizers.html",
    "title": "Optimizers",
    "section": "",
    "text": "source\n\ngradient_descent\n\n gradient_descent (x:numpy.ndarray, y:numpy.ndarray, pini:dict, ll:dict,\n                   niter=1000, eta=0.001)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nndarray\n\nx data of N elements\n\n\ny\nndarray\n\ny data of N elements\n\n\npini\ndict\n\ninitial parameters of the loss function\n\n\nll\ndict\n\ndictionary with the loss (‘loss’), the gradients (‘grads’) and the function (‘fun’) for the regression/classification\n\n\nniter\nint\n1000\nnumber of iterations\n\n\neta\nfloat\n0.001\nlearning rate\n\n\nReturns\ndict\n\ndictionary containing the vector of the losses (‘loss’) and the parameters (following the keys of pini)\n\n\n\n\nsource\n\n\nsgd_epoch\n\n sgd_epoch (x:numpy.ndarray, y:numpy.ndarray, pini:dict, ll:dict, bs=10,\n            eta=0.001)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nndarray\n\nx data of N elements\n\n\ny\nndarray\n\ny data of N elements\n\n\npini\ndict\n\ninitial parameters of the loss function\n\n\nll\ndict\n\ndictionary with the loss (‘loss’), the gradients (‘grads’) and the function (‘fun’) for the regression/classification\n\n\nbs\nint\n10\nBatch size\n\n\neta\nfloat\n0.001\nlearning rate\n\n\nReturns\ndict\n\ndictionary containing the vector of the losses (‘loss’) and the parameters (following the keys of pini)\n\n\n\n\nsource\n\n\nsgd\n\n sgd (x:numpy.ndarray, y:numpy.ndarray, pini:dict, ll:dict, bs=10,\n      eta=0.001, niter=1000)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nndarray\n\nx data of N elements\n\n\ny\nndarray\n\ny data of N elements\n\n\npini\ndict\n\ninitial parameters of the loss function\n\n\nll\ndict\n\ndictionary with the loss (‘loss’), the gradients (‘grads’) and the function (‘fun’) for the regression/classification\n\n\nbs\nint\n10\nBatch size\n\n\neta\nfloat\n0.001\nlearning rate\n\n\nniter\nint\n1000\nnumber of epochs\n\n\nReturns\ndict\n\ndictionary containing the vector of the losses (‘loss’) and the parameters (following the keys of pini)",
    "crumbs": [
      "Library",
      "Optimizers"
    ]
  },
  {
    "objectID": "course/montecarlo_integration.html#gaussian-probability-distribution",
    "href": "course/montecarlo_integration.html#gaussian-probability-distribution",
    "title": "Monte Carlo Integration",
    "section": "1.1 Gaussian Probability Distribution",
    "text": "1.1 Gaussian Probability Distribution\nThe Gaussian (normal) distribution is defined as: \\[p(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}exp-\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\,.\\] Here, the variance and the mean of the distribution appear explicitly in the pdf formula.\nThis distribution is very important in many applications mainly due to two reasons. First, the central limit theorem (informally) states that the probability distribution of the sum of many independent random variables trends towards a normal distribution, even if the original variables themselves are not normally distributed. Meaning that we can model many complicated systems with normal distributions.\nSecond, the gaussian distribution encodes the maximum possible amount of uncertainty for a fixed variance. Thus, it is the probability distribution assumes the least amount of knowledge about the systems we wish to model.",
    "crumbs": [
      "Course",
      "Monte Carlo Integration"
    ]
  },
  {
    "objectID": "course/montecarlo_integration.html#example",
    "href": "course/montecarlo_integration.html#example",
    "title": "Monte Carlo Integration",
    "section": "2.1 Example",
    "text": "2.1 Example\nAs an example, let’s compute the integral of the parabola \\(f(x)=-x^2 +10\\) in the interval \\(x\\in [-3, 3]\\)\n\ndef parabola(x):\n    return -(x)**2 + 10\n\ndef is_below_parabola(point):\n    if parabola(point[0]) &gt; point[1]:\n        return 1\n    else:\n        return 0\n\ndef sample_random_point(interval_x, interval_y):\n    x = np.random.rand()*(interval_x[1]-interval_x[0])+interval_x[0]\n    y = np.random.rand()*(interval_y[1]-interval_y[0])+interval_y[0]\n    return [x, y]\n\n\narea = 0\nrepetition = 10000\nrandom_points = []\ninterval_x = [-3, 3]\ninterval_y = [1, 10]\nfor _ in range(repetition):\n    # Sample random point\n    random_point = sample_random_point(interval_x, interval_y)\n    random_points.append(random_point)\n    \n    # Is it below the parabola?\n    area += is_below_parabola(random_point)\n        \narea = area/repetition\n\n\n\nCode\nx_below, x_above = [], []\ny_below, y_above = [], []\n\nfor random_point in random_points:\n    if is_below_parabola(random_point):\n        x_below.append(random_point[0])\n        y_below.append(random_point[1])\n    else:\n        x_above.append(random_point[0])\n        y_above.append(random_point[1])\n\nxvals = np.linspace(-3, 3, 10000)  # meshgrid to draw the line\nyvals = parabola(xvals)            # evaluate f for each point\n\nplt.figure(figsize=(8, 6), dpi=80)\nplt.plot(xvals, yvals)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.scatter(x_below, y_below, s=0.8)\nplt.scatter(x_above, y_above, s=0.8)\n\n\n\n\n\n\n\n\n\nThe ration between the points below and above the curve 0.663. The analitical value is 2/3, meaning that we have a great approximation to the integral!\n\narea\n\n0.6629\n\n\nAs we mentioned above, the error goes as \\(\\sim \\sqrt{1/N}\\). Thus, if we want to reduce it by an order of magnitude, we need to take 100 times more samples.",
    "crumbs": [
      "Course",
      "Monte Carlo Integration"
    ]
  },
  {
    "objectID": "course/montecarlo_integration.html#metropolis-hasting-algorithm",
    "href": "course/montecarlo_integration.html#metropolis-hasting-algorithm",
    "title": "Monte Carlo Integration",
    "section": "5.1 Metropolis-Hasting algorithm",
    "text": "5.1 Metropolis-Hasting algorithm\nThe Metropolis-Hastings algorithm advances the Markov chain from a configuration \\(x_{n-1}\\) to some new configuration \\(x_n\\) repeatedly applying the following steps:\n\nChoose some candidate configuration \\(x'\\) according to some a priori selection probability \\(T_0(x′|x)\\), where \\(x = x_{n−1}\\).\nAccept the candidate configuration \\(x'\\) as the new configuration \\(x_n\\) with acceptance probability: \\(T_A(x' |x ) =min\\left(1,\\frac{T_0(x|x')e^{−H(x')}}{T_0(x′|x)e^{−H(x)}}\\right)\\). Otherwise, set \\(x_n = x_{n-1}\\).\n\nThis way, if a suggested change is not accepted, the unchanged configuration is considered again in the Markov chain and included in the measurements like the others.\nIt is straightforward to see that the total transition probability \\(T = T_0 T_A\\) fulfills the detailed balance condition by construction: \\[  T(x' |x ) P(x) = T_0(x|x')\\, min\\left(1,\\frac{T_0(x|x')p(x')}{T_0(x′|x)p(x)}\\right)p(x) =  min\\left(T_0(x|x')p(x),T_0(x|x')p(x')\\right) =  T(x |x' ) P(x')\\] In many cases one uses a symmetric selection probability which obeys: \\[  T_0(x' |x )= T_0(x |x' ) \\] In particular for symmetric T0, the information necessary to decide on acceptance or rejection comes only from the change of the Hamiltonia \\(\\Delta H\\) with respect to the change of the configuration. If this change is local, e.g., it just involves a single spin variable, then \\(\\Delta H\\) may be determined from the spin values in the local neighborhood.\n\n\n\n\n\n\nNote\n\n\n\nThe central step of the Monte Carlo procedure needs random numbers. In the computer programs these are so-called pseudo random numbers, generated reproducibly by algorithms. The statistical properties of the pseudo random numbers are very close to those of real random numbers. Typical Monte Carlo runs may need \\(O(10^{12})\\) random numbers. Therefore, utmost care has to be taken in selecting a proper generator. Standard implementations of random number generators are often not reliable enough and produce subtly correlated numbers with too small periods. High-quality generators use the so-called lagged Fibonacci method and there are generators with extremely long guaranteed periods \\(O(10^{171})\\). Pseudo random numbers are usually generated according to a uniform distribution in the interval \\([0, 1)\\). There exists a variety of algorithms to generate from these other distributions. There are nowadays quantum random number generators (e.g the one produced by Quside, a spinoff from ICFO) for particularly expensive Monte Marlo simulations.",
    "crumbs": [
      "Course",
      "Monte Carlo Integration"
    ]
  },
  {
    "objectID": "course/montecarlo_integration.html#monte-carlo-of-a-two-dimensional-ising-model",
    "href": "course/montecarlo_integration.html#monte-carlo-of-a-two-dimensional-ising-model",
    "title": "Monte Carlo Integration",
    "section": "5.2 Monte Carlo of a two dimensional Ising model",
    "text": "5.2 Monte Carlo of a two dimensional Ising model\n\ndef initialstate(N):   \n    ''' \n    Generates a random spin configuration for initial condition\n    '''\n    state = 2*np.random.randint(2, size=(N,N))-1\n    return state\n\ndef mcmove(config, beta):\n    '''\n    Monte Carlo move using Metropolis algorithm \n    '''\n    \n    for i in range(N):\n        for j in range(N):\n            a = np.random.randint(0, N)\n            b = np.random.randint(0, N)\n            s =  config[a, b]\n            nb = config[(a+1)%N,b] + config[a,(b+1)%N] + config[(a-1)%N,b] + config[a,(b-1)%N]\n            cost = 2*s*nb\n\n            if cost &lt; 0:\n                s *= -1\n            elif rand() &lt; np.exp(-cost*beta):\n                s *= -1\n            config[a, b] = s\n    return config\n\ndef calcEnergy(config):\n    '''\n    Energy of a given configuration\n    '''\n    energy = 0 \n    \n    for i in range(len(config)):\n        for j in range(len(config)):\n            S = config[i,j]\n            nb = config[(i+1)%N, j] + config[i,(j+1)%N] + config[(i-1)%N, j] + config[i,(j-1)%N]\n            energy += -nb*S\n    return energy/2.  # to compensate for over-counting\n\ndef calcMag(config):\n    '''\n    Magnetization of a given configuration\n    '''\n    mag = np.sum(config)\n    return mag\n\n\n## NOTE: change these parameters for a smaller and faster simulation \n#----------------------------------------------------------------------\n\nnt      = 16         #  number of temperature points\nN       = 20         #  size of the lattice, N x N\neqSteps = 2**8       #  number of MC sweeps for equilibration\nmcSteps = 2**9       #  number of MC sweeps for calculation\n\nT       = np.linspace(1.53, 3.28, nt); \nE,M,C,X = np.zeros(nt), np.zeros(nt), np.zeros(nt), np.zeros(nt)\nn1, n2  = 1.0/(mcSteps*N*N), 1.0/(mcSteps*mcSteps*N*N) # divide by number of samples, and by system size to get intensive values\n\n\n#  MAIN PART OF THE CODE\n#----------------------------------------------------------------------\n\nfor tt in range(nt):\n    config = initialstate(N)         # initialise\n\n    E1 = M1 = E2 = M2 = 0\n    iT=1.0/T[tt]; iT2=iT*iT;\n    \n    for i in range(eqSteps):         # equilibrate\n        mcmove(config, iT)           # Monte Carlo moves\n\n    for i in range(mcSteps):\n        mcmove(config, iT)           \n        Ene = calcEnergy(config)     # calculate the energy\n        Mag = calcMag(config)        # calculate the magnetisation\n\n        E1 = E1 + Ene\n        M1 = M1 + Mag\n        M2 = M2 + Mag*Mag \n        E2 = E2 + Ene*Ene\n\n    # divide by number of sites and iteractions to obtain intensive values    \n    E[tt] = n1*E1 #Energy\n    M[tt] = n1*M1 #Magnetization \n    C[tt] = (n1*E2 - n2*E1*E1)*iT2 #Specific Heat \n    X[tt] = (n1*M2 - n2*M1*M1)*iT  #Susceptibility\n\n\n\nCode\nf = plt.figure(figsize=(18, 10)); #  \n\n\nsp =  f.add_subplot(2, 2, 1 );\nplt.scatter(T, E, s=50, marker='o', color='IndianRed')\nplt.xlabel(\"Temperature (T)\", fontsize=20);\nplt.ylabel(\"Energy \", fontsize=20);         plt.axis('tight');\n\n\nsp =  f.add_subplot(2, 2, 2 );\nplt.scatter(T, abs(M), s=50, marker='o', color='RoyalBlue')\nplt.xlabel(\"Temperature (T)\", fontsize=20); \nplt.ylabel(\"Magnetization \", fontsize=20);   plt.axis('tight');\n\n\nsp =  f.add_subplot(2, 2, 3 );\nplt.scatter(T, C, s=50, marker='o', color='IndianRed')\nplt.xlabel(\"Temperature (T)\", fontsize=20);  \nplt.ylabel(\"Specific Heat \", fontsize=20);   plt.axis('tight');   \n\n\nsp =  f.add_subplot(2, 2, 4 );\nplt.scatter(T, X, s=50, marker='o', color='RoyalBlue')\nplt.xlabel(\"Temperature (T)\", fontsize=20); \nplt.ylabel(\"Susceptibility\", fontsize=20);   plt.axis('tight');\n\n\n\n\n\n\n\n\n\nExample montecarlo in: http://chi-feng.github.io/mcmc-demo/app.html?algorithm=RandomWalkMH&target=banana",
    "crumbs": [
      "Course",
      "Monte Carlo Integration"
    ]
  },
  {
    "objectID": "course/rbm.html",
    "href": "course/rbm.html",
    "title": "Restricted Boltzmann Machines",
    "section": "",
    "text": "Modern artificial intelligence relies on the representational power and computational capabilities of networks of interconnected elementary processing units (neurons). This concept came to light during the 1950s, where a collective effort, widespread over many different disciplines, took on the daunting mystery of the nature of human intelligence and the mechanisms behind cognition and reasoning. The first proposal of artificial neurons, followed by learning theories, gave rise to the beginning of modern cognitive science, and the birth of artificial neural networks. Much interest in the subject was sparked also by the possibilities offered by the first computing machines. This new technology raised the question whether such computing devices, equipped with powerful algorithms and enough computational power, could show intelligent behaviour.",
    "crumbs": [
      "Course",
      "Restricted Boltzmann Machines"
    ]
  },
  {
    "objectID": "course/rbm.html#unsupervised-learning",
    "href": "course/rbm.html#unsupervised-learning",
    "title": "Restricted Boltzmann Machines",
    "section": "2.1 Unsupervised learning",
    "text": "2.1 Unsupervised learning\nGenerative modeling consists of learning the constraints underlying a probability distribution defined over the input space. This way, the neural network can generate input patterns by itself according to the correct (unknown) probability distribution, which we call \\(q(v)\\). The learning occurs by changing the internal parameters to find an optimal set \\(\\lambda^*\\), such that the probability distribution defined by the RBM mimics the target distribution, \\(p_{\\lambda^*}\\sim q\\).\nThe learning mechanism is formulated, as usual, with an optimization problem through of a cost function \\(C_{\\lambda}\\). For the case of generative modeling, the ultimate goal is to reduce the “distance” between the input distribution \\(q(v)\\) and the RBM distribution \\(p_\\lambda(v)\\). We adopt the standard choice of Kullbach-Leibler (KL) divergence (or relative entropy), defined as: \\[C_\\lambda^q=KL(q||p_{\\lambda})=\\sum_v q(v)\\log \\frac{q(v)}{p_{\\lambda}(v)}\\,.\\] Notice that the term \\(q(v)\\log q(v)\\) does not depend on our model. Thus, we use the average negative log-likelihood: \\[ \\left\\langle \\mathcal{L}_{\\lambda} \\right\\rangle_q=-\\sum_vq(v)log(p_{\\lambda}(v))\\]\n\n\n\n\n\n\nNote\n\n\n\nThe KL divergence is not a proper distance measure, since it is non-symmetric (\\(KL(q||p)\\neq KL(p||q)\\) in general) and it does not satisfy the triangle inequality. Nevertheless, the KL divergence is zero if and only if the two probability distribution are equal almost everywhere. Thus, we can safely use it to quantify the distnace between distributions.\n\n\nWe proceed now to calculate the gradient of the cost function with respect to all network parameters \\(\\lambda\\). The unknown distribution q(v) is, in practice, encoded implicitly into a training dataset \\(D = \\{v1,v2,...\\}\\) containing \\(|D|\\) independent configurations \\(v_k\\), identically distributed according to \\(q(v)\\). The unknown distribution \\(q(v)\\) is approximated by the empirical distribution: \\[ q(v')=\\frac {1}{|D|} \\sum_{v_k\\in D }\\delta (v'-v_k)\\] which results into the approximate divergence: \\[C_\\lambda^q \\simeq \\left\\langle \\mathcal{L}_{\\lambda} \\right\\rangle_D + \\sum_vq'(v)log(q'(v))\\,.\\]\nAs we mention above, the second term of the equation is constant and does not need to be learned. It is also called the entropy of the distribution \\(q'\\). The only relevant term for the optimization is the negative log-likelihood: \\[ \\left\\langle \\mathcal{L}_{\\lambda} \\right\\rangle_D=  \\frac {1}{|D|} \\sum_{v'}\\sum_{v_k\\in D } \\delta (v'-v_k) log(p_{\\lambda}(v'))= \\frac {1}{|D|} \\sum_{v_k\\in D}\\epsilon_\\lambda(v_k)+log(Z_\\lambda)\\] To perform an optimization of this cost function, we need to compute the gradient of the negative log likelihood: \\[\\nabla_\\lambda  \\left\\langle \\mathcal{L}_{\\lambda} \\right\\rangle_D=\\nabla_\\lambda  \\frac {1}{|D|} \\sum_{v_k\\in D}\\epsilon_\\lambda(v_k)+ \\nabla_\\lambda log(Z_\\lambda)\\,,\\] which can be rewritten as: \\[\\nabla_\\lambda  \\left\\langle \\mathcal{L}_{\\lambda} \\right\\rangle_D= \\left\\langle \\nabla_\\lambda  \\epsilon_{\\lambda}(v) \\right\\rangle_{v\\sim D} - \\left\\langle \\nabla_\\lambda  \\epsilon_{\\lambda}(v) \\right\\rangle_{v\\sim p_\\lambda}\\]\nThe gradients of the effective visible energy have the simple form: \\[ \\frac{\\partial}{\\partial W_{i,j}}\\epsilon_{\\lambda}(v)=-\\frac{1}{1+e^{-\\Delta_{h_i}}}v_j\\] \\[ \\frac{\\partial}{\\partial b_{j}}\\epsilon_{\\lambda}(v)=-v_j\\] \\[ \\frac{\\partial}{\\partial c_{i}}\\epsilon_{\\lambda}(v)=-\\frac{1}{1+e^{-\\Delta_{h_i}}}\\] where we recall the definition: \\[\\Delta_{h_i}=\\sum_j W_{i,j}v_j+c_i\\]\nIn the positive phase driven by the data \\(\\left\\langle \\nabla_\\lambda  \\epsilon_{\\lambda}(v) \\right\\rangle_{v\\sim D}\\), the energy is lowered for input data configurations (thus increasing their probabilities). During the negative phase \\(-\\left\\langle \\nabla_\\lambda  \\epsilon_{\\lambda}(v) \\right\\rangle_{v\\sim p_\\lambda}\\), the learning occurs in reverse, with the signal generated by the RBM equilibrium distribution: \\[ \\left\\langle \\nabla_\\lambda  \\epsilon_{\\lambda}(v) \\right\\rangle_{v\\sim p_\\lambda}= \\sum_v p_\\lambda(v) \\epsilon_{\\lambda}(v)\\,.\\]\nSince the evaluation of the negative phase requires a summation over an exponential number of states \\(v\\), we calculate this term using Monte Carlo (MC) sampling of the RBM as: \\[ \\left\\langle \\nabla_\\lambda  \\epsilon_{\\lambda}(v) \\right\\rangle_{v\\sim p_\\lambda} \\sim \\frac 1 M \\sum_{l=1}^M  \\epsilon_{\\lambda}(v_l) \\] The effective energy gradient is averaged over \\(M\\) configurations \\(v_l\\) sampled from the RBM.",
    "crumbs": [
      "Course",
      "Restricted Boltzmann Machines"
    ]
  },
  {
    "objectID": "course/rbm.html#gibbs-sampling",
    "href": "course/rbm.html#gibbs-sampling",
    "title": "Restricted Boltzmann Machines",
    "section": "2.2 Gibbs Sampling",
    "text": "2.2 Gibbs Sampling\nGiven the restricted conditional independence of the RBM graph, an MC sampling technique called Gibbs sampling (a.k.a. heat bath or Glauber dynamics) allows us to obtain a fast evaluation of the negative phase of the loss.\nThe general philosophy behind MC sampling is to simulate the evolution of the system (in our case the visible layer) state by state, and compute the expectation value of some observables (such as the effective energy gradient) as an average in Markov time. Contrary to full enumeration, where each state \\(v\\) is weighted in the sum, we build a sequence of \\(M ≪ 2^N\\) states in such a way that each state v appears with a probability \\(p_\\lambda(v)\\) (also called importance sampling).\nIn Gibbs sampling, we update each variable sequentially, conditioned on the values of all the other variables. For the case of the RBM, this corresponds to samping each visible unit \\(v_j\\) from the conditional distribution \\(p_\\lambda(v_j | h,v_{/j})\\) (this reads the probability of \\(v_j=1\\) given \\(h\\) and all elements of \\(v\\) except \\(v_j\\)), and each hidden unit \\(h_i\\) from the conditional distribution \\(p_\\lambda(h_j | v,h_{/j})\\).\nHowever, the RBM structure has the special property that each unit is conditionally independent from the others of the same layer. Thus, we can instead sample all the units in one layer simultaneously (Block Gibbs sampling)! Given an initial state \\(v\\), the selection probability to sample a new state \\(v′\\) is given by the two RBM layer-wise conditional distributions \\(g(v\\to v′)=p_\\lambda(v′ |h)p_\\lambda(h|v)\\) corresponding to the transitions \\(v \\to h\\) and \\(h \\to v′\\). Given this expression, we can easily verify that \\(g(v \\to v′)\\) satisfies detailed balance condition. Furthermore, once a new state has been chosen according to the selection probability, the move is always accepted with unit probability.\n\n\n\nimage.png\n\n\nTo summarize: given an initial state \\(v(0)\\), a new state \\(v(1)\\) is sampled from the distribution \\(p_\\lambda(v)\\) in two steps:\n\nSample a hidden state from the conditional distribution \\(p_\\lambda(h(0)|v(0))\\)\nSample a visible state from \\(p_\\lambda(v(1) | h(0))\\).\n\nBecause of the bipartite structure of the RBM graph, each variable in a layer can be independently sampled simultaneously, leading to layer-wise block sampling. We can repeat this process as many times as we want to build a Markov chain.\n\ndef gibbs_sampling(k, v_0, W, v_bias, h_bias):\n    v_k = np.zeros_like(v_0)\n    h_0 = np.zeros_like(h_bias)\n    h_k = np.zeros_like(h_bias)\n\n    h_0 = gibbs_vtoh(W, v_0, h_bias)\n    v_k = gibbs_htov(W, h_0, v_bias)\n    for i in range(k):\n        h_k = gibbs_vtoh(W, v_k, h_bias)\n        v_k = gibbs_htov(W, h_k, v_bias)\n    return v_k, h_0, h_k\n\n\ndef gibbs_vtoh(W, v, h_bias):\n    activation=np.zeros_like(h_bias)\n    for i in range(activation.shape[0]):\n        for j in range(activation.shape[1]):\n            activation[i,j] = sigmoid(h_bias[i,j] + np.sum(v*W[:,:,i,j]))\n\n    h_k = np.random.binomial(1,activation,size=activation.shape)\n    return h_k\n\ndef gibbs_htov(W, h, v_bias):\n    activation=np.zeros_like(v_bias)\n    for i in range(activation.shape[0]):\n        for j in range(activation.shape[1]):\n            activation[i,j] = sigmoid(v_bias[i,j] + np.sum(h*W[i,j,:,:]))\n\n    v_k = np.random.binomial(1, activation, size=activation.shape)\n    return v_k",
    "crumbs": [
      "Course",
      "Restricted Boltzmann Machines"
    ]
  },
  {
    "objectID": "course/rbm.html#building-a-dataset",
    "href": "course/rbm.html#building-a-dataset",
    "title": "Restricted Boltzmann Machines",
    "section": "3.1 Building a dataset",
    "text": "3.1 Building a dataset\nBefore we dive into our model, we need a suitable dataset to train it. We will sample 100000 spin configurations with the Metropolis-Hastings algorithm to build our dataset.\n\ndef H(spin):\n    n_r, n_c  = spin.shape # used for for loops\n    spin = 2 * spin - 1 # maps {0,1} to {-1,1}\n    \n    H = 0.\n    for i in range(n_r):\n        for j in range(n_c):\n            if j+1 != n_c: #if next index is not out of bounds\n                H += spin[i,j]*spin[i,j+1]\n            if i+1 != n_r:\n                H += spin[i,j]*spin[i+1,j]\n    return -H\n\ndef Met_Hast(n_v_r,n_v_c, T, num_iterations):\n        #Random spin configuration\n        spin = 2* np.random.randint(2,size = (n_v_r,n_v_c)) - 1 #{0,1} --&gt; {-1,1}\n        E = H(spin) # calculate initial energy\n        data_set = np.zeros((num_iterations, n_v_r,n_v_c),dtype=int)\n        \n        for i in range(num_iterations):\n            #Choose random coordinate on 2d lattice to flip\n            x = np.random.randint(n_v_r)\n            y = np.random.randint(n_v_c)\n            spin[x,y] *= -1\n            \n            #Calculate energy change due to flipped spin\n            E_neighbors = 0.\n            if x != 0:         # if there is a site to the left\n                E_neighbors += spin[x-1,y]\n            if x != (n_v_r-1): # if there is a site to the right\n                E_neighbors += spin[x+1,y]\n            if y != 0:         # if there is a site below\n                E_neighbors += spin[x,y-1]\n            if y != (n_v_c-1): # if there is a site above\n                E_neighbors += spin[x,y+1]\n\n            dE = -2 * spin[x,y] * E_neighbors\n\n            #Accept or reject the flipped site\n            if np.random.random() &lt; np.exp(-dE/T):\n                E += dE\n            else:\n                spin[x,y] *= -1\n\n            #Add spin configuration to data set\n            np.copyto(data_set[i,:,:], spin)\n        return (data_set+1)/2 #{-1,1} --&gt; {0,1}\n\nWith this algorithm, we choose one spin site on the lattice and flip it. The flip is automatically accepted if the energy of the system decreases. However, it can also be accepted with some probability if the energy of the system increases. This probability follows a Boltzmann distribution that depends on the temperature, as we saw in the previous part of the course.\nLet’s sample the configurations and shuffling the data to eliminate any possible correlation between samples.\n\ndata_set_size = 100000\ndata_set = Met_Hast(n_v_r, n_v_c, T, data_set_size)\nnp.random.shuffle(data_set)",
    "crumbs": [
      "Course",
      "Restricted Boltzmann Machines"
    ]
  },
  {
    "objectID": "course/rbm.html#rbm-parameters",
    "href": "course/rbm.html#rbm-parameters",
    "title": "Restricted Boltzmann Machines",
    "section": "3.2 RBM parameters",
    "text": "3.2 RBM parameters\nNow let’s initialize the parameters of our RBM. The dimension of the visible layer must match the size of our physical system. Therefore, we have \\(N_v=2\\times3=6\\) visible neurons. However, the size of the hidden layer is completely free. In this case, we will take \\(N_h=16\\) hidden units.\n\n# Visible layer structure\nn_v_r = rows\nn_v_c = columns\nN_v = n_v_r * n_v_c # Total number of visible neurons\n\n#Hidden Layer structure\nn_h_r = 4\nn_h_c = 4\nN_h = n_h_r * n_h_c\n\nHaving set the sizes, we can initialize the actual parameters. The size of our weight matrix depends entirely on the number of visible and hidden neurons.\n\n# Array Initialization\nwidth = np.sqrt(1./(N_v + N_h)) \nW = np.random.uniform(low=-width/2., high=width/2., size=(N_v*N_h)).reshape((n_v_r, n_v_c, n_h_r, n_h_c))\nv_bias = np.ones(N_v).reshape(n_v_r, n_v_c)\nh_bias = np.ones(N_h).reshape(n_h_r, n_h_c)",
    "crumbs": [
      "Course",
      "Restricted Boltzmann Machines"
    ]
  },
  {
    "objectID": "course/rbm.html#probability-distribution-of-the-rbm",
    "href": "course/rbm.html#probability-distribution-of-the-rbm",
    "title": "Restricted Boltzmann Machines",
    "section": "3.3 Probability distribution of the RBM",
    "text": "3.3 Probability distribution of the RBM\nA probability distribution can be represented by the parameter values of the Restricted Boltzmann Machine based on the formula \\[ p(\\sigma) = \\frac{1}{Z}e^{\\varepsilon} \\] where \\(\\varepsilon\\) is defined as \\[ \\varepsilon = \\sum_{j}b_j \\sigma_j + \\sum_{i} log(1+e^{c_i + \\sum_j W_{ij} \\sigma_j})\\]\n\n\\(\\sigma_j\\): jth visible node\n\\(b_j\\): bias of jth visible node\n\\(c_i\\): bias of ith hidden node\n\\(W_{ij}\\): weight between ith hidden node and jth visible node\n\\(Z\\): Partition Function\n\n\ndef p_model(W, v_bias, h_bias, v_set):\n    n_spin = len(v_set[0,0,:])\n    n_v_r,n_v_c = v_bias.shape\n    n_h_r,n_h_c = h_bias.shape\n    p_model = np.zeros(n_spin)\n\n    for spin in range(n_spin):\n        c_ = 0.\n        a_ = np.sum(v_bias*v_set[:,:,spin])\n        for i in range(n_h_r):\n            for j in range(n_h_c):\n                b_ = np.sum(np.multiply(W[:,:,i,j], v_set[:,:,spin]))\n                c_ += np.log(1 + np.exp(h_bias[i,j] + b_))\n        p_model[spin] = np.exp(a_ + c_)\n    Z = float(np.sum(p_model))\n    return p_model/Z\n\ndef p_s(size, T):\n    #size: tuple - (n_r, n_c)\n    #Used in exact Hamiltonian models\n    #   -calculate e^-(H(sig)/T) /Z\n    N = size[0]*size[1]\n    p_model = np.zeros(2**N)\n    v_set = np.zeros((size[0],size[1],2**N)) #set of all possible spin configurations\n    for i in range(len(p_model)):\n\n        np.copyto(v_set[:,:,i], np.array([int(x) for x in np.binary_repr(i,width=N)]).reshape(size))\n        p_model[i] = np.exp(-H(v_set[:,:,i])/T)\n    Z = np.sum(p_model)\n    return p_model/Z, v_set\n\n\np_exact,v_set = p_s((n_v_r, n_v_c), T)\np_model_before_train = p_model(W, v_bias, h_bias, v_set)",
    "crumbs": [
      "Course",
      "Restricted Boltzmann Machines"
    ]
  },
  {
    "objectID": "course/rbm.html#training-with-stochastic-gradient-descent",
    "href": "course/rbm.html#training-with-stochastic-gradient-descent",
    "title": "Restricted Boltzmann Machines",
    "section": "3.4 Training with Stochastic Gradient Descent",
    "text": "3.4 Training with Stochastic Gradient Descent\nTo train the model, need to compute the gradient of the KL divergence between the probability distribution of our data and the probability distribution defined by the RBM. Then, we can perform a stochastic gradient descent (SGD) update of our parameters.\n\nbatch_size = 200\nnum_batches = int(data_set_size/batch_size)\nnum_epochs = 5\nlearning_rate = 0.2\nk = 5\n\n\ndef sigmoid(x):\n    return 1./(1+np.exp(-x))\n\ndef train(W, v_bias, h_bias, k, data_set, batch_size, num_batches, num_epochs, learning_rate):\n    #Gradient Descent Arrays\n    dW = np.zeros_like(W)\n    dv_bias = np.zeros_like(v_bias)\n    dh_bias = np.zeros_like(h_bias)\n\n    #Gibbs Sampling Arrays\n    v_0 = np.empty_like(v_bias, dtype=int)\n    v_k = np.empty_like(v_bias, dtype=int)\n    h_0 = np.empty_like(h_bias, dtype=int)\n    h_k = np.empty_like(h_bias, dtype=int)\n\n    num_data = data_set.shape[0]\n    start = np.random.randint(num_data)\n\n    for i_epoch in range(num_epochs):\n        for i_batches in range(num_batches):\n            dW *= 0.\n            dv_bias *= 0.\n            dh_bias *= 0.\n            for i_input in range(batch_size):\n                # gibbs sample\n                v_0 = data_set[(i_input + start + batch_size*i_batches) % num_data,:,:]\n                v_k, h_0, h_k = gibbs_sampling(k, v_0, W, v_bias, h_bias)\n\n                # update dW, dv_bias, ...\n                dW += np.tensordot(v_0, h_0,axes=0) - np.tensordot(v_k, h_k,axes=0)\n                dv_bias += v_0 - v_k\n                dh_bias += h_0 - h_k\n            \n            # Update parameters \n            np.copyto(W, W + dW * learning_rate/batch_size)\n            np.copyto(v_bias, v_bias + dv_bias * learning_rate/batch_size)\n            np.copyto(h_bias, h_bias + dh_bias * learning_rate/batch_size)\n            \n            if (i_batches+1) % 100 == 0:\n                print(\"Epoch: %d/%d\"%(i_epoch+1, num_epochs),\n                      ' Batch: %d/%d'%(i_batches+1, num_batches))\n    \n    return W, v_bias, h_bias\n\nCall the training algorithm and then construct the probability distribution of each spin configuration based on the weights and biases in the RBM.\n\n# Train\nW, v_bias, h_bias = train(W, v_bias, h_bias, k, data_set, batch_size, num_batches, num_epochs, learning_rate)\n\n# Find Probability distribution of states\np_model_after_train = p_model(W, v_bias, h_bias, v_set)\n\nEpoch: 1/5  Batch: 100/500\nEpoch: 1/5  Batch: 200/500\nEpoch: 1/5  Batch: 300/500\nEpoch: 1/5  Batch: 400/500\nEpoch: 1/5  Batch: 500/500\nEpoch: 2/5  Batch: 100/500\nEpoch: 2/5  Batch: 200/500\nEpoch: 2/5  Batch: 300/500\nEpoch: 2/5  Batch: 400/500\nEpoch: 2/5  Batch: 500/500\nEpoch: 3/5  Batch: 100/500\nEpoch: 3/5  Batch: 200/500\nEpoch: 3/5  Batch: 300/500\nEpoch: 3/5  Batch: 400/500\nEpoch: 3/5  Batch: 500/500\nEpoch: 4/5  Batch: 100/500\nEpoch: 4/5  Batch: 200/500\nEpoch: 4/5  Batch: 300/500\nEpoch: 4/5  Batch: 400/500\nEpoch: 4/5  Batch: 500/500\nEpoch: 5/5  Batch: 100/500\nEpoch: 5/5  Batch: 200/500\nEpoch: 5/5  Batch: 300/500\nEpoch: 5/5  Batch: 400/500\nEpoch: 5/5  Batch: 500/500\n\n\n\ndef KL_div(exact, model):\n    A = 0.\n    for i in range(len(exact)):\n        A +=  exact[i] * np.log(exact[i]/model[i])\n    return A\n\n\n\nCode\nKL_b = KL_div(p_exact, p_model_before_train)\nKL_a = KL_div(p_exact, p_model_after_train)\n\nprint('KL Divergence')\nprint('Before Training:', KL_b)\nprint('After Training:', KL_a)\n\n#Plot the probability distributions\nplt.figure(figsize=(16, 10))\n\nplt.plot(p_exact, 'k--',label='Exact')\n#plt.plot(p_model_before_train, label='Before Training: KL_Div:%.4f'%KL_b)\nplt.plot(p_model_after_train, label='After Training:KL_Div:%.4f'%KL_a)\n\nplt.yscale('log')\nplt.ylabel(\"Probability\")\nplt.xlabel(\"Spin Configuration\")\nplt.grid(True)\nplt.legend(loc='best')\nplt.show()\n\n\nKL Divergence\nBefore Training: 3.467045765617308\nAfter Training: 0.13813080305017178\n\n\n\n\n\n\n\n\n\n\nSources\n\n\n\nTorlai, Giacomo, and Roger G. Melko. “Learning thermodynamics with Boltzmann machines.” Physical Review B 94.16 (2016): 165134.\n\n\nHinton, Geoffrey E. “Training products of experts by minimizing contrastive divergence.” Neural computation 14.8 (2002): 1771-1800.\n\n\nMorningstar, Alan, and Roger G. Melko. “Deep learning the Ising model near criticality.” arXiv preprint arXiv:1708.04622 (2017).",
    "crumbs": [
      "Course",
      "Restricted Boltzmann Machines"
    ]
  },
  {
    "objectID": "course/applications/applications-cv.html#multi-label-classification",
    "href": "course/applications/applications-cv.html#multi-label-classification",
    "title": "Typical tasks in computer vision",
    "section": "2.1 Multi-label classification",
    "text": "2.1 Multi-label classification\nWithin image classificaiton we, sometimes, encounter applications in which, rather than assigning a single label to each image, we need to provide a list of labels. This is known as multi-label classification and it is typically applied in situations in which we need to enumerate certain categories that appear in the image.\nI find it pretty intuitive to understand this kind of tasks with the analogy of a kid to whom we ask “what do you see in this image?” and the kid enumerates every single thing it can recognize in it: a tree, a dog, the sun, a lake, grass, a house, etc. Nonetheless, it will not be able to tell us things that it does know yet, such as the brand name of the car, or the name of a constellation in a night sky. In this case, the machine will be our kid and we will tell it exactly which things to identify in the images.\nTo provide an example, we use the PASCAL dataset. Let’s see how it looks like.\n\n\nCode\npath = untar_data(URLs.PASCAL_2007)\ndf = pd.read_csv(path/'train.csv')\ndf.head()\n\n\n\n\n\n\n\n\n\nfname\nlabels\nis_valid\n\n\n\n\n0\n000005.jpg\nchair\nTrue\n\n\n1\n000007.jpg\ncar\nTrue\n\n\n2\n000009.jpg\nhorse person\nTrue\n\n\n3\n000012.jpg\ncar\nFalse\n\n\n4\n000016.jpg\nbicycle\nTrue\n\n\n\n\n\n\n\nWe have a list of images with assigned labels and an indicator telling whether the image belongs to the validation set or not. See that the third image has a label ‘horse person’. It is not a centaur, it’s just two labels: horse and person. Let’s have a look at the image.\n\n\nCode\nf = df.iloc[2, 0]\nimg = Image.open(path/\"train\"/f)\nimg\n\n\n\n\n\n\n\n\n\nIndeed, there is a horse and three people. Notice that the task does not involve recognizing how many elements of a certain category are there. It is quite close to ticking a checkbox list of categories. In this dataset there are 20 categories:\n\n\nCode\nlabels = set()\nfor L in df.labels.unique(): labels.update(L.split(' '))\nprint(labels)\n\n\n{'chair', 'cow', 'aeroplane', 'cat', 'pottedplant', 'person', 'bus', 'diningtable', 'sheep', 'tvmonitor', 'sofa', 'train', 'bird', 'horse', 'dog', 'bottle', 'boat', 'bicycle', 'motorbike', 'car'}\n\n\nHence, among those, the associated categories to the image are ‘horse’ and ‘person’. Let us have a look at some more examples to get an idea of the kind of images that we encounter.\n\n\nCode\ndef get_x(r): return path/'train'/r['fname']\ndef get_y(r): return r['labels'].split(' ')\n\ndef splitter(df):\n    train = df.index[~df['is_valid']].tolist()\n    valid = df.index[df['is_valid']].tolist()\n    return train, valid\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x, \n                   get_y=get_y,\n                   item_tfms = RandomResizedCrop(128, min_scale=0.35))\ndls = dblock.dataloaders(df)\n\ndls.show_batch()\n\n\n\n\n\n\n\n\n\nUnlike in the previous task, where all images had either dogs or pets as main body, in this case, we encounter a wide range of different images going from close portraits to general landscape views with many different objects in them. Nonetheless, we will do the same as in the previous example: take a pre-trained model and adapt it to this specific task.\n\n\n\n\n\n\nExpand to learn about the training procedure\n\n\n\n\n\nIn this case, we take a resnet50, which is larger than the previous resnet34. The architecture is also pre-trained in the Imagenet dataset and we fine tune it for this multi-label classification. The output layer now contains 20 neurons indicating whether each category appears in the sample.\n\n\n\nLet’s train!\n\n\nCode\nlearn = vision_learner(dls, resnet50, metrics=[accuracy_multi])\nlearn.fine_tune(3, base_lr=3e-3, freeze_epochs=4)\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n0.940069\n0.709672\n0.613805\n00:06\n\n\n1\n0.820035\n0.558362\n0.734721\n00:06\n\n\n2\n0.598636\n0.201305\n0.949064\n00:06\n\n\n3\n0.356319\n0.124212\n0.957171\n00:06\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n0.135804\n0.114497\n0.959801\n00:07\n\n\n1\n0.117852\n0.107067\n0.962769\n00:07\n\n\n2\n0.097339\n0.101569\n0.964263\n00:07\n\n\n\n\n\nIn less than a minute of training, we are capable of providing all the categories appearing in the given images with an accuracy of ~96%! Let’s have a look at some examples in the validation set.\n\n\nCode\nlearn.show_results()\n\n\n\n\n\n\n\n\n\n\n\n\nThe model has been quite confused by the fourth image in which it had to predict bicycle and pottedplant and, instead, it predicted chair and diningtable. This was a tricky one. Nonetheless, the model has failed in much simpler ones, e.g., in the hrose image, it has predicted two categories: horse and dog, and, in the last one, it has predicted it to be a train, while it had to say bicycle. Despite the confusion, we can visualize the model failures and understand, for instance, how it confused the last image with a train. Hence, we can see that it works consistently with some room for improvement.",
    "crumbs": [
      "Course",
      "Machine learning application overview",
      "Typical tasks in computer vision"
    ]
  },
  {
    "objectID": "course/applications/applications-cv.html#image-segmentation",
    "href": "course/applications/applications-cv.html#image-segmentation",
    "title": "Typical tasks in computer vision",
    "section": "2.2 Image segmentation",
    "text": "2.2 Image segmentation\nIn the image classification tasks that we have tackled so far, we have related classes with whole images, e.g., telling which pet breed appears in an image, or whether it contains any of a bunch of categories like horse and a person. We can go a step further and assign a label to each pixel to identify certain parts of the image. This is known as image segmentation.\nThis technique has numerious applications across very distinct fields. For instance, in autonomous driving we have to tell which parts of the image are road, traffic signs, pedestrians, etc. On a completely different approach, in biomedical imaging, segmentation is used to tell appart healthy tissue from regions affected by certain diseases, such as identifying tumorous cells among healthy ones.\nHere we will show a segmentation example using a subset of the CamVid dataset for autonomous driving. Let us have a look at some examples to get a better understanding of the task at hand.\n\n\nCode\npath = untar_data(URLs.CAMVID_TINY)\ndls = SegmentationDataLoaders.from_label_func(\n    path, bs=8, fnames = get_image_files(path/\"images\"),\n    label_func = lambda o: path/'labels'/f'{o.stem}_P{o.suffix}',\n    codes = np.loadtxt(path/'codes.txt', dtype=str))\n\ndls.show_batch()\n\n\n\n\n\n\n\n\n\nEach pixel in the images is assigned a label indicating whether it is a tree, a traffic sign, a car, a bike, a building, a pedestrian, etc. Here, we see the images overlapped with the color-coded label mask to ease the visualization. The goal is, given an image, generate the color-coding mask, that is, another image. Again, we will leverage a pre-trained model to perform this task.\n\n\n\n\n\n\nExpand to learn about the training procedure\n\n\n\n\n\nJust as in the previous cases, we take the same pre-trained resnet34 and adapt it for our task. In this case, we not only limit ourselves to change the last layer of the network but, also, we also modify its architecture. We use the weights from the pre-trained network and convert it into a U-Net (Ronneberger, Fischer, and Brox 2015).\n\n\n\nLet’s train!\n\n\nCode\nlearn = unet_learner(dls, resnet34)\nlearn.fine_tune(13)\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n3.528453\n2.278538\n00:02\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n1.917065\n1.561294\n00:02\n\n\n1\n1.638206\n1.185067\n00:02\n\n\n2\n1.489484\n1.176791\n00:02\n\n\n3\n1.446579\n1.413508\n00:02\n\n\n4\n1.363572\n1.037536\n00:02\n\n\n5\n1.251376\n0.994358\n00:02\n\n\n6\n1.151080\n0.793147\n00:02\n\n\n7\n1.053900\n0.740798\n00:02\n\n\n8\n0.969674\n0.706727\n00:02\n\n\n9\n0.896650\n0.705011\n00:02\n\n\n10\n0.833096\n0.699626\n00:02\n\n\n11\n0.780772\n0.693014\n00:02\n\n\n12\n0.738692\n0.689372\n00:02\n\n\n\n\n\nGiven that the dataset is rather small, the training is ridiculously fast. Here, we do not have a metric, such as error rate, that allows us to get an idea of the overall performance. Therefore, we will ask the model to generate the classification mask for some images and see how it goes in a rather qualitative way.\n\n\nCode\nlearn.show_results(max_n=4, figsize=(10, 8))\n\n\n\n\n\n\n\n\n\n\n\n\nOn the left, we see the real color-coding and, on the right, the model prediction. We can see that all the buildings, trees, cars and traffic signs are consistently colored, so the model is doing a great work here. The most difficult part seems to be the identification of road lines as well as accurately defining the shapes. The model has room for improvement and it would certainly perform better with a larger training dataset.\nBeware, though, the model has entirely missed a cyclist in the first image, WATCH OUT!!",
    "crumbs": [
      "Course",
      "Machine learning application overview",
      "Typical tasks in computer vision"
    ]
  },
  {
    "objectID": "course/applications/applications-nlp.html",
    "href": "course/applications/applications-nlp.html",
    "title": "Typical tasks in natural language processing",
    "section": "",
    "text": "Note\n\n\n\nIn this notebook we make extensive use of transfer learning, a technique that we will explain in a couple of lessons.\nFurthermore, we use the fastai (Howard and Gugger 2020) library to download the data for the different tasks and easily train our models.\n\n\n\n1 Introduction\nNatural language processing (NLP) is the field of machine learning that handles the interaction with spoken or written language as us, humans, use it. Hence, it is most likely the field that makes more notable the advances in artificial intelligence, as it is our interface with modern machines in our daily lifes. It does not matter that our phone can run the most advanced algorithms if, when we ask for a simple thing, it does not understand us at all or it replies with 70s-robotic-voice. On the other hand, even if the internal processing is not the most advanced, having a smooth interaction makes us feel that whatever artificial intelligence lies within is much more advanced.\nNLP covers many aspects, featuring speech recognition (voice to text), semantic analysis, text generation and text to speech, among others. Here, we will mainly focus on text processing to illustrate a few representative tasks.\nWhile computer vision has been a long established field overcoming human performance, the advances in NLP are much more recent. Contributions in the last few years with algorithms such as ULMFiT, predecessor of the GPT series (GPT, GPT-2 and GPT-3) have brought the field of NLP a step forward. The main idea behind these works consist on, first, training a generative model that ‘understands’ the language involved in the task without any specific goal. Then, we can leverage its knowledge to tackle the specific task at hand. In some cases, we do not even need to train our model any further despite it not being specifically trained for the task!\nWe will dive deeper into generative modeling and the current state of the art in the last lecture of the course :D\n\n\n2 Language modeling\nLanguage models are generative algorithms used to create text given a context. Intuitively, these models learn to reproduce the content they read. They have countless wonderful applications, such as this Trump tweet generator, which writes tweets as if it was Donald Trump.\nWe will illustrate the process of language modeling writing movie reviews using the IMDB dataset. We generate text by asking the model to infer what the next word will be given a text fragment. This way, providing the model with a starting point, we add the predicted word to the text and, recursively, repeat the whole process to write a full text.\n\n\n\n\n\n\nExpand to learn about the training procedure\n\n\n\n\n\nJust like in the computer vision examples, we will leverage transfer learning. As starting point, we take a generative model that has been trained with the text data from wikipedia, which already comes with great knowledge of our world. Such a pre-trained model is highly versatile as, in our application case, movies can feature historical phenomena or specific events and characters that a model with the knowledge from wikipedia will already know. For instance, when the name of an actor appears in a review, the model will already know who it is and other things it has done or, even more, the model will infer the content of the movie out of certain featured names.\n\n\n\nLet’s have a look at the data!\n\n\nCode\npath = untar_data(URLs.IMDB)\nget_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n\ndls_lm = DataBlock(blocks=TextBlock.from_folder(path, is_lm=True),\n                   get_items=get_imdb, splitter=RandomSplitter(0.1)\n                   ).dataloaders(path, path=path, bs=128, seq_len=80)\n\ndls_lm.show_batch(max_n=2)\n\n\n\n\n\n\ntext\ntext_\n\n\n\n\n0\nxxbos xxmaj this film starts as it ends and ends as it starts . xxmaj what is in the middle is a collection of comedy , philosophy , music , observations , commentaries , mini stories , colour and lots more . xxmaj it looks at the world and our lives and tells xxmaj the xxmaj monkees story from the view of the group members themselves . xxmaj it also looks at television and film and makes a visual commentary\nxxmaj this film starts as it ends and ends as it starts . xxmaj what is in the middle is a collection of comedy , philosophy , music , observations , commentaries , mini stories , colour and lots more . xxmaj it looks at the world and our lives and tells xxmaj the xxmaj monkees story from the view of the group members themselves . xxmaj it also looks at television and film and makes a visual commentary .\n\n\n1\nm - g - m . xxmaj with focused narration and voice - tapes from others in xxmaj judy 's life , director xxmaj susan xxmaj lacy is able to put into perspective the ill - fated early marriages , the on - set troubles , and the succession of comebacks . xxmaj still , once xxmaj lacy makes headway into xxmaj garland 's fascinating movie career , she has a tendency to then double - back and give us\n- g - m . xxmaj with focused narration and voice - tapes from others in xxmaj judy 's life , director xxmaj susan xxmaj lacy is able to put into perspective the ill - fated early marriages , the on - set troubles , and the succession of comebacks . xxmaj still , once xxmaj lacy makes headway into xxmaj garland 's fascinating movie career , she has a tendency to then double - back and give us more\n\n\n\n\n\nNotice that the text is tokenized including special symbols such as xxbos, indicating the beginning of sentence, xxmaj indicating that the next word starts with a capital letter, and so on. Truth is that the model does not understand words as they are, but rather it uses a representation of the words in a high dimensional mathematical space.\n\n\n\n\n\n\nExpand to learn about the training procedure\n\n\n\n\n\nLooking at the data, we see that the text is split into fragments that are shifted by one word between one another. This is because the target of the text on the left is its next word, that is, the text on the right. We use a recurrent neural network (RNN) to which the text is recursivelly passed word by word and, at every step, the target is the corresponding word on the right. Using the pre-traiend wikipedia RNN, we already have an embedding representing most of the English words and the network knows how to relate them. We just need to fine-tune the RNN in order to specialize it in the movie review context.\nNotice that training these models takes much longer than training the computer vision or tabular data ones. This is because of the nature of the RNN architecture, that loops over each word of the text and… well, Python does not like loops :)\n\n\n\nLet’s train!\n\n\nCode\nlearn = language_model_learner(dls_lm, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()]).to_fp16()\nlearn.fine_tune(10, base_lr=2e-2)\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n4.141006\n4.018666\n0.290516\n55.626865\n11:10\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n3.806848\n3.775722\n0.312664\n43.628998\n10:39\n\n\n1\n3.769517\n3.749914\n0.314977\n42.517441\n10:32\n\n\n2\n3.728264\n3.719217\n0.319099\n41.232098\n10:31\n\n\n3\n3.677289\n3.685531\n0.323200\n39.866291\n10:32\n\n\n4\n3.614626\n3.654452\n0.327608\n38.646320\n10:32\n\n\n5\n3.552478\n3.628734\n0.330706\n37.665115\n10:35\n\n\n6\n3.487630\n3.608381\n0.333443\n36.906254\n10:38\n\n\n7\n3.422164\n3.594230\n0.335815\n36.387676\n10:36\n\n\n8\n3.359136\n3.591810\n0.336942\n36.299736\n10:41\n\n\n9\n3.309520\n3.595514\n0.336873\n36.434418\n10:36\n\n\n\n\n\nThe model correctly predicts the next word more than a third of the times. The number may not be outstanding compared with the accuracies that we have obtained in the computer vision tasks, but think about its meaning: the model correctly infers what the next word of any arbitrary review will be one out of every three guesses.\nLet’s now ask it to make some movie reviews for us. We provide it with the start of a sentence and see where it goes. Let’s start with ‘I liked this movie because’:\n\n\nCode\nn_words = 50\nstem = \"I liked this movie because\"\npred = learn.predict(stem, n_words, temperature=0.75)\nprint(pred)\n\n\n\n\n\ni liked this movie because it was one of the best thrillers of the 80 's . The acting was above average and the plot was much more interesting . \n\n There are cuts to this movie that make it seem like it was shot with a camcorder , but i think that\n\n\nSee what happens with ‘Such a terrible movie should never’:\n\n\nCode\nstem = \"Such a terrible movie should never\"\npred = learn.predict(stem, n_words, temperature=0.75)\nprint(pred)\n\n\n\n\n\nSuch a terrible movie should never have been made . The director of this film , Paul Anderson , should be fired . So there are some OTHER reasons to waste any time on this flick . i have to disagree with the other comments . If you want to\n\n\nThese are some hilarious reviews! All of them make sense and are more or less coherent. For instance, when we provide the model with a positive beginning, ‘I liked this movie because’, the movie review tries to explain the reasons why it was so good. On the other hand, when we provide a negative starting point, ‘Such a terrible movie should never’, the review rips the movie beefing its director.\nNotice that, in these two reviews, the model has included names such as Paul Anderson (despite him being an actor and not a director), concepts like camcorder or thriller of the 80’s and it has even emphasized the word ‘OTHER’ with capital letters.\nThe dataset is made out of highly polarized movie reviews ando so the model has an easy time writing positive or negative reviews. Let’s see where it takes us whenever we provide it with an open start like ‘This movie about’:\n\n\nCode\nstem = \"This movie about \"\npred = learn.predict(stem, n_words, temperature=0.75)\nprint(pred)\n\n\n\n\n\nThis movie about a number of things that do n't happen to us , most of them are not exciting . The main problem with the movie is the fact that it is more about life than about the life of others . The main character , Bayliss , is\n\n\n\n\nCode\nstem = \"This movie about \"\npred = learn.predict(stem, n_words, temperature=0.75)\nprint(pred)\n\n\n\n\n\nThis movie about a small town , Iowa , is about a young man ( bill Paxton ) who is right about to go to school with a girl he met and he falls in love with her ( deborah Murray ) . She is also a photographer ,\n\n\nWith the same starting point, the first review is negative straight away, but the second remains neutral for the limited size that we have provided.\nNow that we have this powerful language model, we can use it knowledge to address other tasks involving movie reviews, so we will save the main body of the model for later.\n\n\nCode\nlearn.save_encoder(\"imdb_encoder\")\n\n\n\n\n3 Text classification\nOne of the most extended applications of NLP is text classification, which consists on assigning categories to pieces of text. This is highly related with “understanding” texts in artificial intelligence pipelines and data mining. For instance, we can use these classifiers to automatically sort scientific publications into their respective fields, e.g. condensed matter, neurology, …, and similar tasks. We can also use these models to find whether customer feedback is positive or negative at a large scale, separate fake news from real ones, or even tell the native language of the writer from a text in English.\nIn order to illustrate a case, we will continue with the same application example as before, leveraging the IMDB dataset, to assess whether movie reviews are positive or negative. Notice that, despite using the same dataset, the task is completely different. During the training of the langauge model, the target was the same bit of thext shifted by one word. Now, the target is a label indicating whether the review is positive or negative, as we see below.\n\n\nCode\npath = untar_data(URLs.IMDB)\n\ndls_clas = DataBlock(\n    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab), CategoryBlock),\n    get_y = parent_label,\n    get_items=partial(get_text_files, folders=['train', 'test']),\n    splitter=GrandparentSplitter(valid_name='test')\n).dataloaders(path, path=path, bs=128, seq_len=72)\n\ndls_clas.show_batch(max_n=5)\n\n\n\n\n\n\ntext\ncategory\n\n\n\n\n0\nxxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero\npos\n\n\n1\nxxbos xxmaj titanic directed by xxmaj james xxmaj cameron presents a fictional love story on the historical setting of the xxmaj titanic . xxmaj the plot is simple , xxunk , or not for those who love plots that twist and turn and keep you in suspense . xxmaj the end of the movie can be figured out within minutes of the start of the film , but the love story is an interesting one , however . xxmaj kate xxmaj winslett is wonderful as xxmaj rose , an aristocratic young lady betrothed by xxmaj cal ( billy xxmaj zane ) . xxmaj early on the voyage xxmaj rose meets xxmaj jack ( leonardo dicaprio ) , a lower class artist on his way to xxmaj america after winning his ticket aboard xxmaj titanic in a poker game . xxmaj if he wants something , he goes and gets it\npos\n\n\n2\nxxbos xxrep 3 * xxmaj warning - this review contains \" plot spoilers , \" though nothing could \" spoil \" this movie any more than it already is . xxmaj it really xxup is that bad . xxrep 3 * \\n\\n xxmaj before i begin , xxmaj i 'd like to let everyone know that this definitely is one of those so - incredibly - bad - that - you - fall - over - laughing movies . xxmaj if you 're in a lighthearted mood and need a very hearty laugh , this is the movie for you . xxmaj now without further ado , my review : \\n\\n xxmaj this movie was found in a bargain bin at wal - mart . xxmaj that should be the first clue as to how good of a movie it is . xxmaj secondly , it stars the lame action\nneg\n\n\n3\nxxbos xxmaj jim xxmaj carrey is back to much the same role that he played in xxmaj the xxmaj mask , a timid guy who is trying to get ahead in the world but who seems to be plagued with bad luck . xxmaj even when he tries to help a homeless guy from being harassed by a bunch of hoodlums ( and of course they have to be xxmaj mexican , obviously ) , his good will towards his fellow man backfires . xxmaj in that case , it was n't too hard to predict that he was about to have a handful of angry hoodlums , but i like that the movie suggests that things like that should n't be ignored . xxmaj i 'm reminded of the episode of xxmaj michael xxmaj moore 's brilliant xxmaj the xxmaj awful xxmaj truth , when they had a man\npos\n\n\n4\nxxbos xxup the xxup shop xxup around xxup the xxup corner is one of the sweetest and most feel - good romantic comedies ever made . xxmaj there 's just no getting around that , and it 's hard to actually put one 's feeling for this film into words . xxmaj it 's not one of those films that tries too hard , nor does it come up with the oddest possible scenarios to get the two protagonists together in the end . xxmaj in fact , all its charm is innate , contained within the characters and the setting and the plot … which is highly believable to boot . xxmaj it 's easy to think that such a love story , as beautiful as any other ever told , * could * happen to you … a feeling you do n't often get from other romantic comedies\npos\n\n\n\n\n\nLet’s take the previous language model that we had trained to generate movie reviews, which knows quite a lot of the world thanks to wikipedia, and quite a bit more of movies, as starting point for our classifier.\n\n\n\n\n\n\nExpand to learn about the training procedure\n\n\n\n\n\nGiven the language model, we now add a fully connected layer at the back. The language model acts as feature extractor of the text, which feeds a dense classifier that outputs the probability of belonging to each class: positive or negative. Just as in computer vision, we start by freezing the pre-trained part of the model and then we proceed to unfreeze it once the training has advanced. In this case, however, we will gradually unfreeze the different layers of the model, from back to front, rather than unfreezing it all at once. Additionally, we will be using discriminative learning rates for the whole process.This process of taking a generic language model (wikipedia), fine-tunning it to our task (movie reviews) and, finally, using it as a classifier is the core of ULMFiT, as previously hinted.\n\n\n\nLet’s train!\n\n\nCode\nlearn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, \n                                metrics=accuracy).to_fp16()\nlearn = learn.load_encoder('imdb_encoder')\n\nlearn.fit_one_cycle(1, 2e-2)\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.356537\n0.200132\n0.920920\n00:25\n\n\n\n\n\n\n\nCode\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2/(2.6**4), 1e-2))\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.277847\n0.178657\n0.931400\n00:30\n\n\n\n\n\n\n\nCode\nlearn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3/(2.6**4), 5e-3))\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.211942\n0.158949\n0.941560\n00:38\n\n\n\n\n\n\n\nCode\nlearn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3/(2.6**4), 1e-3))\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.181801\n0.154084\n0.942280\n00:46\n\n\n1\n0.159143\n0.157385\n0.944000\n00:47\n\n\n\n\n\nWe reach an accuracy of ~94% in less than 4 minutes of training. This is mainly thanks to using our pre-trained language model as a feature extractor, which can extract rich information from the movie reviews to create a wonderful classifier.\nLet’s see some examples.\n\n\nCode\nlearn.show_results(max_n=4)\n\n\n\n\n\n\n\n\n\ntext\ncategory\ncategory_\n\n\n\n\n0\nxxbos xxmaj there 's a sign on xxmaj the xxmaj lost xxmaj highway that says : \\n\\n * major xxup spoilers xxup ahead * \\n\\n ( but you already knew that , did n't you ? ) \\n\\n xxmaj since there 's a great deal of people that apparently did not get the point of this movie , xxmaj i 'd like to contribute my interpretation of why the plot makes perfect sense . xxmaj as others have pointed out , one single viewing of this movie is not sufficient . xxmaj if you have the xxup dvd of xxup md , you can \" cheat \" by looking at xxmaj david xxmaj lynch 's \" top 10 xxmaj hints to xxmaj unlocking xxup md \" ( but only upon second or third viewing , please . ) ;) \\n\\n xxmaj first of all , xxmaj mulholland xxmaj drive is\npos\npos\n\n\n1\nxxbos xxmaj back in the mid / late 80s , an xxup oav anime by title of \" bubblegum xxmaj crisis \" ( which i think is a military slang term for when technical equipment goes haywire ) made its debut on video , taking inspiration from \" blade xxmaj runner \" , \" the xxmaj terminator \" and maybe even \" robocop \" , with a little dash of xxmaj batman / xxmaj bruce xxmaj wayne - xxmaj iron xxmaj man / xxmaj tony xxmaj stark and xxmaj charlie 's xxmaj angel 's girl power thrown in for good measure . 8 episodes long , the overall story was that in 21st century xxmaj tokyo , xxmaj japan , year xxunk - xxunk , living machines called xxmaj boomers were doing manual labor and sometimes cause problems . a special , xxup swat like branch of law enforcers ,\npos\npos\n\n\n2\nxxbos xxmaj if anyone ever assembles a compendium on modern xxmaj american horror that is truly worth it 's salt , there will * have * to be an entry for xxup sf xxmaj brownrigg 's xxunk xxunk in xxmaj asylum xxmaj horror . xxmaj every time i watch this movie i am impressed by the complete economy of the film , from the compact , totally self - contained plot with a puzzling beginning and an all too horrible ending , the engaging performances by what was essentially a group of non - professional actors , and a xxunk sense of dread and claustrophobia that effectively consumes the xxunk with a certain inevitability which is all the more terrifying because the viewers know what is going on long before the xxunk ] , with the only question being when are they going to wake up & smell the coffee\npos\npos\n\n\n3\nxxbos xxmaj tony xxmaj hawk 's xxmaj pro xxmaj skater 2x , is n't much different at all from the previous games ( excluding xxmaj tony xxmaj hawk 3 ) . xxmaj the only thing new that is featured in xxmaj tony xxmaj hawk 's xxmaj pro xxmaj skater 2x , is the new selection of levels , and tweaked out graphics . xxmaj tony xxmaj hawk 's xxmaj pro xxmaj skater 2x offers a new career mode , and that is the 2x career . xxmaj the 2x career is basically xxmaj tony xxmaj hawk 1 career , because there is only about five challenges per level . xxmaj if you missed xxmaj tony xxmaj hawk 1 and 2 , i suggest that you buy xxmaj tony xxmaj hawk 's xxmaj pro xxmaj skater 2x , but if you have played the first two games , you should still\npos\npos\n\n\n\n\n\nWe can even get the classifier to tell whether our reviews are positive or negative.\n\nreview = \"This movie is fucking awful\"\nlearn.predict(review)\n\n\n\n\n('neg', TensorText(0), TensorText([0.9409, 0.0591]))\n\n\n\nreview = \"Such a wonderful movie!!!\"\nlearn.predict(review)\n\n\n\n\n('pos', TensorText(1), TensorText([9.0573e-04, 9.9909e-01]))\n\n\nFrom left to right, we see the category ‘neg’ or ‘pos’, its numerical equivalent (0 or 1) and the tensor telling the machine confidence for each class. In both cases, the model is pretty certain of the class it is predicting: 0.941 negative and 0.999 positive for each review, respectively. Let’s try with something that is less obvious.\n\nreview = (\"At first, this movie looked great and eganging. I was having a great time. Nonetheless,\"+\n          \" the last half turned out to be complete boring and worthless.\")\nlearn.predict(review)\n\n\n\n\n('neg', TensorText(0), TensorText([0.9985, 0.0015]))\n\n\nHere, we introduced a turning point in the review and the model got it pretty well. Let’s try with something a bit more neutral or ambiguous.\n\nreview = \"The movie is about a penguin's life. I guess it is somewhat informative, at least.\"\nlearn.predict(review)\n\n\n\n\n('pos', TensorText(1), TensorText([0.2826, 0.7174]))\n\n\nWe can see that in this last case, the model is much less confident about this being a positive review and the truth is that it can be interpreted in both ways. The movie can fall in the “not bad” category and we say that it is somewhat informative. However, it could also be that the movie is terrible to watch but that, at least, we get to learn something.\n\n\n4 Other natural language applications\nAs previously mentioned, NLP covers a massive range of applications. Directly related with what we’ve done so far, we could do some text regression tasks in which we aim to predict, for instance, a movie score out of the review. The process would be analogous to fine-tuning the text classifier.\nHowever, there are other completely different tasks that we can tackle. For example, we can translate a text to a different language, we can summarize it, we can extract a conceptual graph, etc.\n\n\n5 Sequence modeling\nSo far, we have seen computer vision applications exclusively involving images, and natural language processing applications involving only text. However, there are many tasks that involve various kinds of data at once. For example, we may be interested in writing captions that describe images. This is a language modeling task that takes an image as starting point. Conversely, we may be interested in generating images given a description, which is a computer vision generative task with a text as starting point.\nThis is performed by considering the data as a sequence of pieces of information all together. Just like the tokenization that we have performed for the text processing, we can come up with ways to tokenize our images. Combining both approaches, our models can then process the series of tokens regardless of their origin. There have recently been huge advances in this field. For example, flamingo is a chatbot that can process images, seamlesly integrating text and images in a conversation. Perhaps even more surprisingly, diffusion models such as DALLE-E can create high resolution images from arbitrary prompts.\nWe can download and play around with one of these models! For example, you can follow this tutorial to use the OpenAI API or this tutorial to use the huggingface stable diffusion models.\n\n\nCode\ndef generate_image(prompt):\n    \"Generates an image with the DALL-E2 API\"\n    response = openai.Image.create(\n        prompt=prompt, n=1, size=\"1024x1024\", response_format=\"b64_json\"\n    )\n    img = b64decode(response['data'][0]['b64_json'])\n    return Image.open(io.BytesIO(img))\n\n\nYou can use these references to draw inspiration for better prompts. Below, you will find some prompts provided by the students and the resulting images. Use with responsability!\n\ndescription = \"Cute and adorable walrus playing the banjo\"\ngenerate_image(description)\n\n\n\n\n\n\n\n\n\ndescription = \"a white siamese cat\"\ngenerate_image(description)\n\n\n\n\n\n\n\n\n\ndescription = \"Quantum master in barcelona in Gaudi trencadis\"\ngenerate_image(description)\n\n\n\n\n\n\n\n\n\ndescription = \"Shrek hyper realistic\"\ngenerate_image(description)\n\n\n\n\n\n\n\n\n\ndescription = \"gigachad homer hyper realistic\"\ngenerate_image(description)\n\n\n\n\n\n\n\n\n\ndescription = \"purple compass with the shape of a horse, neon light, perfect lighting\"\ngenerate_image(description)\n\n\n\n\n\n\n\n\n\ndescription = \"duck beaver fusion, creepy, horror, dark\"\ngenerate_image(description)\n\n\n\n\n\n\n\n\n\ndescription = \"asian Leonardo Dicaprio, steampunk\"\ngenerate_image(description)\n\n\n\n\n\n\n\n\n\ndescription = \"Pooh riding a comunist horse, for the people, military propaganda\"\ngenerate_image(description)\n\n\n\n\n\n\n\n\n\ndescription = \"Pooh riding a comunist horse, for the people, soviet poster\"\ngenerate_image(description)\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nHoward, Jeremy, and Sylvain Gugger. 2020. “Fastai: A Layered API for Deep Learning.” Information 11 (2). https://doi.org/10.3390/info11020108.",
    "crumbs": [
      "Course",
      "Machine learning application overview",
      "Typical tasks in natural language processing"
    ]
  },
  {
    "objectID": "course/generative/introduction.html#learning-the-data-probability-distribution",
    "href": "course/generative/introduction.html#learning-the-data-probability-distribution",
    "title": "Generative models",
    "section": "2.1 Learning the data probability distribution",
    "text": "2.1 Learning the data probability distribution\nThe task is to learn the underlying distribution \\(p_{\\text{data}}(x_i)\\) given a data set \\(\\left\\{x_i\\right\\}\\). We can do this either finding a model to approximate the probability distribution, \\(p_\\theta(x_i)\\approx p_{\\text{data}}\\), and then sample from it, or by training a model to generate new samples and then estimate \\(p_{\\text{data}}\\).\n\n\n\n\n\n\nNote\n\n\n\nJust because we can compute the probability it does not mean that we can sample easily and vice versa. In general, there is a trade-off between sampling and computing the probability.\n\n\nTo illustrate the main concepts, we will consider a toy model with samples \\(\\left\\{x_i\\right\\}\\) drawn from a mixture of two Gaussian distributions: \\(\\mathcal{N}_0(\\mu_0,\\sigma_0)\\) and \\(\\mathcal{N}_1(\\mu_1,\\sigma_1)\\). A common way to model mixture models is through a multinoulli distribution that assigns the probability to sample from each of the possible modes. Since in this case we only have two modes, we can use a Bernoulli distribution instead with \\(p(x)=\\phi^x(1-\\phi)^{1-x}\\), meaning that \\(p(x=1) = \\phi\\) and \\(p(x=0) = 1-\\phi\\).\n\n\n\n\n\n\nExercise\n\n\n\nDefine a function to sample from the Gaussian mixture described above. As input, it should take the desired number of samples GaussianMixture.sample(self, n_samples).\n\n\n\n\nCode\nclass GaussianMixture:\n    def __init__(self, phi, mu_0, std_0, mu_1, std_1):\n        \"\"\"Initialize a Gaussian mixture with two modes. `phi` denotes\n        the probability to sample from distribution 1.\"\"\"\n        self.phi = phi\n        self.mu_0, self.std_0 = mu_0, std_0\n        self.mu_1, self.std_1 = mu_1, std_1\n\n    def sample(self, n_samples):\n        \"Draw samples from a Gaussian mixture model.\"\n        which = np.random.uniform(size=n_samples) &lt; self.phi\n        samples_0 = np.random.normal(self.mu_0, self.std_0, n_samples)\n        samples_1 = np.random.normal(self.mu_1, self.std_1, n_samples)\n        return np.where(which, samples_1, samples_0)\n\n    def pdf(self, x):\n        \"Evaluate the Gaussian mixture pdf over x.\"\n        pdf_0 = self.gaussian_pdf(x, self.mu_0, self.std_0)\n        pdf_1 = self.gaussian_pdf(x, self.mu_1, self.std_1)\n        return (1-self.phi)*pdf_0 + self.phi*pdf_1\n    \n    @staticmethod\n    def gaussian_pdf(x, mu, std):\n        return np.exp(-(x-mu)**2/(2*std**2))/(std*np.sqrt(2*np.pi))\n\n\n\n2.1.1 Empirical distribution and histograms\nWe can see our data set \\(\\left\\{x_i\\right\\}\\) as a collection of samples drawn from the probability distribution \\(p_{\\text{data}}(x_i)\\). The empirical distribution, or Dirac delta distribution, specifies the probability distribution \\(\\hat{p}_{\\text{data}}(x_i)\\) from which we sample as we draw examples \\(x_i\\) from the data set. This way, it maximizes the likelihood of our training samples by construction.\nFor continuous variables, we define the empirical distribution as \\[\\hat{p}_{\\text{data}}(x)=\\frac{1}{m}\\sum_{i=1}^m \\delta(x - x_i)\\,\\] which puts the same probability mass \\(1/m\\) to every data point in a collection of \\(m\\) samples.\nFor discrete variables, however, we define the empirical probability to be the empirical frequency with which the value appears in the training set. This is what we typically visualize in normalized histograms. Indeed, histograms are one of the simplest generative models we can have!\nLet’s see an example with our Gaussian mixture model. First of all, we need to create some data from which we wish to learn the underlying distribution.\n\nphi, mu_0, std_0, mu_1, std_1 = 0.7, 5, 2, 20, 3\nsize_train, size_test = 500, 500\n\nnp.random.seed(0)\nmixture = GaussianMixture(phi, mu_0, std_0, mu_1, std_1)\nx_train = np.round(mixture.sample(size_train)).astype(int)\nx_test = np.round(mixture.sample(size_test)).astype(int)\n\n\n\n\n\n\n\nNote\n\n\n\nNotice tha we have rounded the outputs and converted them to integers. This is because the histograms represent the empirical distribution for discrete random variables.\n\n\nNow we can build the histogram of the training data by computing the empirical frequency of each value.\n\nvalues_train, counts_train = np.unique(x_train, return_counts=True)\nprobs_train = counts_train/counts_train.sum()\n\n\n\nCode\ndiscrete_pdf = lambda x: np.trapz(mixture.pdf(x), x) # Integrate pdf over value range\nhist_pdf = [discrete_pdf(np.linspace(val-0.5, val+0.5, 10)) for val in values_train]\nfig = go.Figure()\nfig.add_bar(x=values_train, y=probs_train, name=\"Histogram\")\nfig.add_trace(go.Scatter(name=\"pdf\", x=values_train, y=hist_pdf, mode='markers+lines'))\nfig.update_layout(xaxis_title='x', yaxis_title='probability', title='Training set')\n\n\n                                                \n\n\nWe can use this histogram as generative model to draw samples according to the empirical distribution of the training data.\n\ndef sample_histogram(n_samples, values, probs):\n    \"\"\"Draw samples from the probability distribution defined by the\n    histogram assigning normalized `probs` to `values`.\"\"\"\n    cumprobs = probs.cumsum()\n    samples = [values[cumprobs &gt;= np.random.uniform()][0]\n               for _ in range(n_samples)]\n    return np.array(samples)\n\n\nsample_histogram(10, values_train, probs_train)\n\narray([20,  5, 21, 23,  7, 25, 21, 15, 20, 16])\n\n\nWe can even make a histogram of the samples drawn form the histogram!\n\n\nCode\nsamples_hist = sample_histogram(2000, values_train, probs_train)\nvalues_hist, counts_hist = np.unique(samples_hist, return_counts=True)\nprobs_hist = counts_hist/counts_hist.sum()\nfig = go.Figure()\nfig.add_bar(x=values_hist, y=probs_hist, name=\"Histogram\")\nfig.add_trace(go.Scatter(name=\"pdf\", x=values_train, y=hist_pdf, mode='markers+lines'))\nfig.update_layout(xaxis_title='x', yaxis_title='probability',\n                  title='Histogram of the training histogram')\n\n\n                                                \n\n\nThe main issue with this approach is that we maximize the likelihood of the training data at the expense of heavily overfitting it. This generally results in terrible generalization to the test set. As we can see below, the histogram for the training set and the test set have some strong differences despite coming from the same underlying distribution. Thus, it is desirable to train a smoother model that can generalize better to unseen data.\n\n\nCode\nvalues_test, counts_test = np.unique(x_test, return_counts=True)\nprobs_test = counts_test/counts_test.sum()\n\nfig = go.Figure()\nfig.add_bar(x=values_test, y=probs_test, name=\"Histogram\")\nfig.add_trace(go.Scatter(name=\"pdf\", x=values_train, y=hist_pdf, mode='markers+lines'))\nfig.update_layout(xaxis_title='x', yaxis_title='probability', title='Test set')\n\n\n                                                \n\n\nEven though this can be mitigated by increasing the amount of data, this solution becomes unfeasable when we go to high-dimensional data and face the curse of dimensionality. For example, if we try to learn the probability distribution of the MNIST data set that we have used in previous lectures, even in its binarized form (black or white pixels), the data is \\(28\\times28\\)-dimensional meaning that there are \\(2^{784}\\sim10^{236}\\) possible configurations. Thus, even if every atom in the observable universe was a training sample (\\(\\sim10^{80}\\)), the resulting histogram would still be extremely sparse assigning null probability almost everywhere.\n\n\n2.1.2 Maximum likelihood estimation\nAs we have seen, it is desirable to find better solutions than the simple empirical distribution to model the underlying probability distribution of our data \\(p_{\\text{data}}\\). We can derive a parametrized estimator \\(p_{\\mathbf{\\theta}}\\approx p_{\\text{data}}\\) directly from the data following the maximum likelihood principle, which minimizes the distance between our model and the empirical distribution of the data: \\[\\mathbf{\\theta}^* = \\text{arg}\\,\\text{min}_{\\mathbf{\\theta}} D_{KL}(\\hat{p}_{\\text{data}}||p_{\\mathbf{\\theta}}) = \\text{arg}\\,\\text{min}_{\\mathbf{\\theta}} -\\mathbb{E}_{x\\sim\\hat{p}_{\\text{data}}}\\left[\\log p_{\\mathbf{\\theta}}(x)\\right]\\,.\\] We can recognize here the negative log-likelihood loss function that we have previously seen in the course, which is the cross entropy between the empirical distribution and the one defined by the model, as we introduced in the logistic regression section.\nThis is known as the maximum likelihood estimator (MLE) and it is the most statistically efficient estimator. This means that no other estimator achieves a lower mean squared error (MSE) than the MLE for a fixed number of samples. Furthermore, it is consistent, which guarantees that it converges to the true value as we increase the number of data points, under two conditions:\n\n\\(p_{\\text{data}}\\) lies within the hypothesis space of \\(p_{\\mathbf{\\theta}}\\).\n\\(p_{\\text{data}}\\) corresponds to a unique \\(\\mathbf{\\theta}\\).\n\n\n\n\n\n\n\nNote\n\n\n\nIntuitively, the MLE tries to maximize the probability of observing the samples in the training set. We would obtain the same estimator by taking \\[\\mathbf{\\theta}^* = \\text{arg}\\,\\text{max}_{\\mathbf{\\theta}} \\prod_{i=1}^m p_{\\mathbf{\\theta}}(x_i) = \\sum_{i=1}^m \\log p_{\\mathbf{\\theta}}(x_i)\\,.\\] This principle is applicable to conditional probability distributions with labeled data \\[\\mathbf{\\theta}^* = \\text{arg}\\,\\text{max}_{\\mathbf{\\theta}} \\sum_{i=1}^m p_{\\mathbf{\\theta}}(y_i|x_i)\\,,\\] from which we can derive the MSE loss for supervised learning tasks. Thus, the MSE provides the MLE!\n\n\nLet’s find the MLE for our toy example. We will cheat and already assume that our distribution follows a Gaussian mixture with two modes. First, we define the loss function for the training data. Since we deal with a fairly low amount of data, we can compute the loss for the whole training set at once.\n\ndef mle_train_loss(params):\n    phi, mu_0, std_0, mu_1, std_1 = params\n    pdf_0 = mixture.gaussian_pdf(x_train, mu_0, std_0)\n    pdf_1 = mixture.gaussian_pdf(x_train, mu_1, std_1)\n    log_likelihood = np.log((1-phi)*pdf_0 + phi*pdf_1)\n    return -np.mean(log_likelihood)\n\n\n\n\n\n\n\nNote\n\n\n\nTo be completely rigurous here, we should consider the discrete probability mass function, instead of a probability density function.\n\n\nNow we can simply use a scipy optimizer to find the minimum.\n\ninitial_parameters = np.array([0.5, 5., 3., 20., 3.])\nresult = minimize(mle_train_loss, x0=initial_parameters, bounds=[(0, 1), (0, 25), (0, 5), (0, 25), (0, 5)])\n\n\nresult.x\n\narray([ 0.72000346,  4.90814727,  1.91073172, 20.26624794,  2.84841709])\n\n\n\n\nCode\nprint(\"The parameters are:\")\nprint(f\"\\tGround truth: phi={mixture.phi:.2f}, mu_0={mixture.mu_0:.2f},\"+\n      f\" std_0={mixture.std_0:.2f}, mu_1={mixture.mu_1:.2f}, std_1={mixture.std_1:.2f}\")\nprint(f\"\\tEstimation:   phi={result.x[0]:.2f}, mu_0={result.x[1]:.2f},\"+\n      f\" std_0={result.x[2]:.2f}, mu_1={result.x[3]:.2f}, std_1={result.x[4]:.2f}\")\n\n\nThe parameters are:\n    Ground truth: phi=0.70, mu_0=5.00, std_0=2.00, mu_1=20.00, std_1=3.00\n    Estimation:   phi=0.72, mu_0=4.91, std_0=1.91, mu_1=20.27, std_1=2.85\n\n\nNot bad! We have obtained a good estimation of the underlying parameters of our data distribution. We see that the estimation of the second mode is a bit rougher than the first mode. However, looking at the data distribution, we can understand why the second distribution appears wider than it is.\nWe can compare the negative log likelihood loss for the MLE and the histogram in the train and test data sets.\n\n\nCode\np_hist = dict(zip(values_train, probs_train))\nnll_hist_train = -np.mean(np.log([p_hist.get(x, 1e-9) for x in x_train]))\nnll_hist_test = -np.mean(np.log([p_hist.get(x, 1e-9) for x in x_test]))\n# To evaluate the MLE over the discrete points we need to integrate around each value\ndef p_mle(x, params):\n    phi, mu_0, std_0, mu_1, std_1 = params\n    def _p(x):\n        pdf_0 = mixture.gaussian_pdf(x, mu_0, std_0)\n        pdf_1 = mixture.gaussian_pdf(x, mu_1, std_1)\n        return (1-phi)*pdf_0 + phi*pdf_1\n    xx = np.linspace(x-0.5, x+0.5, 20)\n    return np.trapz(_p(xx), xx)\nnll_mle_train = -np.mean(np.log([p_mle(x, result.x) for x in x_train]))\nnll_mle_test = -np.mean(np.log([p_mle(x, result.x) for x in x_test]))\nprint(\"Negative log-likelihood loss\")\nprint(\"      Train Test\")\nprint(f\"Hist: {nll_hist_train:.2f}  {nll_hist_test:.2f}\")\nprint(f\"MLE:  {nll_mle_train:.2f}  {nll_mle_test:.2f}\")\n\n\nNegative log-likelihood loss\n      Train Test\nHist: 2.91  3.12\nMLE:  2.95  2.97\n\n\nWe clearly see how the histogram outperforms the MLE in the training data, but it does not generalize well to the test data. In contrast, while the MLE has a lower performance during training, it generalizes well to the test data, keeping a small gap between train and test losses. This way, the MLE is the preferred choice.\nLooking at the resulting probability distributions below, we clearly see how the histogram overfit the training data with large spikes at \\(x=5\\) and \\(x=21\\).\n\n\nCode\nvalues_test, counts_test = np.unique(x_test, return_counts=True)\nprobs_test = counts_test/counts_test.sum()\nprobs_mle = [p_mle(x, result.x) for x in values_test]\nprobs_hist = [p_hist.get(x, 1e-9) for x in values_test]\n\nfig = go.Figure()\nfig.add_bar(x=values_test, y=probs_test, name=\"Histogram\")\nfig.add_trace(go.Scatter(name=\"pdf\", x=values_train, y=hist_pdf, mode='markers+lines'))\nfig.add_trace(go.Scatter(name=\"MLE\", x=values_test, y=probs_mle, mode='markers+lines'))\nfig.add_trace(go.Scatter(name=\"hist\", x=values_test, y=probs_hist, mode='markers+lines'))\nfig.update_layout(xaxis_title='x', yaxis_title='probability', title='Test set')\n\n\n                                                \n\n\n\n\n\n\n\n\nNote\n\n\n\nGaussian mixture models are universal approximators of probability distributions given enough modes, i.e., enough \\((\\mu_i, \\sigma_i)\\). Hence, they are common ansatze in this kind of applications. We haven’t cheated that much :D",
    "crumbs": [
      "Course",
      "Generative models",
      "Generative models"
    ]
  },
  {
    "objectID": "course/probabilistic_view.html#footnotes",
    "href": "course/probabilistic_view.html#footnotes",
    "title": "A probabilistic view on machine learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis notation is easily generalized to vector-valued random variables.↩︎\nIf there is any kind of stochasticity in the underlying distribution, then even an ideal model can be wrong. Such an~error is called the Bayes optimal error that poses a fundamental limit on statistical models.↩︎\nDo not confuse likelihood and probability!↩︎\nWe recommend an illustrative discussion of the asymmetry of \\(D_{\\mathrm{KL}}(p||q)\\) in Fig. 3.6 of Goodfellow, Bengio, and Courville (2016).↩︎\nFor the sake of simplicity, we consider one-dimensional output. The following derivations, however, can easily be extended to multi-dimensional output as well.↩︎\nFor one-dimensional input and output, the hyperplane simply is a line.↩︎\nRemember that we call it posterior because it is computed after the observation of the data set \\(\\mathcal{D}\\).↩︎\nThis corresponds to a uniform prior of the parameters \\(\\theta\\).↩︎",
    "crumbs": [
      "Course",
      "A probabilistic view on machine learning"
    ]
  },
  {
    "objectID": "course/quantum/qaoa.html",
    "href": "course/quantum/qaoa.html",
    "title": "Quantum Approximate Optimization Algorithm (QAOA) from scratch",
    "section": "",
    "text": "The QAOA algorithm is a quantum algorithm used to solve optimization problems by finding an extremum of a given cost function. The QAOA was first introduced by E. Farhi, J. Goldstone, and S. Gutmann in A Quantum Approximate Optimization Algorithm. The algorithm has been shown to be efficient for solving certain optimization problems, such as the Max-Cut problem, which is NP-hard.\nIn the following, we will implement QAOA for the Max-Cut problem from scratch using Pytorch and its automatic differentiation libraries.",
    "crumbs": [
      "Course",
      "Automatic differentiation for quantum computing",
      "Quantum Approximate Optimization Algorithm (QAOA) from scratch"
    ]
  },
  {
    "objectID": "course/quantum/qaoa.html#max-cut-problem-expressed-as-a-spin-12-problem",
    "href": "course/quantum/qaoa.html#max-cut-problem-expressed-as-a-spin-12-problem",
    "title": "Quantum Approximate Optimization Algorithm (QAOA) from scratch",
    "section": "1.1 Max-Cut problem expressed as a spin-1/2 problem",
    "text": "1.1 Max-Cut problem expressed as a spin-1/2 problem\nThe Max-Cut problem can be cast to solving a quantum mechanical problem of spin-1/2 system. To the \\(i\\)-th node with the value \\(z_i\\) we assign the state of the spin-1/2, i.e.:\n\\[\\begin{equation}\n\\begin{split}\n    z_i = +1 \\to & |0\\rangle \\equiv |\\uparrow\\rangle \\\\\n    z_i = -1 \\to & |1\\rangle \\equiv |\\downarrow\\rangle.\n\\end{split}\n\\end{equation}\\] As such, minimizing the cost function \\(C(\\vec{z})\\) can be considered as fidning the spin configuration which minimizes the \\(\\textit{cost Hamiltonian}\\), defined as:\n\\[\\begin{equation}\n\\hat{H}_C = \\sum_{ik} w_{ik} \\hat{\\sigma}^z_i\\hat{\\sigma}^z_k,\n\\end{equation}\\] where \\(\\hat{\\sigma}^z_i\\) is a Pauli-Z matrix acting on \\(i\\)-th qubit.\nWhile we consider a planar graph and enumerate spins with index \\(i\\), the problem is equivalent to considering a one-dimensional chain of spins connected adequately via edges. For simplicity, let’s consider a simple two dimensional grid of size \\(L_x \\times L_y\\), where each node of the grid we \\((i_x, i_y)\\) have one spin-\\(1/2\\), and \\(i_x = 1,\\dots,L_x\\), \\(i_y = 1,\\dots,L_y\\). We assume \\(w_{ik} = 1\\) where two spins are nearest neighbors and \\(w_{ik}=0\\) otherwise.\nNext, to each spin at node \\((i_x,i_y)\\) we can assign a label \\(i \\equiv (i_y - 1)L_x + i_x\\), , i.e. \\((i_x, i_y) \\to i\\). With such a mapping, our original 2-dimensional problem can be cast to a one-dimensional problem with \\(L = L_x L_y\\) spins-\\(1/2\\), where total Hilbert space has size \\({\\cal D} = 2^{L}\\).\nThe Quantum Approximate Optimization Algorithm is based on a specific form of an ansatz for optimal many-body wave function \\(|\\psi_*\\rangle\\) minimizing the cost Hamiltonian \\(\\hat{H}_C\\). The optimal parameters of an ansatz are obtained in an iterative procedure.",
    "crumbs": [
      "Course",
      "Automatic differentiation for quantum computing",
      "Quantum Approximate Optimization Algorithm (QAOA) from scratch"
    ]
  },
  {
    "objectID": "course/quantum/qaoa.html#qaoa-implementation-in-pytorch",
    "href": "course/quantum/qaoa.html#qaoa-implementation-in-pytorch",
    "title": "Quantum Approximate Optimization Algorithm (QAOA) from scratch",
    "section": "2.1 QAOA implementation in Pytorch",
    "text": "2.1 QAOA implementation in Pytorch\n\n2.1.1 Pauli spin chain operators\nThe starting point is to implement the Pauli spin chain operators. On a spin-\\(1/2\\) chain with \\(L\\) spins, the Pauli spin operators \\(X_i,Y_i,Z_i\\) acting on \\(i\\)-th spin are defined as: \\[\\begin{equation}\n\\begin{split}\nX_i  &= \\mathbb{1}_1\\otimes\\dots\\mathbb{1}_{i-1}\\otimes\\hat{\\sigma}^x\\otimes\\mathbb{1}_{i+1}\\dots\\mathbb{1}_{L},\\\\\nY_i  &= \\mathbb{1}_1\\otimes\\dots\\mathbb{1}_{i-1}\\otimes\\hat{\\sigma}^y\\otimes\\mathbb{1}_{i+1}\\dots\\mathbb{1}_{L},\\\\\nZ_i  &= \\mathbb{1}_1\\otimes\\dots\\mathbb{1}_{i-1}\\otimes\\hat{\\sigma}^z\\otimes\\mathbb{1}_{i+1}\\dots\\mathbb{1}_{L},\n\\end{split}\n\\end{equation}\\] where \\(\\hat{\\sigma}^{x,y,z}\\) are \\(2\\times2\\) Pauli operators.\nIn a similar manner we can define Hadamard matrix \\(H_i\\) acting on \\(i\\)-th spin, i.e. \\[\\begin{equation}\nH_i = \\mathbb{1}_1\\otimes\\dots\\mathbb{1}_{i-1}\\otimes H\\otimes\\mathbb{1}_{i+1}\\dots\\mathbb{1}_{L},\n\\end{equation}\\] where \\(H = \\frac{\\hat{\\sigma}^x + \\hat{\\sigma}^z}{\\sqrt{2}}\\).\nLet’s import necessary librarires and define Pauli operators, and Hadamard operator, on a spin chain of lenght \\(L=L_x\\times L_y\\). We consider \\(L_x = 3\\), \\(L_y = 2\\):\n\nimport matplotlib.pyplot as plt\nimport torch as pt\nfrom torch import matrix_exp as expm\nfrom torch.linalg import eigh as eigh\nimport numpy as np\n\ndef get_linear_index(i_x, i_y):\n    return  (i_y-1)*Lx + i_x\n\ndef get_Identity(k):  # returns k-tensor product of the identity operator, ie. Id^k\n    Id = id_local\n    for i in range(0, k-1):\n        Id = pt.kron(Id, id_local)\n    return Id\n         \ndef get_string_operator(A, L, i):\n    Op = A\n    if(i == 1):\n        Op = pt.kron(A,get_Identity(L-1))\n        return Op\n    if(i == L):\n        Op = pt.kron(get_Identity(L-1),A)\n        return Op\n    if(i&gt;0 and i&lt;L):\n        Op = pt.kron(get_Identity(i-1), pt.kron(Op, get_Identity(L-i)))\n        return Op\n    \nid_local = pt.tensor([[1.,0],[0,1.]])\nsigma_x = pt.tensor([[0,1.],[1.,0]])\nsigma_y = 1j*pt.tensor([[0,-1.],[1.,0]])\nsigma_z = pt.tensor([[1.,0],[0,-1.]])\nhadamard = 1.0/pt.sqrt(pt.tensor(2))*pt.tensor([[1,1],[1,-1]])+1j*0    \n\n\nLx = 3\nLy = 2\nL = Lx*Ly\nD = 2**L\nId = get_string_operator(id_local, L, 1)\nX = {}\nY = {}\nZ = {}\nHadamard = {}\n\n\nfor qubit_i_y in range(1, Ly+1):    # Loop over indices on a 2-dimensional grid (i_x,i_y)\n    for qubit_i_x in range(1,Lx+1): #\n        qubit_i = get_linear_index(qubit_i_x, qubit_i_y)\n        X[qubit_i] = get_string_operator(sigma_x, L, qubit_i)        # Define operator X_i acting on spin (i_x,i_y)\n        Y[qubit_i] = get_string_operator(sigma_y, L, qubit_i)        # Define operator Y_i acting on spin (i_x,i_y)\n        Z[qubit_i] = get_string_operator(sigma_z, L, qubit_i)        # Define operator Z_i acting on spin (i_x,i_y)\n        Hadamard[qubit_i] = get_string_operator(hadamard, L, qubit_i)# Define operator H_i acting on spin (i_x,i_y)\n\nAs we mentioned above, all operators are expressed in the diagonal basis of the \\(Z = \\sum_{i=1}^{L} Z_i\\) operator, i.e.\n\\[\\begin{equation}\n\\begin{split}\n   X & = \\sum_{k,l} &lt;v_k|X|v_l&gt;|v_k\\rangle\\langle v_l| \\\\\n   Y & = \\sum_{k,l} &lt;v_k|Y|v_l&gt;|v_k\\rangle\\langle v_l| \\\\\n   Z & = \\sum_{k,l} &lt;v_k|Z|v_l&gt;|v_k\\rangle\\langle v_l| \\delta_{k,l},\n\\end{split}\n\\end{equation}\\] and \\[\\begin{equation}\n\\begin{split}\n   |v_1\\rangle & = |\\uparrow \\uparrow \\dots \\uparrow \\rangle \\\\\n   |v_2\\rangle & = |\\uparrow \\uparrow \\dots \\downarrow \\rangle \\\\\n   & \\vdots \\\\\n   |v_D\\rangle & = |\\downarrow \\downarrow \\dots \\downarrow \\rangle \\\\\n\\end{split}\n\\end{equation}\\]\nIn the following we denote: \\(\\uparrow \\equiv 1\\), \\(\\downarrow \\equiv -1\\).\nLet’s us construct our Hilbert space:\n\nD = 2**L\nbasis = pt.zeros((D,L)) \nfor v_i in range(0,D):\n    fock_state = pt.zeros(D) \n    fock_state[v_i] = 1\n    for i in range(1,L+1):\n        basis[v_i,i-1] = pt.vdot(fock_state, Z[i]@fock_state)\n\nfor v_i in range(0,D):  \n    string_fock_vector = \"|v_\" + \"{:03d}\".format(v_i) + \"&gt; = |\"\n    for i in range(1,L+1):\n        tmp = int(basis[v_i,i-1].item())\n        if(tmp==1):\n            string_plus_minus = \" {:1d}\".format(tmp)\n        if(tmp==-1):\n            string_plus_minus = \"{:1d}\".format(tmp)\n            \n         \n        string_fock_vector = string_fock_vector + string_plus_minus + \" \"\n    string_fock_vector = string_fock_vector + \"&gt;\"\n    print(string_fock_vector)\n\n\n\n2.1.2 Cost and mixer Hamiltonians\nOur cost Hamiltonian for Max-Cut problem has the following form:\n\\[\\begin{equation}\n\\hat{H}_C = \\frac{1}{2}\\sum_{\\langle i,j\\rangle} Z_i Z_j,\n\\end{equation}\\]\nwhere \\(\\langle i,j\\rangle\\) means sum over nearest neighbors. Note, that each node \\(i\\) has two coordinates, i.e. $ i (i_x, i_y)$:\n\nH_C = pt.zeros((D,D)) + 0*1j\nfor qubit_i_x in range(1,Lx+1):\n    for qubit_i_y in range(1,Ly+1):\n        qubit_i = get_linear_index(qubit_i_x,qubit_i_y)\n        for qubit_j_x in range(1,Lx+1):\n            for qubit_j_y in range(1,Ly+1):\n                qubit_j = get_linear_index(qubit_j_x,qubit_j_y)\n                # Sum over nearest neighbors only\n                if(np.abs(qubit_i_x - qubit_j_x)==1 and np.abs(qubit_i_y - qubit_j_y)==0):\n                    H_C = H_C + Z[qubit_i]@Z[qubit_j]\n                if(np.abs(qubit_i_x - qubit_j_x)==0 and np.abs(qubit_i_y - qubit_j_y)==1):\n                    H_C = H_C + Z[qubit_i]@Z[qubit_j]\nH_C = 0.5*H_C\n\nThe mixer Hamiltonian has the following form:\n\\[\\begin{equation}\n\\hat{H}_B = \\sum_i X_i\n\\end{equation}\\]\n\nH_B = pt.zeros((D,D)) + 0*1j\nfor qubit_i_x in range(1,Lx+1):\n    for qubit_i_y in range(1,Ly+1):\n        qubit_i = get_linear_index(qubit_i_x,qubit_i_y)\n        H_B = H_B + X[qubit_i]\n\nFinally, we can define two unitary evolution operators for cost, and mixer Hamiltonians, parametrized by \\(\\gamma\\) and \\(\\beta\\)\n\\[\\begin{equation}\n\\begin{split}\nU_B(\\beta) & = e^{-i\\beta \\hat{H}_B} \\\\\nU_C(\\gamma) & = e^{-i\\gamma \\hat{H}_C}.\n\\end{split}\n\\end{equation}\\]\n\ndef U_B(H_B,beta):\n    return expm(-1j*beta*H_B)\n\ndef U_C(H_C,gamma):\n    return expm(-1j*gamma*H_C)\n\n\n\n2.1.3 QAOA ansatz\nFinally, we can define the QAOA ansatz, parametrized by two set of parameters, \\(\\vec{\\beta}\\), and \\(\\vec{\\gamma}\\), having \\(p\\) elements: \\[\\begin{equation}\n|\\psi(\\vec{\\beta},\\vec{\\gamma})\\rangle = \\prod_{l = 1}^p U_B(\\beta_l)U_C(\\gamma_l)|s\\rangle,\n\\end{equation}\\] where state \\(|s\\rangle\\) is coherent superposition of states \\(|0\\rangle\\) adn \\(|1\\rangle\\) of each qubit, i.e. \\[\\begin{equation}\n|s\\rangle = \\prod_{i = 1}^{L} H_i|0\\rangle,\n\\end{equation}\\] where \\(H_i\\) is Hadamard matrix acting on \\(i\\)-th qubit initialized in \\(|0\\rangle\\) state.\nLength \\(p\\) of vectors \\(\\vec{\\beta}\\), and \\(\\vec{\\gamma}\\) is one of the hyperparameters in \\(QAOA\\) anstaz:\n\ndef psi(parameters):\n    beta_vec  = parameters[0,:]\n    gamma_vec = parameters[1,:]\n    psi_tmp = pt.zeros(D) + 0*1j\n    psi_tmp[0] = 1 # in basis of total Z operator Z = \\sum_i Z_i this elements corresponds  \n                   # to configurations with all spins pointing up\n   \n    # prepare Hadamard operation on each qubit\n    for qubit_i_x in range(1,Lx+1):\n        for qubit_i_y in range(1,Ly+1):\n            qubit_i = get_linear_index(qubit_i_x,qubit_i_y)\n            psi_tmp = Hadamard[qubit_i]@psi_tmp\n    \n    for i in range(0,p):\n        psi_tmp = U_C(H_C, gamma_vec[i])@psi_tmp\n        psi_tmp = U_B(H_B, beta_vec[i])@psi_tmp\n    \n    return psi_tmp\n\nFinally, we define a cost function as a expectation value of the cost Hamilonian \\(F(\\vec{\\beta},\\vec{\\gamma}) = \\langle \\psi(\\vec{\\beta},\\vec{\\gamma})|\\hat{H}_C|\\psi(\\vec{\\beta},\\vec{\\gamma})\\rangle\\):\n\ndef F(parameters):\n    return pt.vdot(psi(parameters), H_C@psi(parameters)).real\n\n\n\n2.1.4 Optimization loop\nIn the optimization loop, we choose a number of layers \\(p\\), and initialize the \\(\\vec{\\beta}\\), and \\(\\vec{\\gamma}\\) as a random number. Next, with the help of automatic differentiation, we find a set of parameters that minimizes the expectation value of the cost Hamiltonian \\(\\hat{H}_C\\). When we find the optimal set of parameters, our solution to the problem (i.e. spin configuration) corresponds to the Fock state with the maximal probability of the final wave-function, i.e. \\(v_{\\rm optimal} = argmax(\\rho)\\)\n\np = 2\nparameters = pt.rand((2,p),requires_grad=True)   # first row corresponds to (beta_1,...,beta_p)\n                                                 # second row corresponds to (gamma_1,...,gamma_p)\noptimizer = pt.optim.Adam([parameters],lr = 1e-2)\nloss_vs_epoch = []\nfor i in range(0,100):\n    loss = F(parameters)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    loss_vs_epoch.append(loss.item())\n\npsi_optimal = psi(parameters)\nrho = np.abs(psi_optimal.detach().numpy())**2\ni_optimal = np.argmax(rho)\nprint('Optimal spins configuration after single run corresponds to basis vector with index k = ' + str(i_optimal))\n\nFinally, we have to repeat an optimization procedure and make histogram of optimal configurations.\n\nN_runs = 10\nhistogram = np.zeros(D)\nfor run in range(0,N_runs):\n    parameters = pt.rand((2,p),requires_grad=True) \n    optimizer = pt.optim.Adam([parameters],lr = 1e-2)\n    loss_vs_epoch = []\n    for i in range(0,200):\n        loss = F(parameters)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        loss_vs_epoch.append(loss.item())\n    psi_optimal = psi(parameters)\n    rho = np.abs(psi_optimal.detach().numpy())**2\n    histogram[np.argmax(rho)] += 1\n\nThe optimal configuration corresponds to:\n\nk_optimal = np.argmax(histogram)\nstring_fock_vector = \"|v_\" + \"{:03d}\".format(v_i) + \"&gt; = |\"\nfor i in range(1,L+1):\n    tmp = int(basis[k_optimal,i-1].item())\n    if(tmp==1):\n        string_plus_minus = \" {:1d}\".format(tmp)\n    if(tmp==-1):\n        string_plus_minus = \"{:1d}\".format(tmp)\n    string_fock_vector = string_fock_vector + string_plus_minus + \" \"\nstring_fock_vector = string_fock_vector + \"&gt;\"\nprint(string_fock_vector)",
    "crumbs": [
      "Course",
      "Automatic differentiation for quantum computing",
      "Quantum Approximate Optimization Algorithm (QAOA) from scratch"
    ]
  },
  {
    "objectID": "course/linear_models/linear_regression.html#footnotes",
    "href": "course/linear_models/linear_regression.html#footnotes",
    "title": "Linear regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe literature also uses the terms of criterion or cost, error, or objective functions. Their definitions are not very strict. Following (Goodfellow, Bengio, and Courville 2016): ‘’The function we want to minimize or maximize is called the objective function, or criterion. When we are minimizing it, we may also call it the cost function, loss function, or error function. In this book, we use these terms interchangeably, though some machine learning publications assign special meaning to some of these term’’. For example, loss function may be defined for a single data point, the cost or error function may be a sum of loss functions, so check the definitions used in each paper.↩︎\nFor classification, a~more intuitive measure of the performance could be, e.g., accuracy, which is the ratio between the number of correctly classified examples and the data set size. Note, however, that gradient-based optimization requires measures of performance that are smooth and differentiable. These conditions distinguish loss functions from evaluation metrics such as accuracy, recall, precision, etc.↩︎\nBeware that the notion of iteration is different for gradient descent and for stochastic gradient descent. For the former, an iteration corresponds to an epoch (the whole training set), while for the latter it corresponds to a minibatch.↩︎",
    "crumbs": [
      "Course",
      "Linear models",
      "Linear regression"
    ]
  },
  {
    "objectID": "course/linear_models/polynomial_fit.html",
    "href": "course/linear_models/polynomial_fit.html",
    "title": "Polynomial fit",
    "section": "",
    "text": "1 Fitting a noisy polynomial curve\nWe now consider the task of fitting a polynomial curve with noise. Despite dealing with higher order polynomials than before, this problem can also be rewritten as a linear regression task. Let us first generate a dataset with the help of the function noisy_curve, which maps \\(x\\) to a polynomial of degree \\(d\\) \\(f(x)=\\mathbf{w}^T\\mathbf{x}+\\text{noise}\\) and where \\(\\mathbf{x}=(x^0,x^1,\\ldots,x^d)\\).\n\ncoeffs = [2., 1., 0., 1.]\nx, y = noisy_curve(coeffs, interval=[-3., 1.5], noise=[0., 2.])\n\nFigure 1 shows the generated data with the ground truth.\n\n\nCode\nfig = go.Figure()\nfig.add_scatter(x=x, y=y, mode=\"markers\", name='data',\n                hovertemplate='x:%{x:.2f}'\n                +'&lt;br&gt;y:%{y:.2f}&lt;/br&gt;&lt;extra&gt;&lt;/extra&gt;')\nx1 = np.linspace(x.min(),x.max(),num=50)\nx1, y1 = noisy_curve(coeffs,x=x1)\nfig.add_scatter(x=x1, y=y1, mode=\"lines\",name='Ground Truth')\nfig.update_layout(width=800,height=400,xaxis_title='x',yaxis_title='f(x)')\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 1: Parabola with Gaussian noise\n\n\n\n\nAs in the previous section, we choose the mean square error for the loss function.\n\n\n2 Polynomial fit as multivariable linear regression\nThe polynomial regression can be seen as a linear regression of multiple variables with the help of the following trick. Let us rewrite \\(f(x)=\\sum_{i=0}^d w_i x^i\\) as \\(f(x)=\\mathbf{w}^T\\mathbf{x}\\), where \\(\\mathbf{x}=(x^0, x^1, \\ldots, x^d)\\). We can then see the polynomial fit as a linear regression of the fucntion \\(y=\\sum_{i=0}^dw_i x_i\\), where \\(x_i=x^i\\).\nThe mean square error function therefore reads\n\\[\\begin{aligned}\nMSE &= \\sum_{i=1}^N (\\mathbf{w}^T\\mathbf{x}_i-y_i)^2\\\\\n&= \\parallel \\mathbf{y}-X\\mathbf{w}\\parallel^2\\\\\n&=(\\mathbf{y}-X\\mathbf{w})^T(\\mathbf{y}-X\\mathbf{w}),\n\\end{aligned}\\]\nwhere\n\\[ X=\n\\begin{pmatrix}\n1 & x_1^1 & \\ldots & x_1^d\\\\\n1 & x_2^1 & \\ldots & x_2^d \\\\\n\\vdots& \\vdots & \\vdots & \\vdots\\\\\n1 & x_N^1 & \\ldots & x_N^d\n\\end{pmatrix} .\\]\nWe now take the derivative with respect to all the weights \\(w\\) and set it to \\(0\\). We therefore find the estimator\n\\[w =(X^TX)^{-1}X^T\\mathbf{y}.\\]\n\n\n\n\n\n\nExercise\n\n\n\nImplement a exact_poly_fit(x, y, degree) that numerically computes the weights w with the expression above. You can use np.linalg.inv to do the matrix inversion.\n\n\n\n\nCode\ndef exact_poly_fit(x,y,degree):\n    mat = np.ones((x.size,degree+1))\n    for i in range(degree):\n        mat[:,i+1] = x**(i+1)\n    w = np.linalg.inv(np.transpose(mat)@mat)@np.transpose(mat)@y\n    return w\n\n\nLet us run the algorithm for our dataset. We will fit a 3rd degree polynomial and compare the resulting parameters to the original ones.\n\nw_best = exact_poly_fit(x,y,3)\np1, p2 = dict(coeffs=w_best), dict(coeffs=coeffs)\nprint (np.array([w_best,coeffs]))\nprint (f'Best parameters: {MSE(x,y,curve,params=p1):.3f}')\nprint(f'Original parameters: {MSE(x,y,curve,params=p2):.3f}')\n\n[[2.04366107 1.05858036 0.21556575 1.0784744 ]\n [2.         1.         0.         1.        ]]\nBest parameters: 3.767\nOriginal parameters: 3.825\n\n\nThe algorithm does a fairly good job. It is quite interesting to have a look at the mean square error on this dataset. The best parameters have a lower loss than the actual true parameters! We will come back to this point later ;)\n\n\n3 Stochastic Gradient Descent\nJust like we did with the linear regression, we can optimize our model parameters with a gradient-based method. Let us see what we obtain with the stochastic gradient descent algorithm for the poynomial fit. Figure 2 shows the fit after optimization.\n\ncoeffs0 = np.random.normal(loc=0,scale=0.1,size=4)\n\nll = dict(loss=MSE, grads=grad_MSE_pr, fun=curve)\n\npini = dict(coeffs=coeffs0)\n\ndf = pd.DataFrame(columns=['coeffs','value'])\n\ntrackers = sgd(x,y, pini, ll, eta=1E-5, niter=int(1E4))\ndf1 = pd.DataFrame(data={'coeffs':trackers['coeffs'],'value':trackers['loss']})\ndf = pd.concat([df, df1])\n\nprint(f'final Loss:{df[\"value\"].iloc[-1]:3f}')\nprint (df[\"coeffs\"].iloc[-1])\nprint(coeffs)\n\nfinal Loss:3.981939\n[1.29711172 0.64731396 0.65844775 1.27037435]\n[2, 1.0, 0, 1]\n\n\n\n\nCode\ncc = df[\"coeffs\"].iloc[-1]\n\nfig = go.Figure()\nfig.add_scatter(x=x, y=y, mode=\"markers\", name='data',\n                hovertemplate='x:%{x:.2f}'\n                +'&lt;br&gt;y:%{y:.2f}&lt;/br&gt;&lt;extra&gt;&lt;/extra&gt;')\nx1 = np.linspace(x.min(),x.max(),num=50)\ny1 = curve(x1,cc)\nfig.add_scatter(x=x1, y=y1, mode=\"lines\",name='Fit')\nfig.update_layout(width=800,height=400,xaxis_title='x',yaxis_title='f(x)')\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 2: Polynomial fit of the data\n\n\n\n\nFigure 3 shows how the algotihm adjusts the polynomial curve during the optimization.\n\n\nCode\nstep = 100\nx1 = np.linspace(x.min(),x.max(),num=50)\n\nframes = [go.Frame(data=[go.Scatter(x=x1, y=curve(x1,df[\"coeffs\"].iloc[i*step]),mode='lines')],layout=go.Layout(title_text=f'step:{i*step}, MSE:{df[\"value\"].iloc[i*step]:.2f}')) for i in range(len(df)//step)]\n\nbuttons = [dict(label=\"Play\",method=\"animate\",\n                args=[None, {\"frame\": {\"duration\": 100, \"redraw\": True},\n                             \"fromcurrent\": True, \n                             \"transition\": {\"duration\": 300,\"easing\": \"quadratic-in-out\"}}]),\n           dict(label=\"Pause\",method=\"animate\",\n                args=[[None], {\"frame\": {\"duration\": 0, \"redraw\": False},\"mode\": \"immediate\",\"transition\": {\"duration\": 0}}]),\n          dict(label=\"Restart\",method=\"animate\",\n                args=[None,{\"frame\": {\"duration\": 100, \"redraw\": True}}])]\n\nFig = go.Figure(\n    data=[go.Scatter(x=x1, y= curve(x1,df[\"coeffs\"].iloc[0]),mode='lines',name = 'line',\n                     hovertemplate='x:%{x:.2f}'+'&lt;br&gt;y:%{y:.2f}&lt;/br&gt;&lt;extra&gt;&lt;/extra&gt;'),\n          go.Scatter(x=x, y=y, mode=\"markers\", name='data',\n                hovertemplate='x:%{x:.2f}'\n                +'&lt;br&gt;y:%{y:.2f}&lt;/br&gt;&lt;extra&gt;&lt;/extra&gt;')],\n    layout=go.Layout(\n        xaxis=dict(range=[x.min()-2, x.max()+2], autorange=False),       \n        yaxis=dict(range=[y.min()-2, y.max()+2], autorange=False),\n        updatemenus=[dict(\n            type=\"buttons\",\n            buttons=buttons)]\n    ),\n    frames= frames\n)\nFig.update_layout(xaxis_title='x',yaxis_title='f(x)')\nFig.show()\n\n\n\n\n                                                \n\n\nFigure 3: Animation of the optimization\n\n\n\n\n\n\n4 Overfitting\nUp until now, we have not discussed a very important hyper-parameter in this problem: the degree of the polynomail. In particular, we have fixed the degree to be the one of the original function. However, this is typically unknown in practice and we either rely on educated guesses or we resort to perform various fits for different degrees and keep the best one (but what is the best one?! We’ll see). To this end, we use the polyfit subroutine of numpy, which is much more stable than the exact_poly_fit we prepared.\n\nvec_cc = []\nmse_t = []\nmse_v = []\n\nnpoly = 20\nndata = 50\nfor i in np.arange(1,npoly):\n    vec_cc.append(polyfit(x[:ndata],y[:ndata],deg=i))\n    p1, p2 = dict(coeffs=vec_cc[i-1]), dict(coeffs=vec_cc[i-1])\n    mse_t.append(MSE(x[:ndata], y[:ndata],curve,params=p1))\n    mse_v.append(MSE(x[ndata:], y[ndata:],curve,params=p2))\n\nFigure 4 shows the loss function over the training data as function of the polynomial degree. At a first glance, it looks like a higher degree gives rise to better loss. However, in Figure 5, we can really see the overfitting of the higher order polynomials. This can be detected by dividing the training set into two subsets: the training set and the validation set. We train the algorithm using data exclusively from the training set and, then, we evaluate its performance on the validation set. The validation set contains new data for our model, which allows us to assess how well our algorithm generalizes to unseen data. We can see that we are overfitting to the training set when we see that the loss in the validation set stagnates or increases. Turn on the orange line in Figure 4 to see it!\n\n\nCode\nfig = go.Figure()\nfig.add_scatter(x=np.arange(1,npoly), y=mse_t, mode='lines+markers', name='training')\nfig.add_scatter(x=np.arange(1,npoly), y=mse_v, mode='lines+markers', visible='legendonly', name='validation')\nfig.update_layout(yaxis_range=[0,10],xaxis_title='degree',yaxis_title='Loss')\n\n\n\n\n                                                \n\n\nFigure 4: Training loss with respect to the degree of the polynomial\n\n\n\n\n\n\nCode\nfig = go.Figure()\nfig.add_scatter(x=x[:ndata], y=y[:ndata], mode=\"markers\", name='data',\n                hovertemplate='x:%{x:.2f}'\n                +'&lt;br&gt;y:%{y:.2f}&lt;/br&gt;&lt;extra&gt;&lt;/extra&gt;')\nx1 = np.linspace(x.min(),x.max(),num=50)\n\npoly = [1, 2, 3, 4, 6, 8, 10, 19]\nfor i,k in enumerate(poly):\n    visible = True if k == 0 else 'legendonly'\n    x1, y1 = noisy_curve(vec_cc[k-1],x=x1)\n    fig.add_scatter(x=x1, y=y1, mode=\"lines\",name=f'{k}th degree', visible=visible)\nfig.update_layout(width=800, height=400, yaxis_range=[y.min(),y.max()], xaxis_title='x', yaxis_title='f(x)')\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 5: Training loss with respect to the degree of the polynomial\n\n\n\n\nA typical instance for overfitting appears when we have many free parameters compared to the number of data points. Indeed, we can achieve a zero training loss with some algorithms when we have as many parameters as data points. However, this usually comes at the expense of extreme overfitting and poor generalization.\n\n\n\n\n\n\nExercise\n\n\n\nIn the experiment above, we have used 50 data points to fit up to a 19th-degree polynomial. Run the same procedure with increasingly less data, e.g., set ndata to 30, 20 and 10, and observe what happens with the resulting curves. Do we see overfitting for lower degree polynomials? Do you think these models would provide a reasonable prediction if we drew a new data sample form the same experiment?\n\n\nWe will now discuss two strategies to prevent overfitting.\n\n\n5 More data\nAs we have seen right above, the relative number of our model parameters compared to the amount of data that we have is a key factor for overfitting. If having less data makes our model more prone to overfitting, having more data naturally helps us mitigate it.\nTherefore, let us generate more samples.\n\nnsamples = int(1E3)\nxn, yn = noisy_curve(coeffs, interval = [-3,1.5], noise=[0.,2], nsamples=nsamples)\n\nWe then perform the split between the training set and validation set and compute the loss function for both sets.\n\nvec_cc, mse_t,mse_v  = [], [], []\n\nnpoly = 20\nndata = int(0.8*nsamples) #We set 80% of the data for the training and 20% for the validation\n\nfor i in np.arange(1,npoly):\n    vec_cc.append(polyfit(xn[:ndata],yn[:ndata],deg=i))\n    p1, p2 = dict(coeffs=vec_cc[i-1]), dict(coeffs=vec_cc[i-1])\n    mse_t.append(MSE(xn[:ndata], yn[:ndata], curve, params=p1))\n    mse_v.append(MSE(xn[ndata:], yn[ndata:], curve, params=p2))\n\nFigure 6 shows the comparison between the trainng loss and the validation loss for different degrees. We observe a much better behavior than in the previous case with small dataset. This is also confirmed in Figure 7 where we can clearly see the advantage of using a larger dataset.\n\n\nCode\nfig = go.Figure()\nfig.add_scatter(x=np.arange(1,npoly), y=mse_t, mode='lines+markers', name='training')\nfig.add_scatter(x=np.arange(1,npoly), y=mse_v, mode='lines+markers', visible='legendonly', name='validation')\nfig.update_layout(yaxis_range=[0,10], xaxis_title='degree', yaxis_title='Loss')\n\n\n\n\n                                                \n\n\nFigure 6: Training loss with respect to the degree of the polynomial for a larger dataset\n\n\n\n\n\n\nCode\nfig = go.Figure()\nfig.add_scatter(x=xn[:ndata], y=yn[:ndata], mode=\"markers\", name='data',\n                hovertemplate='x:%{x:.2f}'\n                +'&lt;br&gt;y:%{y:.2f}&lt;/br&gt;&lt;extra&gt;&lt;/extra&gt;')\nx1 = np.linspace(x.min(),x.max(),num=50)\n\npoly = [1, 2, 3, 4, 6, 8, 10, 19]\nfor i,k in enumerate(poly):\n    visible = True if k == 0 else 'legendonly'\n    x1, y1 = noisy_curve(vec_cc[k-1],x=x1)\n    fig.add_scatter(x=x1, y=y1, mode=\"lines\",name=f'{k}th degree', visible=visible)\nfig.update_layout(width=800, height=400, yaxis_range=[y.min(),y.max()],xaxis_title='x',yaxis_title='f(x)')\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 7: Polynomial fit for the different degrees\n\n\n\n\n\n\n6 Regularization\nAnother way to avoid overfitting is regularization. The idea here is to add a term to the loss function that prevents the weights to behave in an ill-deined way. An example of such regularization is the \\(l_2\\) regularization which consists in adding to the loss function the term \\(\\alpha\\parallel \\mathbf{w} \\parallel^2\\). Intituitively, this parabolic term avoids to have exploding weights. The regression with such regularization is called Ridge regression.\n\n\n\n\n\n\nExercise\n\n\n\nFor the analytical solution, show that the regularization term gives rise to the following solution\n\\[w =(X^TX+\\alpha \\mathbb{1})^{-1}X^T\\mathbf{y}.\\]\n\n\n\n\n\n\n\n\nExercise\n\n\n\nFor the gradient descent algorithm, show that the regularization leads to the update rule for each weight \\(w\\)\n\\[ w_{i+1} = (1-2\\, \\alpha\\,\\eta) w_i - \\eta \\partial_w L \\]\n\n\nWe here use an implementation of scikit learn. To this end, we define a function that creates the matrix \\(X\\).\n\ndef poly_cond(x, n):\n    matx = np.zeros((x.size,n))\n    for i,k in enumerate(range(1,n+1)):\n        matx[:,i] = x**k\n    return matx\n\nWe then perform the Ridge regression for the polynomials of different degrees.\n\nalpha = 0.5\n\nvec_cc, mse_t, mse_v = [], [], []\n\nnpoly = 20\nndata = 50\nfor i in np.arange(1,npoly):\n    matx = poly_cond(x[:ndata],i)\n    reg = linear_model.Ridge(alpha=alpha)\n    reg.fit(matx,y[:ndata])\n    c = np.insert(reg.coef_,0,reg.intercept_)\n    vec_cc.append(c)\n    p1, p2 = dict(coeffs=vec_cc[i-1]), dict(coeffs=vec_cc[i-1])\n    mse_t.append(MSE(xn[:ndata], yn[:ndata], curve, params=p1))\n    mse_v.append(MSE(xn[ndata:], yn[ndata:], curve, params=p2))\n\nFigure 8 shows the Loss funtion in terms of the degree of the polynomials. We can now see that the validation curve behaves much better for higher polymomials. The latter is also confirmed with Figure 9, which shows smaller behaviors for higher polynomials.\n\n\nCode\nfig = go.Figure()\nfig.add_scatter(x=np.arange(1,npoly), y=mse_t, mode='lines+markers', name='training')\nfig.add_scatter(x=np.arange(1,npoly), y=mse_v, mode='lines+markers', visible='legendonly', name='validation')\nfig.update_layout(yaxis_range=[0,10],xaxis_title='Loss',yaxis_title='degree')\n\n\n\n\n                                                \n\n\nFigure 8: Polynomial fit for the different degrees\n\n\n\n\n\n\nCode\nfig = go.Figure()\nfig.add_scatter(x=x[:ndata], y=y[:ndata], mode=\"markers\", name='data',\n                hovertemplate='x:%{x:.2f}'\n                +'&lt;br&gt;y:%{y:.2f}&lt;/br&gt;&lt;extra&gt;&lt;/extra&gt;')\nx1 = np.linspace(x.min(),x.max(),num=50)\n\npoly = [1, 2, 3, 4, 6, 8, 10, 19]\nfor i,k in enumerate(poly):\n    visible = True if k == 0 else 'legendonly'\n    x1, y1 = noisy_curve(vec_cc[k-1],x=x1)\n    fig.add_scatter(x=x1, y=y1, mode=\"lines\",name=f'{k}th degree', visible=visible)\nfig.update_layout(width=800, height=400, yaxis_range=[y.min(),y.max()])\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 9: Polynoms for the different degrees\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nChange the value of \\(\\alpha\\) and perform the same analysis. What do you see?",
    "crumbs": [
      "Course",
      "Linear models",
      "Polynomial fit"
    ]
  },
  {
    "objectID": "course/deep_learning/neural_networks_from_scratch.html#sigmoid-perceptron-update",
    "href": "course/deep_learning/neural_networks_from_scratch.html#sigmoid-perceptron-update",
    "title": "Neural networks from scratch",
    "section": "1.1 Sigmoid perceptron update",
    "text": "1.1 Sigmoid perceptron update\nThe training of a perceptron consists on the iterative update of its parameters, \\(W\\) and \\(\\mathbf{b}\\), in order to minimize the loss function \\(L\\). Here, we will consider the mean-squared error:\n\\[\\begin{equation}\n\\begin{split}\nL & = \\frac{1}{N}\\sum_{i=1}^n L_i\\\\\nL_i & = \\frac{1}{2}(y_i - \\hat{y}_i)^2,\n\\end{split}\n\\end{equation}\\]\nwhere \\(\\hat{y}_i\\) is our prediction and \\(y_i\\) the ground truth.\nWe update the weights \\(W\\) and biases \\(\\mathbf{b}\\) with a gradient descent procedure:\n\\[\\begin{equation}\n\\begin{split}\nW & \\leftarrow W - \\eta \\frac{\\partial L}{\\partial W} = W - \\frac{\\eta}{n} \\sum_{i=1}^N \\frac{\\partial L_i}{\\partial W}\\\\\n\\mathbf{b} & \\leftarrow \\mathbf{b} - \\eta \\frac{\\partial L}{\\partial \\mathbf{b}} = W - \\frac{\\eta}{n} \\sum_{i=1}^n \\frac{\\partial L_i}{\\partial \\mathbf{b}},\n\\end{split}\n\\end{equation}\\]\nwhere \\(\\eta\\) is the learning rate.\nIn this case, we can obtain analytical expressions for the gradients:\n\\[\\begin{equation}\n\\begin{split}\n\\frac{\\partial L_i}{\\partial W} & = (y_i - \\hat{y}_i)\\frac{\\partial y_i}{\\partial W}\\\\\n\\frac{\\partial L_i}{\\partial \\mathbf{b}} & = (y_i - \\hat{y}_i)\\frac{\\partial y_i}{\\partial \\mathbf{b}}\\\\\ny_i & = \\sigma(z_i) \\\\\nz_i & = x_i^T W + \\mathbf{b}\n\\end{split}\n\\end{equation}\\]\nWith the chain rule we have:\n\\[\\begin{equation}\n\\begin{split}\n\\frac{\\partial y_i}{\\partial W} & = \\frac{\\partial \\sigma(z_i)}{\\partial z_i}\\frac{\\partial z_i}{\\partial W} \\\\\n\\frac{\\partial y_i}{\\partial \\mathbf{b}} & = \\frac{\\partial \\sigma(z_i)}{\\partial z_i}\\frac{\\partial z_i}{\\partial \\mathbf{b}}\n\\end{split}\n\\end{equation}\\]\n\n\n\n\n\n\nExercise\n\n\n\nShow that update rule for weights \\(W\\) and bias \\(\\mathbf{b}\\) is:\n\\[\\begin{equation}\n\\begin{split}\nW & \\leftarrow W - \\frac{\\eta }{n}\\sum_{i=1}^n (y_i - \\hat{y}_i) y_i(1-y_i)x_i\\\\\n\\mathbf{b} & \\leftarrow \\mathbf{b} - \\frac{\\eta }{n}\\sum_{i=1}^n (y_i - \\hat{y}_i) y_i(1-y_i)\n\\end{split}\n\\end{equation}\\]",
    "crumbs": [
      "Course",
      "Fundamentals of deep learning",
      "Neural networks from scratch"
    ]
  },
  {
    "objectID": "course/deep_learning/neural_networks_from_scratch.html#network-definition",
    "href": "course/deep_learning/neural_networks_from_scratch.html#network-definition",
    "title": "Neural networks from scratch",
    "section": "2.1 Network definition",
    "text": "2.1 Network definition\nIn a neural network with a single hidden layer, we have two perceptrons: one between the input and the hidden layer, and one between the hidden layer and the output.\nThe input layer has the same size as the number of features in our data, i.e., \\(m\\) neurons. Then, the hidden layer has \\(h\\) neurons, and the output layer has as many neurons as classes \\(c\\). In a regression task, \\(c=1\\) as we predict a single scalar. Thus, the first weigth matrix \\(W_1\\) has shape \\(m\\times h\\) and , and the second weight matrix \\(W_2\\) has shape \\(h\\times c\\). In this case, we only consider biases in the hidden layer \\(\\mathbf{b}_1\\) which is a vector with \\(h\\) entries.\n\n2.1.1 Feed-forward pass\nNow, let’s go through the feed-forward pass of the training data \\(X\\in\\mathbb{R}^{n\\times m}\\) through our network.\n\nThe input goes through the first linear layer \\(\\mathbf{h} \\leftarrow X^TW_1 + \\mathbf{b}_1\\) with shapes \\([n, m] \\times [m, h] = [n, h]\\)\nThen, we apply the activation function \\(\\hat{\\mathbf{h}} \\leftarrow \\sigma(\\mathbf{h})\\) with shape \\([n, h]\\)\nThen, we apply the second linear layer \\(\\mathbf{g} \\leftarrow \\mathbf{h}^TW_{2}\\) with shapes \\([n, h] \\times [h, c] = [n, c]\\)\nFinally, we apply the activation function \\(\\hat{\\mathbf{y}} \\leftarrow \\sigma(\\mathbf{g})\\) with shape \\([n, c]\\)\n\n\n\n2.1.2 Parameter update\nWe will use the MSE loss function denoted in matrix rerpesentation as \\(L = \\frac{1}{2n}||Y - \\hat{Y}||^2\\),\nThe parameter update rule is \\[\\begin{equation}\n\\begin{split}\n  W_1 & = W_1 - \\frac{\\eta}{n}\\frac{\\partial L}{\\partial W_1} \\\\\n  \\mathbf{b}_1 & = \\mathbf{b}_1 - \\frac{\\eta}{n}\\frac{\\partial L}{\\partial \\mathbf{b}_1} \\\\\n  W_2 & = W_2 - \\frac{\\eta}{n}\\frac{\\partial L}{\\partial W_2}.\n\\end{split}\n\\end{equation}\\]\nLet us calculate gradients of the loss function with respect to \\(W_1\\), \\(\\mathbf{b}_1\\) and \\(W_2\\) using the chain rule:\n\\[\\begin{equation}\n\\begin{split}\n\\frac{\\partial L}{\\partial W_{2}} & = \\frac{\\partial L}{\\partial \\hat{Y}}\\frac{\\partial \\hat{Y}}{\\partial \\mathbf{g}}\\frac{\\partial \\mathbf{g}}{\\partial W_{2}} \\\\\n\\frac{\\partial L}{\\partial W_{1}} & = \\frac{\\partial L}{\\partial \\hat{Y}}\\frac{\\partial \\hat{Y}}{\\partial \\mathbf{g}}\\frac{\\partial \\mathbf{g}}{\\partial \\hat{\\mathbf{h}}}\\frac{\\partial \\hat{\\mathbf{h}}}{\\partial \\mathbf{h}}\\frac{\\partial \\mathbf{h}}{\\partial W_{1}}\\\\\n\\frac{\\partial L}{\\partial \\mathbf{b}_{1}} & = \\frac{\\partial L}{\\partial \\hat{Y}}\\frac{\\partial \\hat{Y}}{\\partial \\mathbf{g}}\\frac{\\partial \\mathbf{g}}{\\partial \\hat{\\mathbf{h}}}\\frac{\\partial \\hat{\\mathbf{h}}}{\\partial \\mathbf{h}}\\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{b}_{1}}\n\\end{split}\n\\end{equation}\\]\nWe can write down every term:\n\\[\\begin{equation}\n\\begin{split}\n\\frac{\\partial L}{\\partial \\hat{Y}} & = \\hat{Y} - Y \\\\\n\\frac{\\partial \\hat{Y}}{\\partial \\mathbf{g}} & = \\hat{Y}(1-\\hat{Y}) \\\\\n\\frac{\\partial \\mathbf{g}}{\\partial W_{2}} & = \\hat{\\mathbf{h}} \\\\\n\\frac{\\partial \\mathbf{g}}{\\partial \\hat{\\mathbf{h}}} & = W_{2} \\\\\n\\frac{\\partial \\hat{\\mathbf{h}}}{\\partial \\mathbf{h}} & = \\hat{\\mathbf{h}}(1-\\hat{\\mathbf{h}}) \\\\\n\\frac{\\partial \\mathbf{h}}{\\partial W_1} & = X\\\\\n\\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{b}_1} & = \\mathbb{1}\n\\end{split}\n\\end{equation}\\]\n\n\n\n\n\n\nExercise\n\n\n\nShow explicitly that \\(Q_1\\), and \\(Q_2\\) read: \\[\\begin{equation}\n\\begin{split}\nQ_2 & \\equiv \\frac{\\partial L}{\\partial \\hat{Y}}\\frac{\\partial \\hat{Y}}{\\partial \\mathbf{g}} = (\\hat{Y}-Y)\\hat{Y}(1-\\hat{Y}) \\\\\nQ_1 & \\equiv \\frac{\\partial L}{\\partial \\hat{Y}}\\frac{\\partial \\hat{Y}}{\\partial \\mathbf{g}}\\frac{\\partial \\mathbf{g}}{\\partial \\hat{\\mathbf{h}}}\\frac{\\partial \\hat{\\mathbf{h}}}{\\partial \\mathbf{h}} = Q_2 W_{2}\\hat{\\mathbf{h}}(1-\\hat{\\mathbf{h}})\n\\end{split}\n\\end{equation}\\]\n\n\n\n\n\n\n\n\nExercise\n\n\n\nShow that update rules for weights \\(W_1\\) and \\(W_2\\) are\n\\(W_2 = W_2 - \\frac{\\eta}{n}\\hat{\\mathbf{h}}^TQ_2\\)\n\\(B_1 = B_1 - \\frac{\\eta}{n}Q_1\\)\n\\(W_1 = W_1 - \\frac{\\eta}{n}X^TQ_1\\)\nHint 1: Operations in \\((\\hat{Y}-Y)Y(1-Y)\\) are element-wise multiplications.\nHint 2: Operations in \\(\\hat{\\mathbf{h}}(1-\\hat{\\mathbf{h}})\\) are element-wise multiplications.\nHint 3: The resulting weight updates must have the same dimension as the weight matrices.",
    "crumbs": [
      "Course",
      "Fundamentals of deep learning",
      "Neural networks from scratch"
    ]
  },
  {
    "objectID": "course/deep_learning/neural_networks_from_scratch.html#example-task-handwritten-digits-with-the-mnist-dataset",
    "href": "course/deep_learning/neural_networks_from_scratch.html#example-task-handwritten-digits-with-the-mnist-dataset",
    "title": "Neural networks from scratch",
    "section": "2.2 Example task: handwritten digits with the MNIST dataset",
    "text": "2.2 Example task: handwritten digits with the MNIST dataset\nWe test the concepts introduced above using the MNIST dataset. MNIST stands for Modified National Institute of Standards and Technology and the dataset consists of \\(28\\times28\\) images of handwritten digits. Here, we will perform a regression task trying to predict the value of the digit from the image.\nWe will use the following architecture:\n\nInput layer with \\(m = 28\\times28 = 784\\) neurons.\nHidden layer with \\(h = 25\\) neurons.\nOuptut layer with \\(c = 1\\) neuron.\n\n\n\n\nnn_MNIST.svg\n\n\n\n2.2.1 Process the data\nWe start by importing the MNIST dataset\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n\n# split train and validation\nnp.random.seed(7)\nidx_rnd = np.random.permutation(np.arange(x_train.shape[0]))\nsplit = int(0.2*x_train.shape[0]) # Take 20% for validation\nidx_val, idx_train = idx_rnd[:split], idx_rnd[split:]\nx_val, y_val = x_train[idx_val], y_train[idx_val]\nx_train, y_train = x_train[idx_train], y_train[idx_train]\n\nn_train, n_val, n_test = x_train.shape[0], x_val.shape[0], x_test.shape[0]\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11493376/11490434 [==============================] - 1s 0us/step\n\n\nLet’s have a look at some examples to get a better idea about the task.\n\n\nCode\nk = 10\nfor i in range(0,10):\n    idx = np.where(y_train == i)[0] # find indices of i-digit\n    fig, ax = plt.subplots(1, k, sharey=True)\n    for j in range(0, k):\n        ax[j].imshow(x_train[idx[j]])\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBefore putting our images through the model, we first need to pre-process the data. We will:\n\nFlatten the images\nNormalize them \\(\\mathbf{x}_i \\to \\frac{\\mathbf{x}_i - \\text{mean}(\\mathbf{x}_i)}{\\text{std}(\\mathbf{x})}\\)\nBecause output of our network comes from a simgoid activation function in the range \\((0, 1)\\), we will bring the image labels \\(y \\in \\{0,1,2,\\dots,9\\}\\) to the \\((0,1)\\) range dividing by 10.\n\n\n# Flatten the images\nX_train = x_train.reshape(n_train, -1)\nX_val = x_val.reshape(n_val, -1)\nX_test = x_test.reshape(n_test, -1)\n\n# Normalize the data\ndef normalize(x):\n    return (x - np.mean(x, axis=1)[:, None])/np.std(x, axis=1)[:, None]\n\nX_train = normalize(X_train)\nX_val = normalize(X_val)\nX_test = normalize(X_test)\n\n# Bring the targets in range\nY_train = y_train/10.0\nY_val = y_val/10.0\nY_test = y_test/10.0\n\n\n\n2.2.2 Define the model\nLet’s define now the neural network parameters\n\n# NN parameters\nm = X_train.shape[1] # number of input neurons\nh = 21               # number of hidden neurons\nc = 1                # number of output neurons\n\nAnd define a forward and a backward function to compute the output and the gradients.\n\ndef forward(x, w_1, b_1, w_2):\n    \"Forward pass through our neural network.\"\n    h = x @ w_1 + b_1\n    h_hat = sigmoid(h)\n    g = h_hat @ w_2\n    return sigmoid(g), g, h_hat, h\n\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\n\ndef backward(x, y, y_pred, w_2, h_hat):\n    \"Backward pass through our neural network.\"\n    Q_2 = (y_pred-y[:, None])*y_pred*(1-y_pred)\n    Q_1 = Q_2 @ w_2.T*h_hat*(1-h_hat)\n\n    grad_w_1 = x.T @ Q_1\n    grad_b_1 = np.sum(Q_1, axis=0)\n    grad_w_2 = h_hat.T @ Q_2\n\n    return grad_w_1, grad_b_1, grad_w_2\n\n\n\n2.2.3 Train the model\nWe can now train the model!\n\n# Training parameters\neta = 50      # learning rate\nn_epoch = 500 # training epochs\n\n# Initialize the weights randomly\nnp.random.seed(0)\nw_1 = 1*(np.random.rand(m, h) - 0.5)  \nw_2 = 1*(np.random.rand(h, c) - 0.5)  \nb_1 = 1*(np.random.rand(h)   - 0.5)\n\n\nloss_train_vs_epoch = []\nloss_val_vs_epoch = []\n\nfor epoch in tqdm(range(n_epoch)):\n    # Forward pass\n    Y_pred, g, h_hat, h = forward(X_train, w_1, b_1, w_2)    \n    loss_train = 0.5*np.mean((Y_pred.squeeze() - Y_train)**2)    \n    loss_train_vs_epoch.append(loss_train)\n\n    # Backward pass\n    grad_w_1, grad_b_1, grad_w_2 = backward(X_train, Y_train, Y_pred, w_2, h_hat)\n\n    # Update parameters \n    w_1 -= eta/n_train*grad_w_1 \n    b_1 -= eta/n_train*grad_b_1\n    w_2 -= eta/n_train*grad_w_2\n\n    # Validate the performance\n    Y_pred, _, _, _ = forward(X_val, w_1, b_1, w_2)\n    loss_val = 0.5*np.mean((Y_pred.squeeze() - Y_val)**2)    \n    loss_val_vs_epoch.append(loss_val)\n\n\n\n\n\n\nCode\nprint(f\"Train and validation loss of {loss_train:.5f} and {loss_val:.5f}, respectively\")\n\n\nTrain and validation loss of 0.00625 and 0.00735, respectively\n\n\n\n\nCode\nplt.figure(figsize=(6, 4))\nplt.plot(loss_val_vs_epoch, label = \"validation\")\nplt.plot(loss_train_vs_epoch, label = \"training\")\nplt.grid(alpha=0.2)\nplt.title(\"MNIST\")\nplt.xlabel(\"Epoch\", fontsize=14)\nplt.ylabel(\"Loss (MSE)\", fontsize=14)\nplt.legend()\nplt.tick_params(labelsize=12);\n\n\n\n\n\n\n\n\n\nNow, we can look at the performance over unseen data from the test set.\n\n\nCode\nY_pred, _, _, _ = forward(X_test, w_1, b_1, w_2)\nloss_test = 0.5*np.mean((Y_pred.squeeze() - Y_test)**2)\nprint(f\"The test loss is {loss_test:.5f}\")\n\n\nThe test loss is 0.00699\n\n\nThe model seems to generalize fairly well, as the performance is comparable to the one obtained in the training set. Indeed, looking at the training losses, we see that the model is barely overfitting as there is almost no difference between the training and validation loss. This is mainly due to the simplicity of the model that we are considering.\nWe can also pretend for a moment that this is a classification task. This is definitely not how you would frame a classification problem, but we can assign prediction intervals to the MNIST labels and see how we would do.\n\n# Train set prediction\npred_train, _, _, _ = forward(X_train, w_1, b_1, w_2)\npred_train = np.around(10*pred_train).astype(int).squeeze()\ntrue_train = (10*Y_train).astype(int)\nconf_mat_train = get_confusion_matrix(pred_train, true_train)\naccuracy_train = (pred_train == true_train).mean()\n\n# Test set prediction\npred_test, _, _, _ = forward(X_test, w_1, b_1, w_2)\npred_test = np.around(10*pred_test).astype(int).squeeze()\ntrue_test = (10*Y_test).astype(int)\nconf_mat_test = get_confusion_matrix(pred_test, true_test)\naccuracy_test = (pred_test == true_test).mean()\n\n\n\nCode\nfig, ax = plt.subplots(1, 2, sharey=True, constrained_layout=True)\nax[0].imshow(conf_mat_train)\nax[0].set_title(\"Train | Accuracy : \" + \"{:2.2f}\".format(accuracy_train*100) + \"%\")\nax[0].set_xlabel(\"True label\", fontsize=14)\nax[0].set_ylabel(\"Predicted label\", fontsize=14)\nax[0].set_xticks(np.arange(0, 10))\nax[0].set_yticks(np.arange(0, 10))\n\nax[1].imshow(conf_mat_test)\nax[1].set_title(\"Test | Accuracy : \" + \"{:2.2f}\".format(accuracy_test*100) + \"%\")\nax[1].set_xlabel(\"True label\", fontsize=14)\nax[1].set_xticks(np.arange(0, 10))\nax[1].set_yticks(np.arange(0, 10));\n\n\n\n\n\n\n\n\n\nAs we can see, the accuracy matrix has quite diagonal structure! With an accuracy far beyond what we would obtain from a random guess! We see, however, that most errors occur between consecutive classes, which is mainly due to rounding errors from the imperfect regression.\n\n\n\n\n\n\nNote\n\n\n\nWe reiterate that this is not the proper way to handle a classification task. This is just an academic experiment to get familiar with the perceptron and see that neural networks are just a bunch of affine transformations.\n\n\nLet’s see if we can get a better understanding of the model by looking at some predictions:\n\n\nCode\nk = 5\nfor i in range(10):\n    idx = np.where(true_test == i)[0] # find indices of i-digit\n    fig, ax = plt.subplots(1, k, sharey=True)\n    for j in range(k):\n        title_string = \"\\n P: \" + \"{:01d}\".format(int(pred_test[idx[j]]))\n        ax[j].set_title(title_string)\n        ax[j].imshow(X_test[idx[j], :].reshape(28, 28))\n    # plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this examples, we see more clearly that, indeed, most errors occur due to categories being close to each other. For instance, all the errors in the images with 6s are either 5s or 7s. This is one of the main reasons why classification problems are not framed this way, but rather we treat every class as an independent instance of the rest.\n\n\n\n\n\n\nExercise\n\n\n\nCheck training/test accuracy and confusion matrix for different numbers of training data samples.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nSet initial biases to zero, and freeze its training. Check the change in the confusion matrix and accuracy.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCheck prediction accuracy and confusion matrix for weights and bias initialization taken from: 1. Uniform distribution [-0.5,0.5] 2. Uniform distribution [0,1] 3. Normal distribution \\({\\cal N}(0,1)\\)\n\n\n\n\n\n\n\n\nExercise\n\n\n\nImplement function for adaptative learning rate \\(\\eta\\) with the following rule: check relative change of the training loss after 10 epochs: if it is smaller than 5%, then \\(\\eta_{new} = \\kappa\\eta_{old}\\), \\(\\kappa&lt;1\\).",
    "crumbs": [
      "Course",
      "Fundamentals of deep learning",
      "Neural networks from scratch"
    ]
  },
  {
    "objectID": "course/deep_learning/neural_networks_from_scratch.html#activation-functions",
    "href": "course/deep_learning/neural_networks_from_scratch.html#activation-functions",
    "title": "Neural networks from scratch",
    "section": "2.3 Activation functions",
    "text": "2.3 Activation functions\nSo far we have been using softmax \\(\\sigma(z) = \\frac{1}{1+e^{-x}}\\) activation function only. The other activation functions are:\n\n\n\n0_lo8wlkwReDcXkts0.png\n\n\nLoss function should be calculated accordignly to the given activation function!",
    "crumbs": [
      "Course",
      "Fundamentals of deep learning",
      "Neural networks from scratch"
    ]
  },
  {
    "objectID": "course/deep_learning/neural_networks_from_scratch.html#stochastic-gradient-descent---sgd",
    "href": "course/deep_learning/neural_networks_from_scratch.html#stochastic-gradient-descent---sgd",
    "title": "Neural networks from scratch",
    "section": "3.1 Stochastic gradient descent - SGD",
    "text": "3.1 Stochastic gradient descent - SGD\nIn the standard gradient descent, we compute the gradient of the cost function with respect to the parameters for the entire training dataset. In most cases, it is extremely slow and even intractable for datasets that don’t even fit in memory. It also doesn’t allow us to update our model online, i.e. with new examples on-the-fly.\nIn SGD gradient descent, we use mini-batches comprised of a few training samples, and the model’s parameters are updated based on the average loss across the samples in each mini-batch. This way, SGD is able to make faster progress through the training dataset, and it can also make use of vectorized operations, which can make the training process more efficient.",
    "crumbs": [
      "Course",
      "Fundamentals of deep learning",
      "Neural networks from scratch"
    ]
  },
  {
    "objectID": "course/deep_learning/neural_networks_from_scratch.html#momentum",
    "href": "course/deep_learning/neural_networks_from_scratch.html#momentum",
    "title": "Neural networks from scratch",
    "section": "3.2 Momentum",
    "text": "3.2 Momentum\nMomentum optimization is an algorithm that can be used to improve SGD. It works by adding a fraction \\(\\gamma\\) of the previous parameter update to the current one, which helps the model make faster progress in the right direction and avoid getting stuck in local minima. This fraction is called the momentum coefficient, and it is a hyperparameter that can be adjusted according to the problem.\nThe momentum algorithm accumulates a history of the past gradients and continues to move in their direction:\n\\[\\begin{equation}\n\\begin{split}\ng_t &=  \\frac{\\partial L(\\theta_{t-1})}{\\partial \\theta}\\\\\nv_t &= \\gamma v_{t-1} - \\eta g_t \\\\\n\\theta &= \\theta + v_t,\n\\end{split}\n\\end{equation}\\] where \\(t\\) enumerates training epoch, \\(\\theta\\) are the trainable parameters of the Neural Network, \\(\\gamma\\) is the momentum coefficient and \\(\\eta\\) is the learning rate.\nThe velocity \\(v\\) accumulates the gradient of the loss function \\(L\\); the larger \\(\\gamma\\) with respect to \\(\\eta\\), the more previous gradients affect the current direction. In the standard SGD algorithm, the update size depended on the gradient and the learning rate. With momentum, it also depends on how large and how aligned consecutive gradients are.\nThe momentum algorithm can be understood as the simulation of a particle subjected to Newtonian dynamics. The position of particle at time \\(t\\) is given by value of the trainable parameters \\(\\theta(t)\\). The particle experiences the net force \\[\\begin{equation}\nf(t) = \\frac{\\partial^2}{\\partial t^2}\\theta(t),\n\\end{equation}\\] which can be expressed as a set of first time-derivatives: \\[\\begin{equation}\n\\begin{split}\nv(t) = \\frac{\\partial}{\\partial t}\\theta(t) \\\\\nf(t) = \\frac{\\partial}{\\partial t}v(t)\n\\end{split}\n\\end{equation}\\]\nIn the momentum optimization we have two forces: 1. One force is proportional to the negative gradient of the cost function \\[\\begin{equation}\nf_1 = -\\frac{\\partial L(\\theta)}{\\partial \\theta}.\n\\end{equation}\\] This force pushes the particle downhill along the cost function surface.\n\nSecond force is proportional to \\(-v(t)\\). Physical intuition is that this force corresponds to viscous drag, as if the particle must push through a resistant medium.\n\nIn addition to speeding up training, momentum optimization can also help the model to generalize better to new data.",
    "crumbs": [
      "Course",
      "Fundamentals of deep learning",
      "Neural networks from scratch"
    ]
  },
  {
    "objectID": "course/deep_learning/neural_networks_from_scratch.html#adaptative-gradient---adagrad",
    "href": "course/deep_learning/neural_networks_from_scratch.html#adaptative-gradient---adagrad",
    "title": "Neural networks from scratch",
    "section": "3.3 Adaptative Gradient - Adagrad",
    "text": "3.3 Adaptative Gradient - Adagrad\nAdaptative Gradient algorithm [2] is based on the idea of adapting the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters.\nThe AdaGrad algorithm works by accumulating the squares of the gradients for each parameter, and then scaling the learning rate for each parameter by the inverse square root of this sum. This has the effect of reducing the learning rate for parameters that have been updated frequently, and increasing the learning rate for parameters that have been updated infrequently.\nThe update rule for AdaGrad algorithm reads\n\\[\\begin{equation}\n\\begin{split}\n\\theta_{t+1} & = \\theta_t + \\Delta\\theta,\n\\end{split}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\begin{split}\n\\Delta \\theta &= - \\frac{\\eta}{\\sqrt{diag( \\epsilon\\mathbb{1} + G_t )}} \\odot g_t,\\\\\ng_t &= \\frac{\\partial L(\\theta_{t-1})}{\\partial \\theta}\\\\\nG_t &= \\sum_{\\tau = 1}^{t} g_\\tau g_\\tau^T.\n\\end{split}\n\\end{equation}\\] where \\(\\odot\\) means element-wise multiplication. The \\(\\epsilon \\ll 0\\) is a regularizing parameter, preventing from division by 0.\nAdagrad eliminates the need to manually tune the learning rate, i.e. initially \\(\\eta \\ll 1\\), and it is effectively adapted during training process. Algorithm can be sensitive to the choice of the initial learning rate, and it may require careful tuning to achieve good results.",
    "crumbs": [
      "Course",
      "Fundamentals of deep learning",
      "Neural networks from scratch"
    ]
  },
  {
    "objectID": "course/deep_learning/neural_networks_from_scratch.html#adadelta-extension-of-the-adaptative-gradient",
    "href": "course/deep_learning/neural_networks_from_scratch.html#adadelta-extension-of-the-adaptative-gradient",
    "title": "Neural networks from scratch",
    "section": "3.4 Adadelta: extension of the Adaptative Gradient",
    "text": "3.4 Adadelta: extension of the Adaptative Gradient\nAdadelta algorithm [3] is based on the idea of adapting the learning rate to the parameters, similar to AdaGrad, but it does not require the specification of a learning rate as a hyperparameter. Adadelta uses an Exponentially Weighted Moving Average (EWMA) of the squared gradients to scale the learning rate. The Exponentially Weighted Moving Average (EWMA) for \\(x_t\\) is defined recursively as:\n\\[\\begin{equation}\nE[x]_t = \\gamma E[x]_{t-1} + (1-\\gamma) x_t\n\\end{equation}\\]\nIn general Adadelta algorithm uses EMWA for \\(g_t^2\\) instead \\(G_t = \\sum_{\\tau = 1}^t g_\\tau g_\\tau^T\\), as in Adagrad, i.e.:\n\\[\\begin{equation}\nE[g^2]_t = \\gamma E[g^2]_{t-1} + (1-\\gamma)g_t^2,\n\\end{equation}\\] and we update parameters as \\[\\begin{equation}\n\\begin{split}\n\\theta_{t+1} & = \\theta_t + \\Delta\\theta_t \\\\\n\\Delta\\theta_t & = - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}}\\odot g_t.\n\\end{split}\n\\end{equation}\\] Let’s introduce notation \\(RMS[g]_t = \\sqrt{E[g^2]_t + \\epsilon}\\), where \\(RMS\\) stands for root-mean-square.\nHowever, Matthew D. Zeiler, the Author of Adadelta noticed that the parameter updates \\(\\Delta\\theta\\) being applied to \\(\\theta\\) shoud have matching units. Considering, that the parameter had some hypothetical units \\([\\theta]\\), the changes to the parameter should be changes in those units as well, i.e. \\[\\begin{equation}\n[\\theta] = [\\Delta\\theta].\n\\end{equation}\\] However, assuming the loss function is unitless, we have \\[\\begin{equation}\n[\\Delta\\theta] = \\frac{1}{[\\theta]},\n\\end{equation}\\] thus units do not match. This is the case for SGD, Momentum, or Adagrad algorithms.\nThe second order methods such as Newton’s method that use the second derivative information preserve units for the parameter updates. For function \\(f(x)\\), we have \\[\\begin{equation}\n\\Delta x = \\frac{\\frac{\\partial f}{\\partial x}}{\\frac{\\partial^2 f}{\\partial x^2}},\n\\end{equation}\\] thus units \\([\\Delta x] = [x]\\) are preserved. Keeping this in mind, the update rule in Adadelta algorithm is defined as: \\[\\begin{equation}\n\\begin{split}\n  \\theta_{t+1} & = \\theta_t + \\Delta_t\\theta_t \\\\\n  \\Delta\\theta_t &= -\\frac{RMS[\\Delta\\theta]_{t-1}}{RMS[g]_t}\\odot g_t.\n\\end{split}\n\\end{equation}\\]\nThis has the effect of automatically adapting the learning rate to the characteristics of the problem, which can make it easier to use than other optimization algorithms that require manual tuning of the learning rate.",
    "crumbs": [
      "Course",
      "Fundamentals of deep learning",
      "Neural networks from scratch"
    ]
  },
  {
    "objectID": "course/deep_learning/neural_networks_from_scratch.html#adaptive-moment-estimation---adam",
    "href": "course/deep_learning/neural_networks_from_scratch.html#adaptive-moment-estimation---adam",
    "title": "Neural networks from scratch",
    "section": "3.5 Adaptive Moment Estimation - Adam",
    "text": "3.5 Adaptive Moment Estimation - Adam\nAdam algorithm [4] combines the ideas of momentum optimization and Adagrad to make more stable updates and achieve faster convergence.\nLike momentum optimization, Adam uses an exponentially decaying average of the previous gradients to determine the direction of the update. This helps the model to make faster progress in the right direction and avoid oscillations. Like AdaGrad, Adam also scales the learning rate for each parameter based on the inverse square root of an exponentially decaying average of the squared gradients. This has the effect of reducing the learning rate for parameters that have been updated frequently, and increasing the learning rate for parameters that have been updated infrequently.\nAdam uses Exponentially Modified Moving Average for gradients and its square:\n\\[\\begin{equation}\n\\begin{split}\ng_t &= \\frac{\\partial L(\\theta_{t-1})}{\\partial \\theta}\\\\\nm_t &= \\beta_1 m_{t-1} + (1-\\beta_1)g_t \\\\\nv_t &= \\beta_2 v_{t-1} + (1-\\beta_2)g_t^2.\n\\end{split}\n\\end{equation}\\]\nThe update rule for the parameters reads:\n\\[\\begin{equation}\n\\Delta \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon}\\hat{m}_t,\n\\end{equation}\\] where \\[\\begin{equation}\n\\begin{split}\n\\hat{m}_t &= \\frac{m_t}{1-\\beta_1}\\\\\n\\hat{v}_t &= \\frac{v_t}{1-\\beta_2},\n\\end{split}\n\\end{equation}\\] are bias-corrected first and second gradient moments estimates.\nAuthors suggest to set \\(\\beta_1 = 0.9\\), \\(\\beta_2 = 0.999\\), \\(\\eta = 10^{-8}\\).\n\n\n\n\n\n\nExercise\n\n\n\nImplement mini-batch SGD algorithm. In the training loop feed-forward training data with fixed size batches and update gradients after each batch. Steps:\n\nFix batch size \\(n_b\\)\nCalculate number of batches \\(N_b = N_{train}/n_b\\)\nLoop over training epochs:\n3.1. Loop over number of batches:\n 3.1.1 at each iteration randomly choose batch size training points from training data set\n\n 3.1.2 feed-forward through network\n\n 3.1.3 backpropagate and update weights\n3.2 Calculate loss on test dataset, and on a single batch from the training dataset",
    "crumbs": [
      "Course",
      "Fundamentals of deep learning",
      "Neural networks from scratch"
    ]
  },
  {
    "objectID": "course/deep_learning/automatic_differentiation_pytorch.html#forward-function-evaluation",
    "href": "course/deep_learning/automatic_differentiation_pytorch.html#forward-function-evaluation",
    "title": "Automatic differentiation",
    "section": "2.1 Forward function evaluation",
    "text": "2.1 Forward function evaluation\nLet us consider expression \\(f(x_1,x_2) = \\ln x_1 + \\cos x_2 - x_1 x_2\\), Function \\(f\\) is a mapping \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\) with \\(n = 2\\), \\(m=1\\).\nFollowing [1], we introduce the notation for the computational graph as follows:\n\nInput variables are denoted as \\(v_{1-i}\\), where \\(i = 1,\\dots,n\\).\nIntermediate variables are denoted as \\(v_i\\), \\(i = 1,\\dots,l\\).\nOutput variables are denoted as \\(v_{l+i}\\), \\(i = 1,\\dots,m\\).\n\nThe computational graph related to considered function \\(f(x_1,x_2)\\) is\n\n\n\ncomputational-graph.png\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCalculate value of \\(f(x_1,x_2)\\) at \\((x_1, x_2) = (2,1)\\) via passing the diagram from left to right:",
    "crumbs": [
      "Course",
      "Fundamentals of deep learning",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "course/deep_learning/automatic_differentiation_pytorch.html#calculating-gradients-1",
    "href": "course/deep_learning/automatic_differentiation_pytorch.html#calculating-gradients-1",
    "title": "Automatic differentiation",
    "section": "2.2 Calculating gradients",
    "text": "2.2 Calculating gradients\nAutomatic differentiation allows us to calculate exact value of the gradient at given point. In our example, we are interested in value of \\(\\frac{\\partial f}{\\partial x_1}\\) at given point \\((x_1, x_2) = (2,1)\\). This can be obtain in two modes.\n\n2.2.1 Forward-mode AD\nForward-mode AD is implemented by complementing each intermediate variable \\(v_i\\) with a derivative: \\[\\begin{equation}\n\\dot{v}_i = \\frac{\\partial v_i}{\\partial x_1},\n\\end{equation}\\] and by applying chain rule for differentiation we can obtain desired gradient. Derivativeas are propagated forward in sync with the function evaluation.\n\n\n\n\n\n\nExercise\n\n\n\nCalculate value of \\(\\frac{\\partial f}{\\partial x_1} = \\dot{v}_5\\) at \\((x_1, x_2) = (2,1)\\) through passing the diagram: \n\n\n\n\n\n\n\n\nDual numbers and forward-mode AD\n\n\n\nIn practive, forward-mode is implemented by extending the algebra of real numbers via introducing \\(\\textit{dual}\\) numbers, defined as \\[\\begin{equation}\n\\tilde{z}_1 = a_1 +\\epsilon b_1,\n\\end{equation}\\] where \\(a,b \\in \\mathbb{R}\\), and \\(\\epsilon^2 = 0\\). Next, addition and multiplication of dual numbers is defined as:\n\nAddition: \\(z_1 + z_2 = (a_1 + a_2) + \\epsilon(b_1 + b_2)\\)\nMultiplication: \\(z_1z_2 = a_1a_2 + + a_1b_2\\epsilon +b_1a_2\\epsilon + b_1b_2\\epsilon^2 = a_1a_2 + \\epsilon(a_1b_2+a_2b_1)\\)\n\nNext, when we consider Taylor series expansion around \\(\\epsilon\\), we have\n\\[\\begin{equation}\nf(z) = f(a+\\epsilon) = f(a) + f'(a)\\epsilon + \\frac{1}{2}f''(a)\\epsilon^2 + \\dots,\n\\end{equation}\\] we see that this simplifies to \\[\\begin{equation}\nf(a+\\epsilon) = f(a) + \\epsilon f'(a),\n\\end{equation}\\] which means that operations on dual number \\(a\\) automatically provides numerical value for \\(f(a)\\) and derivative \\(f'(a)\\).\nIn numerical implementation, dual numbers are handled by operator overloading where all mathematical operators are working appropriately on the new algebra of dual numbers.\n\n\n\n\n2.2.2 Reverse-mode (backpropagation) AD\nIn a reverse mode we calculate gradients backwards. Let’s have a look at our computational graph once more time:\n\n\n\ncomputational-graph.png\n\n\nWe are interested in calculating derivative of \\(y\\) with respect to \\(v_i\\), i.e. $ $. For a computational graph we can write the chain rule as \\[\\begin{equation}\n\\frac{\\partial y_j}{\\partial v_i} = \\frac{\\partial y_j}{\\partial v_k}\\frac{\\partial v_k}{\\partial v_i},\n\\end{equation}\\] where \\(v_k\\) is a parent of a \\(v_i\\) in a computational graph. When \\(v_i\\) has more than one parent we sum up the chain rule as: \\[\\begin{equation}\n\\frac{\\partial y_j}{\\partial v_i} = \\sum_{p\\in parents(i)} \\frac{\\partial y_j}{\\partial v_p}\\frac{\\partial v_p}{\\partial v_i}.\n\\end{equation}\\] In the literature the above expression is called as \\(\\textit{adjont}\\) and denoted as \\[\\begin{equation}\n\\bar{v}_i = \\frac{\\partial y_i}{\\partial v_i}.\n\\end{equation}\\]\nNext, we can rewrite the adjont in term of the adjonts of the parents, i.e. \\[\\begin{equation}\n\\bar{v}_i = \\sum_{p\\in \\text{parents(i)}} \\bar{v}_p \\frac{\\partial v_p}{\\partial v_i}\n\\end{equation}\\] which gives us a recursive algorithm node \\(y\\) with setting starting point as \\(\\bar{y} = 1\\).\nLet’s write parents of each node in our example: \\[\\begin{equation}\n\\begin{split}\n\\text{parents}(i=5) &\\to  y \\\\\n\\text{parents}(i=4) &\\to \\{v_5\\} \\\\\n\\text{parents}(i=3) &\\to \\{v_5\\} \\\\\n\\text{parents}(i=2) &\\to \\{v_4\\} \\\\\n\\text{parents}(i=1) &\\to \\{v_4\\} \\\\\n\\text{parents}(i=0) &\\to \\{v_2, v_3\\} \\\\\n\\text{parents}(i=-1) &\\to \\{v_1, v_2\\}\\\\\n\\end{split}\n\\end{equation}\\]\nNow we can write adjonts: \\[\\begin{equation}\n\\begin{split}\n  \\bar{v}_5 & = \\bar{y} \\\\\n  \\bar{v}_4 & = \\bar{v}_5\\frac{\\partial v_5}{\\partial v_4}\\\\\n  \\bar{v}_3 & = \\bar{v}_5\\frac{\\partial v_5}{\\partial v_3}\\\\\n  \\bar{v}_2 & = \\bar{v}_4\\frac{\\partial v_4}{\\partial v_2}\\\\\n  \\bar{v}_1 & = \\bar{v}_4\\frac{\\partial v_4}{\\partial v_1}\\\\\n  \\bar{v}_0 & = \\bar{v}_2\\frac{\\partial v_2}{\\partial v_0} + \\bar{v}_3\\frac{\\partial v_3}{\\partial v_0} \\\\\n  \\bar{v}_{-1} & = \\bar{v}_1\\frac{\\partial v_1}{\\partial v_{-1}} + \\bar{v}_2\\frac{\\partial v_2}{\\partial v_{-1}} \\\\\n\\end{split}\n\\end{equation}\\]\nFinally, we notice that \\[\\begin{equation}\n\\begin{split}\n\\bar{v}_0    & = \\bar{x}_2 = \\frac{\\partial y}{\\partial x_2}\\\\\n\\bar{v}_{-1} & = \\bar{x}_1 = \\frac{\\partial y}{\\partial x_1}.\n\\end{split}\n\\end{equation}\\]\nIn other words, with the single backward pass we have both \\(\\frac{\\partial y}{\\partial x_1}\\) and \\(\\frac{\\partial y}{\\partial x_2}\\) (in forward mode we can obtain \\(\\frac{\\partial y}{\\partial x_1}\\) in one pass).\n\n\n\n\n\n\nExercise\n\n\n\nCalculate value of \\(\\frac{\\partial f}{\\partial x_1} = \\dot{v}_5\\) at \\((x_1, x_2) = (2,1)\\) through passing the diagram: \n\n\n\n\n\n\n\n\nHow to choose between forward-mode and reverse-mode?\n\n\n\nLet’s consider function \\(f:\\mathbb{R}^m \\to \\mathbb{R}^n\\)\n\nIf \\(m \\ll n\\), i.e. number of inputs is much smaller than number of outputs, from computational point of view it is more faborable to use forward-mode automatic differentiation.\nIf \\(m \\gg n\\), i.e. number of inputs is much larger than number of outputs (and this is the case of neural networks), from computational point of view it is more faborable to use backward-mode automatic differentiation.",
    "crumbs": [
      "Course",
      "Fundamentals of deep learning",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "course/deep_learning/automatic_differentiation_pytorch.html#calculating-gradietns-with-pytorch-torch.autograd",
    "href": "course/deep_learning/automatic_differentiation_pytorch.html#calculating-gradietns-with-pytorch-torch.autograd",
    "title": "Automatic differentiation",
    "section": "2.3 Calculating gradietns with PyTorch: torch.autograd",
    "text": "2.3 Calculating gradietns with PyTorch: torch.autograd\ntorch.autograd is PyTorch’s automatic differentiation engine that helps in neural network training.\n\n2.3.1 Example 1\nTo compute the gradient of a scalar function \\(f\\) with respect to a single variable \\(x\\), we can use PyTorch’s autograd module. For example:\n\nimport torch\n\n# Create a tensor with requires_grad set to True\nx = torch.tensor([4.0], requires_grad=True)\n\n# Define a scalar function f\ndef f(x):\n    return x ** 2\n\n# Compute the gradient of f with respect to x\ny = f(x)\ny.backward()\n\n# The gradient of f with respect to x is stored in x.grad\nprint(x.grad)\n\ntensor([8.])\n\n\n\n\n2.3.2 Example 2\nTo compute the gradient of a function with respect to multiple variables, we can pass a tensor with requires_grad set to True to the function and then use the backward method on the resulting tensor.\n\nimport torch\n\n# Create tensors with requires_grad set to True\nx = torch.tensor([1.0], requires_grad=True)\ny = torch.tensor([1.0], requires_grad=True)\nz = torch.tensor([1.0], requires_grad=True)\n# Define a function that takes two variables as input and returns their sum\ndef f(x, y, z):\n    return torch.log(torch.sin(x) + torch.tanh(y**2))/z\n\n# Compute the gradient of f with respect to x and y\ng = f(x, y, z)\ng.backward()\n\n# The gradients of f with respect to x and y are stored in x.grad and y.grad\nprint(x.grad)   \nprint(y.grad)   \nprint(z.grad)\n\ntensor([0.3370])\ntensor([0.5240])\ntensor([-0.4719])\n\n\n\n\n2.3.3 Example 3\nAutomatic differentiation can be used in more complicated problems.\nLet us consider eigenproblem for the double well potential modeled as a quantum harmonic oscillator with the barrier modeled as a gaussian profile:\n\\[\\begin{equation}\n\\begin{split}\n\\hat{H}\\psi_n(x) & = E_n(x)\\psi_n(x) \\\\\n\\hat{H} & = -\\frac{1}{2}\\frac{\\partial^2}{\\partial x^2} + \\frac{1}{2}x^2 + \\kappa e^{-x^2/2}\n\\end{split}\n\\end{equation}\\]\nWe are interested in change of the ground state energy \\(E_{0}\\) as a function of \\(\\kappa\\) parameter. To tackle this problem, we have to first calculate eigenstates of the considered Hamiltonian.\nLet us consider equaly distributed set of points \\(x_i\\) lying in interval \\([-L/2, L/2]\\) with \\(\\Delta x = \\frac{L}{N_x}\\), where \\(N_x\\) is a number of discretization points.\nFirst, we have to, i.e. solve the eigenproblem of the form \\[\\begin{equation}\n\\bar{H}\\vec{\\psi}_n = E_n \\vec{\\psi}_n,\n\\end{equation}\\] where \\(\\bar{H}\\) is \\(N_x\\times N_x\\) matrix representation of Hamiltonian \\(\\hat{H}\\) in a discretized space. After constructing the matrix \\(\\bar{H}\\), we can diagonalize it, and find eigenvectors \\(\\vec{\\psi}_n\\), and corresponding eigenvalues \\(E_n\\). Note, \\(n\\in \\{0,N_x\\}\\).\n\n\n\n\n\n\nDiscrete second-order derivative and Hamiltonian matrix representation\n\n\n\nTo construct the matrix representation of \\(\\hat{H}\\), we have to implement discrete second order derivative: \\[\\begin{equation}\n\\frac{\\partial^2 \\psi(x)}{\\partial x^2}\\big|_{x} \\approx \\frac{\\psi_{i+1} - 2\\psi_i + \\psi_{i-1}}{\\Delta x^2}\n\\end{equation}\\]\nThe matrix representation of the kinetic part of the Hamiltonian \\(\\hat{H}\\) is\n\\[\\begin{equation}\n\\bar{H}_{\\text{T}} = -\\frac{1}{2}\\frac{1}{\\Delta x}\\begin{bmatrix}\n-2 & 1 & 0 & 0 & 0 & . \\\\\n1 & -2 & 1 & 0 & 0 & . \\\\\n0 & 1 & -2 & 1 & 0 & . \\\\\n0 & 0 & 1 & -2 & 1 & . \\\\\n0 & 0 & 0 & 1 & -2 & 1 \\\\\n. & . & . & . & . & .\n\\end{bmatrix}  \n\\end{equation}\\]\nThe matrix representation of the potential part of the Hamiltonian \\(\\hat{H}\\) is a diagonal matrix with elements \\([\\bar{H}_\\text{V}]_{i,j} = \\delta_{i,j} \\big(\\frac{1}{2}x_i^2 + \\kappa e^{-x_i^2/2} \\big)\\)\nNow, the Hamiltonian matrix representation is simply \\(\\bar{H} = \\bar{H}_T + \\bar{H}_V\\).\n\n\nLet’s import necessary libraries\n\nimport matplotlib.pyplot as plt\nimport torch\nimport numpy as np\nfrom torch.linalg import eigh\n\nLet’s define function returning matrix of the considered Hamiltonian. First, let’s define system size \\(L = 10\\), \\(N_x = 500\\) discretization points, function returning external potential and function returning matrix representation of the Hamiltonian:\n\nL = 10\nNx = 100\nx = torch.linspace(-L/2,L/2,Nx)\ndx = x[1]-x[0]\n\n\n\ndef get_potential(x,kappa):\n    return 0.5*x**(2.0) + kappa*np.exp(-(x)**2/2) \n    \ndef get_H(kappa):\n    H_T = torch.zeros((Nx,Nx))\n    ones = torch.ones(Nx)\n    H_T = -0.5/dx**2*( torch.diag_embed(ones[:-1], offset = 1) - 2*torch.diag_embed(ones, offset = 0) + torch.diag_embed(ones[:-1], offset = -1))  \n    H_V = torch.diag(get_potential(x,kappa))\n    H = H_T + H_V\n    return H\n\nLet’s have a look at potential shape for different parameters \\(\\kappa\\):\n\nkappa_vec = torch.tensor([0, 2, 4, 10])\n \nN_kappa = kappa_vec.size(0)\n \nfig, ax = plt.subplots(1, N_kappa, figsize=(16,4))\nFontSize = 16\nfor kappa_i in range(0, N_kappa):\n    kappa = kappa_vec[kappa_i]\n    V = get_potential(x,kappa)\n    ax[kappa_i].plot(x,V.detach().numpy())\n    ax[kappa_i].set_title(r\"$\\kappa = $ \" + \"{:2.2f}\".format(kappa.item()))\n    ax[kappa_i].set_xlabel(\"x\",fontsize=FontSize)\nax[0].set_ylabel(r\"$V(x)$\",fontsize=FontSize)\nplt.show()\n\n\n\n\n\n\n\n\nPytorch function torch.eigh calculates eigenfunctions and eigenvalues of a given matrix: this allows us to write two functions returning ground state energy \\(E_{\\text{GS}}\\), and ground state gap \\(\\Delta E\\), i.e. energy difference between two first eigenenergies:\n\ndef get_energy_ground_state(kappa):\n    H = get_H(kappa)\n    Energies, Vectors = eigh(H)\n    E_GS = Energies[0]    \n    return E_GS\n\ndef get_gap(kappa):\n    H = get_H(kappa)\n    Energies, Vectors = eigh(H)\n    gap = Energies[1] - Energies[0]    \n    return gap\n\nLet’s see density of few first eigenstates of the Hamiltonian for given parameter \\(\\kappa\\):\n\nfig, ax = plt.subplots(3, N_kappa, figsize=(16,12))\nFontSize = 16\nfor kappa_i in range(0, N_kappa):\n    kappa = kappa_vec[kappa_i]\n    V = get_potential(x,kappa)\n    ax[0,kappa_i].plot(x,V.detach().numpy())\n    ax[0,kappa_i].set_title(r\"$\\kappa = $ \" + \"{:2.2f}\".format(kappa.item()),fontsize=FontSize)\n    ax[0,kappa_i].set_xlabel(\"x\",fontsize=FontSize)\n\n    H = get_H(kappa)\n    Energies, Vectors = eigh(H)\n    rho = torch.abs(Vectors)**2/dx\n    n_max =  4            # maximal number of eigenstates\n    E_max = Energies[n_max] #\n \n    for i in range(0,n_max):\n        ax[1,kappa_i].plot(x,rho[:,i].detach().numpy())\n    ax[2,kappa_i].plot(Energies[0:n_max].detach().numpy(),'x')\n   \n    ax[0,kappa_i].set_xlabel(\"-0\",fontsize=FontSize)\n\n    ax[2,kappa_i].set_xlabel(r\"n\",fontsize=FontSize)\n    ax[2,kappa_i].set_yticks(np.arange(0,E_max,0.5))\n    \n    \nax[0,0].set_ylabel(r\"$V(x)$\",fontsize=FontSize) \nax[1,0].set_ylabel(r\"$|\\psi_n(x)|^2$\",fontsize=FontSize) \nax[2,0].set_ylabel(r\"$E_n$\",fontsize=FontSize) \nplt.show()\n\n\n\n\n\n\n\n\nNow, let’s check how ground state energy changes with \\(\\kappa\\):\n\nfig = plt.figure()\nE_gs_vs_kappa = []\ngap_vs_kappa = []\nkappa_max = 11\nkappa_vec = np.linspace(0,kappa_max,100)\nfor kappa in kappa_vec:\n    E_gs_vs_kappa.append([get_energy_ground_state(kappa)])\n    gap_vs_kappa.append([get_gap(kappa)])\n    \nFontSize = 16\nfig, ax = plt.subplots(1,2,figsize=(10,4))\nax[0].plot(kappa_vec, E_gs_vs_kappa, '--')\nax[1].plot(kappa_vec, gap_vs_kappa, '--')\nax[0].set_xlabel(r\"$\\kappa$\",fontsize=FontSize)\nax[0].set_ylabel(r\"ground state energy $E_{GS}$\",fontsize=FontSize)\nax[1].set_xlabel(r\"$\\kappa$\",fontsize=FontSize)\nax[1].set_ylabel(r\"energy gap $\\Delta{E}$\", fontsize=FontSize)\n\nText(0, 0.5, 'energy gap $\\\\Delta{E}$')\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nFinally, let’s calculate derivative \\(\\frac{d E_{\\text{GS}}(\\kappa)}{d \\kappa}\\) at given \\(\\kappa\\) using torch.autograd:\n\nkappa_fixed = torch.tensor(10., requires_grad = True)\nE_GS = get_energy_ground_state(kappa_fixed)\nE_GS.backward()\nprint(kappa_fixed.grad)\n\ntensor(0.1249)\n\n\nLet’s plot \\(\\frac{d E_{\\text{GS}}(\\kappa)}{d \\kappa}\\), and \\(\\frac{d \\Delta E_{\\text{GS}}(\\kappa)}{d \\kappa}\\)\n\ndEdkappa = []\ndgapdkappa = []\nfor kappa in kappa_vec:\n    kappa_fixed_1 = torch.tensor(kappa, requires_grad = True)\n    E_GS = get_energy_ground_state(kappa_fixed_1)\n    E_GS.backward()\n    diff = kappa_fixed_1.grad\n    dEdkappa.append([diff.item()])\n    \n    kappa_fixed_2 = torch.tensor(kappa, requires_grad = True)    \n    gap = get_gap(kappa_fixed_2)\n    gap.backward()\n    diff = kappa_fixed_2.grad\n    dgapdkappa.append([diff.item()])    \n\nfig, ax = plt.subplots(1,2,figsize=(14,4))\nax[0].plot(kappa_vec,dEdkappa,'--')\nax[1].plot(kappa_vec,dgapdkappa,'--')\nFontSize=16\nax[0].set_xlabel(r\"$\\kappa$\",fontsize=FontSize)\nax[0].set_ylabel(r\"$\\frac{dE}{d\\kappa}$\",fontsize=FontSize)\n\nax[1].set_xlabel(r\"$\\kappa$\",fontsize=FontSize)\nax[1].set_ylabel(r\"$\\frac{d\\Delta E}{d\\kappa}$\",fontsize=FontSize)\n\nText(0, 0.5, '$\\\\frac{d\\\\Delta E}{d\\\\kappa}$')",
    "crumbs": [
      "Course",
      "Fundamentals of deep learning",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine learning course",
    "section": "",
    "text": "The content of this course was originally developed for the master in quantum science and technology in Barcelona, where we taught the Machine learning for quantum and classical systems course. The content of the course has been progressively expanded, updated and adapted, as we have been developing new mateiral and using this course to teach in multiple masters, schools, conferences, and workshops.\nPlease, visit the  Course and Documentation page to get started!"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Machine learning course",
    "section": "Course Description",
    "text": "Course Description\nThis course gives an introduction to machine learning and deep learning: starting from linear linear models al the way up to state of the art generative models. The material covers the following topics:\n\nWhat is learning?\nLinear Models (linear regression, polynomial regression and logistic regression)\nA probabilistic view of machine learning\nFundamentals of deep learning\nConvolutional Neural Networks\nRestricted Boltzmann Machines\nGenerative Models\n\nThe course combines theory and practice in the form of jupyter notebooks with python. We make extensive use of specific librairies such as numpy, PyTorch and fastai.\nSome of the content has been adapted from our book in machine learning for the quantum sciences (Dawid et al. 2022), as well as other material such as our PhD theses."
  },
  {
    "objectID": "index.html#original-instructors-and-main-contributors",
    "href": "index.html#original-instructors-and-main-contributors",
    "title": "Machine learning course",
    "section": "Original instructors and main contributors",
    "text": "Original instructors and main contributors\n\n\n\n\n\n\n\n\n\nAlexandre Dauphin\n\n\n\n\n\n\n\nBorja Requena\n\n\n\n\n\n\n\nMarcin Płodzień\n\n\n\n\n\n\n\nPaolo Stornati"
  },
  {
    "objectID": "homeworks/generative.html",
    "href": "homeworks/generative.html",
    "title": "3 - Generative models",
    "section": "",
    "text": "In our session about typical machine learning applications, we saw a bunch of prototypical tasks on various fields. A prominent example was generating images from a given description we wrote. This is possible thanks to the latest advances in generative models: stable diffusion, the current state-of-the-art text to image model.\nIn this homework, we will download a pre-trained stable diffusion model and we will have a bit of fun with it! However, we will not dive into the details or inner workings of stable diffusion, as they are far beyond the level we have covered in the course so far. The goal of this homework is to become familiar with the process of downloading a pre-trained model, follow a tutorial and troubleshoot any potential issues we may encounter (send your questions by email to Borja). All while having some fun!\nThe first part of the homework will focus on becoming familiar with the library that we will use to perform the tasks. To do so, we will adapt part of the tutorial in the fastai course by Pedro Cuenca, Patrick von Platen, Suraj Patil and Jeremy Howard, which uses the 🤗 Hugging Face 🧨 Diffusers library to download and use diffusion models.\nThen, we describe the tasks to conduct to write your report, which are related to the introductory part.",
    "crumbs": [
      "Homework",
      "3 - Generative models"
    ]
  },
  {
    "objectID": "homeworks/generative.html#stable-diffusion-pipeline",
    "href": "homeworks/generative.html#stable-diffusion-pipeline",
    "title": "3 - Generative models",
    "section": "Stable Diffusion Pipeline",
    "text": "Stable Diffusion Pipeline\nStableDiffusionPipeline is an end-to-end diffusion inference pipeline that allows us to generate images with just a few lines of code. Many Hugging Face libraries (along with others like scikit-learn) use the concept of a “pipeline” to indicate a sequence of steps that achieve a task when combined.\n\n\n\n\n\n\nImplementation details\n\n\n\nWe use from_pretrained to create the pipeline and download the pretrained weights. We indicate that we want to use the fp16 (half-precision) version of the weights, and we tell diffusers to expect the weights in that format. This allows us to perform much faster inference with almost no discernible difference in quality. The string passed to from_pretrained in this case (CompVis/stable-diffusion-v1-4) is the repo id of a pretrained pipeline hosted on Hugging Face Hub; it can also be a path to a directory containing pipeline weights. The weights for all the models in the pipeline will be downloaded and cached the first time you run this cell.\n\n\nLet’s create our pipeline!\n\npipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16).to(\"cuda\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf your GPU is not big enough to use pipe, run pipe.enable_attention_slicing() in the cell right below.\nAs described in the docs: “When this option is enabled, the attention module will split the input tensor in slices, to compute attention in several steps. This is useful to save some memory in exchange for a small speed decrease.”\n\n\n\n#pipe.enable_attention_slicing()\n\nNow that we have a working pipeline, let’s generate some images. As we saw in class, we need to provide this model with a description of the image that we want to generate.\n\nprompt = \"a photograph of an astronaut riding a horse\"\n\n\n\nCode\nimages = pipe(prompt).images\nimages[0]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese models are stochastic, so every time we execute the code with the same prompt, we will obtain a different image (try executing the cell above again!). We can fix the result by providing a manual seed for the random number genertor in pytorch with torch.manual_seed(), as we do below. This allows us to consistently see the effect of the parameters on the output.\n\n\n\n\nCode\ntorch.manual_seed(1024)\npipe(prompt).images[0]\n\n\n\n\n\n\n\n\n\n\n\n\nRunning the pipeline shows a progress bar with a certain number of steps. This is because Stable Diffusion is based on a progressive denoising algorithm that is able to create convincing images from pure random noise. Models in this family are known as diffusion models. Here, we show an example of the process starting from random noise at top, to progressively improved images towards the bottom, of a model that creates handwritten digits.\n\n\n\nimage.png\n\n\nWe can change the number of denoising steps in a trade-off between faster predictions and image quality. Let’s see what happens when we try to generate the image from above with only 3 and 16 steps.\n\n\nCode\ntorch.manual_seed(1024)\npipe(prompt, num_inference_steps=3).images[0]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntorch.manual_seed(1024)\npipe(prompt, num_inference_steps=16).images[0]",
    "crumbs": [
      "Homework",
      "3 - Generative models"
    ]
  },
  {
    "objectID": "homeworks/generative.html#classifier-free-guidance",
    "href": "homeworks/generative.html#classifier-free-guidance",
    "title": "3 - Generative models",
    "section": "Classifier-Free Guidance",
    "text": "Classifier-Free Guidance\nClassifier-Free Guidance is a method to increase the adherence of the output to the conditioning signal we used (the text).\nRoughly speaking, the larger the guidance the more the model tries to represent the text prompt. However, large values tend to produce less diversity. The default is 7.5, which represents a good compromise between variety and fidelity. This blog post goes into deeper details on how it works.\nWe can generate multiple images for the same prompt by simply passing a list of prompts instead of a string.\n\nnum_rows, num_cols = 4, 4\nprompts = [prompt] * num_cols\nguidances = [1.1, 3, 7, 14]\n\n\nimages = concat(pipe(prompts, guidance_scale=g).images for g in guidances)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimage_grid(images, rows=num_rows, cols=num_cols)",
    "crumbs": [
      "Homework",
      "3 - Generative models"
    ]
  },
  {
    "objectID": "homeworks/generative.html#negative-prompts",
    "href": "homeworks/generative.html#negative-prompts",
    "title": "3 - Generative models",
    "section": "Negative prompts",
    "text": "Negative prompts\nNegative prompting refers to the use a second prompt to increase the difference between generations of the original prompt and the second one.\n\nprompt = \"Labrador in the style of Vermeer\"\n\n\n\nCode\ntorch.manual_seed(1000)\npipe(prompt).images[0]\n\n\n\n\n\n\n\n\n\n\n\n\n\nneg_prompt = \"blue\"\n\n\n\nCode\ntorch.manual_seed(1000)\npipe(prompt, negative_prompt=neg_prompt).images[0]\n\n\n\n\n\n\n\n\n\n\n\n\nWith the negative prompt, we move more towards the direction of the positive one reducing the importance of the negative prompt in our composition. Here, we see how the blue scarf has disappeared from the image.",
    "crumbs": [
      "Homework",
      "3 - Generative models"
    ]
  },
  {
    "objectID": "homeworks/generative.html#image-to-image",
    "href": "homeworks/generative.html#image-to-image",
    "title": "3 - Generative models",
    "section": "Image to Image",
    "text": "Image to Image\nEven though Stable Diffusion was trained to generate images, and optionally drive the generation using text conditioning, we can use the raw image diffusion process for other tasks.\nFor example, instead of starting from pure noise, we can start from an image an add a certain amount of noise to it. This way, we skip the initial steps of the denoising process and pretend our noisy image is what the algorithm came up with from the raw noise. Then we continue the diffusion process from that state as usual. This usually preserves the composition, although the details may change significantly. This process works wonders with sketches!\n\n\n\n\n\n\nImplementation details\n\n\n\nWe can do these operations with a special image to image pipeline: StableDiffusionImg2ImgPipeline. This is the source code for its __call__ method, which takes an initial image, adds some noise to it and runs the diffusion process from there.\n\n\n\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    revision=\"fp16\",\n    torch_dtype=torch.float16,\n).to(\"cuda\")\n\n\n\n\nAs an example, we can use the sketch created by VigilanteRogue81.\n\n\nCode\np = FastDownload().download('https://s3.amazonaws.com/moonup/production/uploads/1664665907257-noauth.png')\ninit_image = Image.open(p).convert(\"RGB\")\ninit_image\n\n\n\n\n\n\n\n\n\nNow let’s create some images starting from the sketch.\n\nprompt = \"Wolf howling at the moon, photorealistic 4K\"\n\n\n\nCode\ntorch.manual_seed(1000)\nimages = pipe(prompt=prompt, num_images_per_prompt=3, image=init_image, strength=0.8, num_inference_steps=50).images\nimage_grid(images, rows=1, cols=3)\n\n\n\n\n\n\n\n\n\n\n\n\nWhen we get a composition we like we can use it as the next seed for another prompt and further change the results. For example, let’s take the third image above and try to use it to generate something in the style of Van Gogh.\n\nprompt = \"Oil painting of wolf howling at the moon by Van Gogh\"\ninit_image = images[2]\n\n\n\nCode\ntorch.manual_seed(1000)\nimages = pipe(prompt=prompt, num_images_per_prompt=3, init_image=init_image, strength=1, num_inference_steps=70).images\nimage_grid(images, rows=1, cols=3)\n\n\n\n\n\n\n\n\n\n\n\n\nCreative people use different tools in a process of iterative refinement to come up with the ideas they have in mind. Here’s a list with some suggestions to get started.",
    "crumbs": [
      "Homework",
      "3 - Generative models"
    ]
  },
  {
    "objectID": "homeworks/generative.html#basic-stable-diffusion",
    "href": "homeworks/generative.html#basic-stable-diffusion",
    "title": "3 - Generative models",
    "section": "1.1 - Basic stable diffusion",
    "text": "1.1 - Basic stable diffusion\nWe start by experimenting with the basic model. Come up with a prompt and generate various images for it (have some fun!). Show three representative images and comment on the general behaviour of the model and trends that you may observe. For instance, comment on whether it can consistently account for all the aspects in your prompt or whether it struggles to include some of those. Do all images have the same composition or do you observe significant changes in the layouts?",
    "crumbs": [
      "Homework",
      "3 - Generative models"
    ]
  },
  {
    "objectID": "homeworks/generative.html#messing-with-the-inference-process",
    "href": "homeworks/generative.html#messing-with-the-inference-process",
    "title": "3 - Generative models",
    "section": "1.2 - Messing with the inference process",
    "text": "1.2 - Messing with the inference process\nExperiment with the inference process. Define a prompt (could be the same as before) and fix a random seed with torch.manual_seed, as we show in the introductory part to always obtain the same outcome for the same prompt. Generate images with a different number of inference steps: 3, 15, 50, and as much as you can go in a reasonable time (higher than 60). Do so by appropiately setting num_inference_steps. Show the resulting images and describe what you observe qualitatively. In what sense do the images improve as we increase the number of steps?\n\n\n\n\n\n\nTip\n\n\n\nIn the astronaut riding a horse above, we see that with very few steps the model cannot generate a coherent image. Increasing the number of steps, the image starts to shape up according to our prompt but it struggles with some basic physical properties, such as the horse legs (it has only three and one of them has a ghost hoof). If we increase the number of steps further, these errors disappear and we obtain the first image.",
    "crumbs": [
      "Homework",
      "3 - Generative models"
    ]
  },
  {
    "objectID": "homeworks/generative.html#classifier-free-guidance-1",
    "href": "homeworks/generative.html#classifier-free-guidance-1",
    "title": "3 - Generative models",
    "section": "2.1 - Classifier-free guidance",
    "text": "2.1 - Classifier-free guidance\nIn the introductory part above, we have seen how to tune our method’s adherance to our description. Define a prompt and a random seed. Generate images for four guidance values 1.1, 3.5, 7 and 14. Comment on the general behaviour.",
    "crumbs": [
      "Homework",
      "3 - Generative models"
    ]
  },
  {
    "objectID": "homeworks/generative.html#negative-prompts-1",
    "href": "homeworks/generative.html#negative-prompts-1",
    "title": "3 - Generative models",
    "section": "2.2 - Negative prompts",
    "text": "2.2 - Negative prompts\nRepeat the exact same process from 2.1 but, this time, include an appropiate negative prompt for our image generation (keep the original prompt and seed). How does the guidance impact the effect of the negative prompt?\n\n\n\n\n\n\nTip\n\n\n\nUse the results from the best images to define an appropiate negative prompt.",
    "crumbs": [
      "Homework",
      "3 - Generative models"
    ]
  },
  {
    "objectID": "homeworks/generative.html#a-starting-from-a-sketch",
    "href": "homeworks/generative.html#a-starting-from-a-sketch",
    "title": "3 - Generative models",
    "section": "(a) Starting from a sketch",
    "text": "(a) Starting from a sketch\nDraw a sketch (e.g. in paint), save it in png format and use it to generate a few cool images. Show the sketch and a few representative examples of the resulting images. Comment on the model’s behaviour, which parts of the sketch are preserved or discarded (composition? colors?) and what they have become.",
    "crumbs": [
      "Homework",
      "3 - Generative models"
    ]
  },
  {
    "objectID": "homeworks/generative.html#b-starting-from-an-image",
    "href": "homeworks/generative.html#b-starting-from-an-image",
    "title": "3 - Generative models",
    "section": "(b) Starting from an image",
    "text": "(b) Starting from an image\nGenerate an image with the diffusion model and use it as starting point to generate new images. Show the initial image and a few representative examples of the resulting ones. Comment on the model’s behaviour, which parts of the image have been preserved (composition? colors?) and what they have become.",
    "crumbs": [
      "Homework",
      "3 - Generative models"
    ]
  },
  {
    "objectID": "homeworks/ml_physics.html",
    "href": "homeworks/ml_physics.html",
    "title": "4 - ML techniques in physics",
    "section": "",
    "text": "Throughout the course, we have seen how to use automatic differentiation to train neural networks. We have briefly showcased a few examples of applications of machine learning to solve some problems physics.\nIn this homework, we tackle two problems in quantum physics: an inverse eigenproblem and a Hamiltonian learning task. Follow the instructions to complete the tasks and write a report explaining the results and the procedure.",
    "crumbs": [
      "Homework",
      "4 - ML techniques in physics"
    ]
  },
  {
    "objectID": "homeworks/ml_physics.html#solve-the-problem",
    "href": "homeworks/ml_physics.html#solve-the-problem",
    "title": "4 - ML techniques in physics",
    "section": "1.1 - Solve the problem",
    "text": "1.1 - Solve the problem\nConsider a wave-function of the form \\(\\psi_0(x) = \\frac{1}{{\\cal N}}f(R_0 - |x|)\\), where \\(f(x) = \\frac{\\sin(x/2)}{1+e^{-x}}\\), \\({\\cal N}\\) is a normalization constant, and \\(R_0\\) is a wave-function parameter.\nUsing automatic differentiation, find such \\(V(x)\\) for which \\(\\psi_0(x)\\) is a ground state of the Hamiltonian \\(\\hat{H}\\). To do the calculation, set \\(R_0=10\\) and use the code below as starting point.\n\ndef psi_profile(x):\n    return 1.0/(1 + pt.exp(-x))*pt.sin(x/2)\n\ndef psi_0(x):\n    return psi_profile(R0 - pt.abs(x))\n\ndef get_norm(psi, dx):\n    return pt.sum(pt.abs(psi)**2)*dx\n\n\nL = 60\nNx = 500\nR0 = 10\n\nx = pt.linspace(-L/2, L/2, Nx)\ndx = x[1] - x[0]\n\npsi_GS = psi_0(x)\nnorm = get_norm(psi_GS, dx)\n\npsi_GS = psi_GS/np.sqrt(norm)\nrho_GS = pt.abs(psi_GS)**2\nnorm = pt.sum(rho_GS)*dx\n\n\n\nCode\nFontSize = 20\nplt.title(r\"$R_0 = $\" + \"{:2.2f}\".format(R0) + \" | Norm: \" + \"{:2.2f}\".format(norm), fontsize = FontSize)\nplt.plot(x, rho_GS)\nplt.xlabel(\"x\", fontsize = FontSize)\nplt.ylabel(r\"$|\\psi_0(x)|^2$\", fontsize = FontSize)\nprint()",
    "crumbs": [
      "Homework",
      "4 - ML techniques in physics"
    ]
  },
  {
    "objectID": "homeworks/ml_physics.html#different-parameters",
    "href": "homeworks/ml_physics.html#different-parameters",
    "title": "4 - ML techniques in physics",
    "section": "1.2 - Different parameters",
    "text": "1.2 - Different parameters\nNow that you have solved the task, repeat the whole process for \\(R_0=1, 2, 5\\) and \\(20\\). Considering also the previously obtained results for \\(R_0=10\\), what conclusions can you draw?",
    "crumbs": [
      "Homework",
      "4 - ML techniques in physics"
    ]
  },
  {
    "objectID": "homeworks/ml_physics.html#consider-different-profiles",
    "href": "homeworks/ml_physics.html#consider-different-profiles",
    "title": "4 - ML techniques in physics",
    "section": "1.3 - Consider different profiles",
    "text": "1.3 - Consider different profiles\nFinally, we have enforced a specific wave function profile \\(f(x) = \\frac{\\sin(x/2)}{1+e^{-x}}\\). Repeat the calculation from 1.1 with a different profile: \\(f(x) = \\frac{\\sin(x)}{1+e^{-x}}\\).",
    "crumbs": [
      "Homework",
      "4 - ML techniques in physics"
    ]
  },
  {
    "objectID": "homeworks/ml_physics.html#spin-hamiltonian-in-one-dimension",
    "href": "homeworks/ml_physics.html#spin-hamiltonian-in-one-dimension",
    "title": "4 - ML techniques in physics",
    "section": "Spin Hamiltonian in one-dimension",
    "text": "Spin Hamiltonian in one-dimension\nAs an example let us consider one-dimensional spin chain of length \\(L\\) given by the general Heisenberg Hamiltonian:\n\\[\\begin{equation}\n\\hat{H} = \\sum_j J_x \\hat{S}^x_j\\hat{S}^x_{j+1} + J_y\\hat{S}^y_j\\hat{S}^y_{j+1} + J_z\\hat{S}^z_j\\hat{S}^z_{j+1} + h\\sum_j \\hat{S}^z_j,\n\\end{equation}\\] where each spin operator is \\(\\hat{S}^\\tau_j = \\frac{1}{2}\\hat{\\sigma}^\\tau_j\\), \\(\\hat{\\sigma}^\\tau\\) are Pauli operators with \\(\\tau = \\{x,y,z\\}\\) and \\[\\begin{equation}\n\\hat{\\sigma}^\\tau_j = \\mathbb{1}^{j-1}\\otimes\\hat{\\sigma}^\\tau\\otimes\\mathbb{1}^{L-j}\\,.\n\\end{equation}\\]",
    "crumbs": [
      "Homework",
      "4 - ML techniques in physics"
    ]
  },
  {
    "objectID": "homeworks/ml_physics.html#magnetization-dynamics",
    "href": "homeworks/ml_physics.html#magnetization-dynamics",
    "title": "4 - ML techniques in physics",
    "section": "Magnetization dynamics",
    "text": "Magnetization dynamics\nLet us consider an example from the work by Kushal Seetharam, et. al. Digital quantum simulation of NMR experiments, where they study Nuclear Magnetic Resonance (NMR).\nWe are interested in the change in time of the total spin magnetization: \\[\\begin{equation}\nS(t) = \\langle \\psi_0 |\\hat{U}^\\dagger(t)\\hat{S}^z_{\\rm total} \\hat{U}(t)|\\psi_0\\rangle\\,,\n\\end{equation}\\] where \\(\\hat{U}(t) = e^{-i\\hat{H} t}\\) is the time evolution operator.\nWe assumed that the initial state \\(|\\psi_0\\rangle\\) is a superposition of states with different total magnetization \\(m_i\\), i.e.  \\[\\begin{equation}\n|\\psi_0\\rangle = \\frac{1}{{\\cal N}}\\sum_{m_i&gt;0} m_i|m_i\\rangle,\n\\end{equation}\\] where \\({\\cal N}\\) is a normalization constant, \\(\\{m_i, |m_i\\rangle\\}\\) are eigenvalues and eigenstates of the total \\(\\hat{S}^z_{\\rm total}\\) operator. Thus, \\[\\begin{align}\n\\hat{S}^z_{\\rm total} & = \\sum_{j} \\hat{S}^z_j\\,, \\\\\n\\hat{S}^z_{\\rm total} |m_i\\rangle & = m_i |m_i\\rangle\\,.\\\\\n\\end{align}\\]\nSuch an assumption about the initial state can be motivated by the fact that spins are not in the ground state of the Hamiltonian. They are rather in a superposition of different magnetization states due to a relatively ‘high’ temperature of the spins.",
    "crumbs": [
      "Homework",
      "4 - ML techniques in physics"
    ]
  },
  {
    "objectID": "homeworks/ml_physics.html#the-task",
    "href": "homeworks/ml_physics.html#the-task",
    "title": "4 - ML techniques in physics",
    "section": "The task",
    "text": "The task\nThe magnetization through time \\(S(t)\\) depends on the value of the Hamiltonian parameters, i.e., \\(y = [J_x, J_y, J_z, h]\\). Your task is to train a neural network to extract these parameters based on the provided signal \\(S(t)\\).\nHowever, when we consider time-dependent data, it is more favorable to work with its Fourier Transform, \\(FFT[S(t)]\\), or the related power spectrum \\(PS = |FFT[S(t)]|^2\\). As such, the task is to infer \\([J_x, J_y, J_z, h]\\) from the \\(PS\\). For simplicity, we will assume \\(J_x = J_y = J_z = J\\).",
    "crumbs": [
      "Homework",
      "4 - ML techniques in physics"
    ]
  },
  {
    "objectID": "homeworks/ml_physics.html#data-generation",
    "href": "homeworks/ml_physics.html#data-generation",
    "title": "4 - ML techniques in physics",
    "section": "Data generation",
    "text": "Data generation\nThe first thing we need is to generate a suitable datast to train our machine learning model. We provide the code to generate the data below. However, generating proper dataset could be computationally demanding, so you don’t have to do it yourself, and you can downloaded prepared dataset from here.\nExplore the data and its format. The dataset is comprised of \\(20^4\\) tuples \\(\\{x, y\\} = \\{PS, [J_x, J_y, J_z, h]\\}\\) obtained for \\(L = 6\\) spins, and \\(J_x, J_y, J_z, h \\in [-2, 2]\\). The total time evolution was \\(t_{\\rm max} = 5\\), with \\(N_t = 200\\) time discretization points. As such, the PS is a vector with \\(N_t\\) entries.\n\n# Pauli operators\nid_local = pt.tensor([[1.,0],[0,1.]]) + 0*1j\nsigma_x = pt.tensor([[0,1.],[1.,0]]) + 0*1j   \nsigma_y = 1j*pt.tensor([[0,-1.],[1.,0]]) + 0*1j\nsigma_z = pt.tensor([[1.,0],[0,-1.]]) + 0*1j\n\n# spin-1/2 operators\ns_x = 0.5*sigma_x \ns_y = 0.5*sigma_y\ns_z = 0.5*sigma_z \n\n\ndef get_Identity(k):  # returns k-tensor product of the identity operator, ie. Id^k\n    Id = id_local\n    for i in range(0, k-1):\n        Id = pt.kron(Id, id_local)\n    return Id\n         \ndef get_hamiltonian_H(parameters):\n    D = 2**L\n    # Quantum Heisenberg model\n    H = pt.zeros((D,D)) \n    J = parameters[0]\n    h  = parameters[1]\n    for i in range(1,L):\n        H = H + J*(Sx[i]@Sx[i+1] + Sy[i]@Sy[i+1] + Sz[i]@Sz[i+1])\n    for i in range(1,L+1):\n        H = H + h*Sz[i]\n    return H\n\ndef get_string_operator(A, L, i):\n    Op = A\n    if(i == 1):\n        Op = pt.kron(A,get_Identity(L-1))\n        return Op\n    if(i == L):\n        Op = pt.kron(get_Identity(L-1),A)\n        return Op\n    if(i&gt;1 and i&lt;L):\n        Op = pt.kron(get_Identity(i-1), pt.kron(Op, get_Identity(L-i)))\n        return Op\n\nLet’s see how the \\(PS\\) behaves differently depending on the Hamiltonian parameters for \\(L = 6\\) spins.\n\n\nCode\nL_vec = [6]\n\nJ_vec = np.array([-2, -1, 0, 1, 2])\nh_vec = np.array([-2, -1, 0, 1, 2])\n \nN_total = J_vec.shape[0]*h_vec.shape[0]\n\nfor L in L_vec:   \n    D = 2**L\n    # Operators acting on j-th spin in spin chain\n    Sigmax = {}\n    Sigmay = {}\n    Sigmaz = {}\n\n    Sx = {}\n    Sy = {}\n    Sz = {}\n    \n    for j in range(1,L+1):\n        index = j\n        Sigmax[index] = get_string_operator(sigma_x, L, j)\n        Sigmay[index] = get_string_operator(sigma_y, L, j)\n        Sigmaz[index] = get_string_operator(sigma_z, L, j)\n        Sx[index] = get_string_operator(s_x, L, j)\n        Sy[index] = get_string_operator(s_y, L, j)\n        Sz[index] = get_string_operator(s_z, L, j)\n \n    Sx_total = sum(Sx[i] for i in range(1,L+1))\n    Sy_total = sum(Sy[i] for i in range(1,L+1))\n    Sz_total = sum(Sz[i] for i in range(1,L+1))    \n       \n    # Prepare initial state as a linear combination of all eigenstates of Sz_total operator\n    Sz_eval, Sz_evec = eigh(Sz_total)\n    psi_0 = pt.zeros(D) + 1j*0\n    for i in range(0,D):\n        if(Sz_eval[i] &gt; 0):\n            psi_0 = psi_0 + Sz_eval[i]*Sz_evec[:,i]\n    norm = pt.sum(pt.abs(psi_0)**2)\n    psi_0 = psi_0/pt.sqrt(norm)\n    \n    data = []\n    data_x_y = []\n    counter = 0\n    \n    # Data generation\n    for (J_i, J) in enumerate(J_vec):\n        for (h_i, h) in enumerate(h_vec):\n            counter = counter + 1                \n            parameters_H = np.array([J, h])\n            H = get_hamiltonian_H(parameters_H)\n            N_t = 200\n            t_max = 5\n            t_vec = pt.linspace(0,t_max, N_t)\n\n            # Calculate time-evolution of the expectation value of the Sz_total\n            Sz_total_val = pt.zeros(N_t)\n            for (t_i,t) in enumerate(t_vec):                         \n                U_t = expm(-1j*t*H)               # contruct time-evolution operator for Hamiltonian\n                U_t_dag = expm(1j*t*H)            # and its complex conjugate\n                Sz_total_t = U_t_dag@Sz_total@U_t # \n                Sz_total_val[t_i] = pt.vdot(psi_0, Sz_total_t@psi_0) # calculate &lt;S^z_total(t)&gt; \n\n            # Calculate and process Fourier Transform of time signal S(t)\n            FFT_Sz_total_val = np.fft.fft(Sz_total_val)                 # get Fast Fourier Transform\n            FFT_Sz_total_val = FFT_Sz_total_val[1:]                     # remove 0-frequency peak\n            FFT_Sz_total_val = np.fft.fftshift(FFT_Sz_total_val)        # center the spectrum\n            PS = np.abs(FFT_Sz_total_val)**2                            # calculate power spectrum  \n            normalized_PS = (PS - np.mean(PS))/np.std(PS)               # normalize ps\n\n            log_PS = np.log(PS)\n            normalized_logPS = (log_PS - np.mean(log_PS))/np.std(log_PS)\n\n            title_string = r\"$J = $\" + \"{:2.1f}\".format(J)\n            title_string = title_string + r\" | $h = $\" + \"{:2.1f}\".format(h)\n            fig, ax = plt.subplots(1,2, figsize = (12,4))\n            ax[0].plot(normalized_PS,'-o')\n            ax[1].plot(normalized_logPS,'-o')    \n            ax[0].set_ylabel(\"PS\")\n            ax[1].set_ylabel(\"log(PS)\")\n            ax[0].set_title(title_string)\n            plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/tmp/ipykernel_230546/3124271572.py:122: RuntimeWarning: divide by zero encountered in log\n  log_PS = np.log(PS)\n/tmp/ipykernel_230546/3124271572.py:123: RuntimeWarning: invalid value encountered in subtract\n  normalized_logPS = (log_PS - np.mean(log_PS))/np.std(log_PS)",
    "crumbs": [
      "Homework",
      "4 - ML techniques in physics"
    ]
  },
  {
    "objectID": "homeworks/ml_physics.html#data-preprocessing",
    "href": "homeworks/ml_physics.html#data-preprocessing",
    "title": "4 - ML techniques in physics",
    "section": "2.1 - Data preprocessing",
    "text": "2.1 - Data preprocessing\nAs an input for the neural network, we can consider either the power spectrum \\(PS = |FFT[S(t)]|^2\\), or its logarithm \\(\\log(PS)\\). Furthermore, we usualy also normalize the input data to zero mean, and unit standard deviation. We achieve this with the normalization: \\[\\begin{equation}\n  x \\leftarrow \\frac{x - {\\rm mean}(x)}{{\\rm std}(x)}.\n\\end{equation}\\]\nPre-process the data and split it into a training and a test set. For now, consider the normalized logarithm of the power spectrum.",
    "crumbs": [
      "Homework",
      "4 - ML techniques in physics"
    ]
  },
  {
    "objectID": "homeworks/ml_physics.html#train-the-model",
    "href": "homeworks/ml_physics.html#train-the-model",
    "title": "4 - ML techniques in physics",
    "section": "2.2 - Train the model",
    "text": "2.2 - Train the model\nTrain a neural network with \\(N_t\\) input and 2 output neurons, which correspond to \\([J, h]\\). Consider 0, 1 and 2 hidden layers with 500 neurons. Use the MSE loss to train and report it for the training and test sets. What conclusions can you extract? Is the network overfitting the data?\n\n\n\n\n\n\nNote\n\n\n\nDon’t worry about the poor performance of the neural network. Hamiltonian learning is a hard task - this is just to get a flavour of an application of machine learning to quantum many-body physics.",
    "crumbs": [
      "Homework",
      "4 - ML techniques in physics"
    ]
  },
  {
    "objectID": "homeworks/ml_physics.html#regularization",
    "href": "homeworks/ml_physics.html#regularization",
    "title": "4 - ML techniques in physics",
    "section": "2.3 - Regularization",
    "text": "2.3 - Regularization\nTake the best performing model and experiment with dropout. First, add dropout with 0.2 and 0.5 before the last layer. Then, add dropout only to the input layer with the same values. What differences do you observe? Does the model improve? Where is it more effective?\nFinally, add dropout to both layers and compare with the obtained results.",
    "crumbs": [
      "Homework",
      "4 - ML techniques in physics"
    ]
  },
  {
    "objectID": "homeworks/ml_physics.html#input-data",
    "href": "homeworks/ml_physics.html#input-data",
    "title": "4 - ML techniques in physics",
    "section": "2.4 - Input data",
    "text": "2.4 - Input data\nNow let’s take the best performing architecture and train it with the normalized power spectrum (we have used the logarithm so far). What differences can we observe with respect to training with the logarithm?\n\n\n\n\n\n\nTip\n\n\n\nUse the code below as a guideline to complete the tasks.\n\n\n\n# import torch as pt\n# from torch.utils.data import Dataset, DataLoader, random_split\n# from torch.utils.data.sampler import SubsetRandomSampler\n# import torch.optim as optim\n# import torch.nn as nn\n\n# import numpy as np\n# import pandas as pd\n# import matplotlib.pyplot as plt\n\nReading data from file, preparing train/test datasets and preparing train/test PyTorch DataLoader:\n\n# from torch.utils.data import Dataset, DataLoader, random_split\n# from torch.utils.data.sampler import SubsetRandomSampler\n# import torch.optim as optim\n# import torch.nn as nn\n\n# import numpy as np\n# import pandas as pd\n# import matplotlib.pyplot as plt\n\n# class CustomDataset(Dataset):\n#     def __init__(self, x, y):\n#         self.x = x\n#         self.y = y\n    \n#     def __len__(self):\n#         return len(self.y)\n    \n#     def __getitem__(self, index):\n#         y = self.y[index]\n#         x = self.x[index]\n#         return x, y\n    \n# # Read data from pandas dataframe, prepare train/test data sets and train/test data loaders\n# np.random.seed(0)\n# pt.manual_seed(0)\n# df = pd.read_pickle(\"Hamiltonian_learning_data.pkl\")\n\n# # split datafram into train and test datasets\n# N_train = int(0.7*len(df))\n# N_test  = int(0.3*len(df))\n \n# idx_train = np.random.randint(0, len(df), N_train)\n# idx_test  = np.random.randint(0, len(df), N_test)\n# train_dataset = df.iloc[idx_train].reset_index()\n# test_dataset  = df.iloc[idx_test].reset_index()\n\n\n# train_data = CustomDataset(train_dataset['normalized_log_PS'], train_dataset['hamiltonian_parameters'])\n# test_data  = CustomDataset(test_dataset['normalized_log_PS'], test_dataset['hamiltonian_parameters'])\n\n# train_data = CustomDataset(train_dataset['normalized_PS'], train_dataset['hamiltonian_parameters'])\n# test_data  = CustomDataset(test_dataset['normalized_PS'], test_dataset['hamiltonian_parameters'])\n\n\n\n# # build DataLoaders\n# BatchSize = 1024\n# train_loader = DataLoader(train_data, batch_size = BatchSize, shuffle=True)\n# test_loader  = DataLoader(test_data , batch_size = BatchSize, shuffle=True)\n\nDefine neural network model, loss function and optimizer:\n\n# # Build neural network\n# N_t = 199\n# h_1 = 500\n# h_2 = 250\n# h_3 = 125\n# output_layer = 2\n\n# model = nn.Sequential(\n#           nn.Linear(N_t,h_1),\n#           nn.ReLU(),\n#           nn.Linear(h_1, h_2),\n#           nn.BatchNorm1d(h_2),\n#           nn.ReLU(),\n#           nn.Linear(h_2, h_3),\n#           nn.BatchNorm1d(h_3),\n#           nn.ReLU(),         \n#           nn.Dropout(0.5), # Careful with dropout, follow the instructions!\n#           nn.Linear(h_3, output_layer)\n#         )\n\n# model.double()\n\n# # Define the loss function and optimizer\n# criterion = nn.MSELoss()\n# optimizer = optim.SGD(model.parameters(), lr=.1)\n\nTraining pipeline:\n\n# N_epochs = 1000\n\n# # Training loop\n# loss_vs_epoch = []\n# for epoch in range(0, N_epochs):\n#     running_loss = 0.0\n#     for x, y in train_loader:\n \n#         # Clear the gradients\n#         optimizer.zero_grad()\n                \n            \n#         # Forward pass\n#         y_pred = model(x)\n        \n#         # Calculate the loss\n#         loss = criterion(y_pred, y)        \n\n#         # Backward pass\n#         loss.backward()  # automatic calculating the loss with respects to trainable parameters\n        \n#         # Update the weights according to chosen optimization function. Here: Adam\n#         optimizer.step()\n        \n#         # Print statistics\n#         running_loss += loss.item()\n#     loss_vs_epoch.append(running_loss)\n#     print(f\"Epoch {epoch} - Training loss: {running_loss/len(train_loader)}\")",
    "crumbs": [
      "Homework",
      "4 - ML techniques in physics"
    ]
  },
  {
    "objectID": "homeworks/perceptron.html",
    "href": "homeworks/perceptron.html",
    "title": "2 - Perceptron",
    "section": "",
    "text": "In this homework, we will implement the perceptron algorithm from scratch and experiment with it in different conditions. Submit a written report in .pdf format (no code) to the corresponding deliverable entry in the aula virtual. The report should contain a description of the whole procedure with some representative figures.",
    "crumbs": [
      "Homework",
      "2 - Perceptron"
    ]
  },
  {
    "objectID": "homeworks/perceptron.html#sparse-data-points",
    "href": "homeworks/perceptron.html#sparse-data-points",
    "title": "2 - Perceptron",
    "section": "2.1 - Sparse data points",
    "text": "2.1 - Sparse data points\nGenerate a dataset with 20 data points and run the perceptron algorithm. Plot the examples \\(\\left\\{\\left(\\mathbf{x}_n,y_n\\right)\\right\\}\\), and the target function \\(f\\) and the final hypothesis \\(g\\) on a plane. Report the number of updates that the algorithm took to converge.\nRepeat the procedure with more randomly generated datasets of the same size. Can you observe any differences? Report the results as described for the first dataset for two significant additional datasets.",
    "crumbs": [
      "Homework",
      "2 - Perceptron"
    ]
  },
  {
    "objectID": "homeworks/perceptron.html#some-more-data",
    "href": "homeworks/perceptron.html#some-more-data",
    "title": "2 - Perceptron",
    "section": "2.2 - Some more data",
    "text": "2.2 - Some more data\nGenerate two more random datasets with 100 and 1000 examples and run the perceptron algorithm. Plot the examples \\(\\left\\{\\left(\\mathbf{x}_n,y_n\\right)\\right\\}\\), and the target function \\(f\\) and the final hypothesis \\(g\\) on a plane. Report the number of updates that the algorithm took to converge.",
    "crumbs": [
      "Homework",
      "2 - Perceptron"
    ]
  },
  {
    "objectID": "homeworks/paper_report.html",
    "href": "homeworks/paper_report.html",
    "title": "Paper report",
    "section": "",
    "text": "The goal of this task is to get used to reading and understanding scientific papers. You will find that the best papers tend to have a clear message, making it easy to understand the motivation and the novelty of the presented work. Thus, the task consists of reading a paper about a relevant topic for the course and writing a report (max 2 pages) explaining it (see the guidelines below).\n\nList of papers\nHere, we provide a list of high quality papers that have had a significant impact on the field of machine learning and physics. Choose one of them for your report. If you find a paper that you would like to work on outside of this list, ask us! :D\n\n\n\n\n\n\nTip\n\n\n\nDo not hesitate to look for other high-level resources about the papers, such as blog posts or youtube videos. This is perfectly fine and it may help you understand the technical details. However, be aware that the internet is full of terrible content, so choose your sources carefully!\n\n\nPapers about machine learning in physics:\n\nG. Carleo and M. Troyer. Solving the quantum many-body problem with artificial neural networks (2017).\n\nThe first paper to introduce neural network quantum states.\n\nJ. Carrasquilla and R. G. Melko. Machine learning phases of matter (2017).\n\nOne of the first works using machine learning to study phase transitions.\n\nG. Torlai, et. al. Neural network quantum state tomography (2018).\n\nPioneering work using generative models for quantum tomography.\n\nR. Iten, et. al. Discovering physical concepts with neural networks (2020).\n\nOne of the first works to extract physical insight from raw data.\n\n\nSignificant advances in the machine learning field:\n\nJ. Howard and S. Ruder. Universal language model fine-tuning for text classification (2018).\n\nPaper introducing generative pre-training.\n\nA. Vaswani, et. al. Attention is all you need (2017).\n\nThe current state of the art model: the transformer.\n\nT. Brown, et. al. Language Models are Few-Shot Learners (2020).\n\nThe famous GPT-3 showed how generic language models could accomplish various tasks at once.\n\nI. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks (2014).\n\n(One of) the first paper(s) to introduce seq2seq models to deal with variable sequence lengths.\n\nA. Radford, et. al. Learning Transferable Visual Models From Natural Language Supervision (2021).\n\nCLIP is an essential part of the current diffusion models like DALLE-2\n\n\nMachine learning perspective:\n\nR. Schwartz, et al. Green AI (2020).\n\nOverview of the cost of machine learning both economically and environmentally.\n\n\nQuantum algorithms:\n\nV. Havlíček, et. al. Supervised learning with quantum-enhanced feature spaces (2019).\n\nThe first experimental realization of a quantum classifier.\n\nJ. R. McClean, et. al. The theory of variational hybrid quantum-classical algorithms (2016).\n\nPaper laying down the fundamentals of hybrid quantum-classical algorithms that lie at the core of quantum machine learning.\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere, we provide the links to the official publications. However, in some cases, you may not be able to access the pdf. Nonetheless, these papers are all accessible on arXiv.\n\n\n\n\nGuidelines\nThe report should be, at maximum, 2 pages long explaining the paper. As a guideline, it should contain the answer to the following questions:\n\nWhat is the motivation of the authors?\nWhat are the goals?\nWhat had been done before? What was the state of the art?\nWhere is the novelty?\nAre the results important? In which sense?\nAre the results of general interest or just for the specific area?\nIs the paper well written, clear, and understandable?",
    "crumbs": [
      "Homework",
      "Paper report"
    ]
  },
  {
    "objectID": "homeworks/regression.html",
    "href": "homeworks/regression.html",
    "title": "1 - Regression",
    "section": "",
    "text": "In this homework we will have two main tasks: a regression and a classification. However, your first goal will be to actually design each of these by choosing an appropiate problem and dataset.\nYou have complete freedom to choose any dataset that you may wish to work with, from some personal project to any random dataset in the internet, as long as it allows you to perform the tasks below (read them first!). We encourage you to choose a problem that you may find interesting or that you have any curiosity about the topic. We recommend kaggle, although you can also download data from scikit-learn or, simply, do a google search.\nSubmit a written report in .pdf format (no code) to the corresponding deliverable entry in the aula virtual. The report should contain a description of the whole procedure with some representative figures.",
    "crumbs": [
      "Homework",
      "1 - Regression"
    ]
  },
  {
    "objectID": "homeworks/regression.html#become-familiar-with-the-data",
    "href": "homeworks/regression.html#become-familiar-with-the-data",
    "title": "1 - Regression",
    "section": "1.1 - Become familiar with the data",
    "text": "1.1 - Become familiar with the data\nDescribe the dataset that you have chosen, describe and look at the different features in the samples and try to identify a couple of potential linear (or polynomial) relationships between any pairs of variables.\n\n\n\n\n\n\nTip\n\n\n\nFor example, if we look at the weather, we may find a nearly linear relationship between the apparent temperature and the ambient temperature, or the apparent temperature and the relative humidity.",
    "crumbs": [
      "Homework",
      "1 - Regression"
    ]
  },
  {
    "objectID": "homeworks/regression.html#perform-the-regression",
    "href": "homeworks/regression.html#perform-the-regression",
    "title": "1 - Regression",
    "section": "1.2 - Perform the regression",
    "text": "1.2 - Perform the regression\nTake any pair of related features and perform a regression. Plot the data and the resulting fit and report an appropiate metric to evaluate the performance of the resulting method. Is the fit good?\nDoes the data follow the relationship that you were expecting? Would a polynomial regression of a different degree result in a better fit? Try it and explain why it does or why it doesn’t.",
    "crumbs": [
      "Homework",
      "1 - Regression"
    ]
  },
  {
    "objectID": "homeworks/regression.html#deepening-the-analysis-based-on-the-results",
    "href": "homeworks/regression.html#deepening-the-analysis-based-on-the-results",
    "title": "1 - Regression",
    "section": "1.3 - Deepening the analysis based on the results",
    "text": "1.3 - Deepening the analysis based on the results\nAfter the first preliminary analysis, we can proceed based on the previous results. We propose two possible directions but one is enough. Also, if you find anything interesting that you would like to explore instead of the following options, feel free to do it and report it.\n\n1.3 (a) - Second relationship\nIf you could identify more linear or polynomial relationships between your features, repeat 1.2 for these variables. Which one of them results in a better model? Can you explain why?\n\n\n\n\n\n\nTip\n\n\n\nIn the weather example, we could predict the apparent temperature from the ambient temperature or from the relative humidity, which one would result in a better regressor?\n\n\n\n\n1.3 (b) - Improving our fit with more features\nIn 1.2, we only looked at the dependency between two variables. However, we can consider further features in our samples to obtain better predictions. Perform the same regression task as before considering additional features, e.g., one and two more features, and report the results. Explain these features and discuss why they improve the performance or not.\n\n\n\n\n\n\nTip\n\n\n\nIn the weather example, would we obtain a better regressor for the apparent temperature combining the ambient temperature and the relative humidity?",
    "crumbs": [
      "Homework",
      "1 - Regression"
    ]
  },
  {
    "objectID": "homeworks/regression.html#become-familiar-with-the-data-1",
    "href": "homeworks/regression.html#become-familiar-with-the-data-1",
    "title": "1 - Regression",
    "section": "2.1 - Become familiar with the data",
    "text": "2.1 - Become familiar with the data\nDescribe the dataset that you have chosen, describe and look at the different features in the samples and try to identify a possible binary classification task from a few relevant features.",
    "crumbs": [
      "Homework",
      "1 - Regression"
    ]
  },
  {
    "objectID": "homeworks/regression.html#train-the-classifier",
    "href": "homeworks/regression.html#train-the-classifier",
    "title": "1 - Regression",
    "section": "2.2 - Train the classifier",
    "text": "2.2 - Train the classifier\nTrain a logistic regression on one of the relevant features. Plot the results and report an appropiate metric to evaluate the method’s performance. Is the classifier good? Is this one feature enough to perform the task?\n\n\n\n\n\n\nTip\n\n\n\nIn the flowers example, we can distinguish between some species, such as daisies and poppies, just by looking at the flower radius. We can visualize this by drawing the one dimensional radius line and placing markers where the examples lie colored by class.",
    "crumbs": [
      "Homework",
      "1 - Regression"
    ]
  },
  {
    "objectID": "homeworks/regression.html#considering-further-features",
    "href": "homeworks/regression.html#considering-further-features",
    "title": "1 - Regression",
    "section": "2.3 - Considering further features",
    "text": "2.3 - Considering further features\nSo far, we have only used a single feature to perform the classification. Consider now a second one and retrain the logistic regression classifier with both of them. Do the results improve? Why? Visualize the results.",
    "crumbs": [
      "Homework",
      "1 - Regression"
    ]
  },
  {
    "objectID": "homeworks/regression.html#understanding-the-model",
    "href": "homeworks/regression.html#understanding-the-model",
    "title": "1 - Regression",
    "section": "2.4 - Understanding the model",
    "text": "2.4 - Understanding the model\nFinally, train the regressor with as many relevant features as you consider. Does the performance improve? Visualize the weights of the resulting model and discuss which features have the highest and lowest impact in the prediction.",
    "crumbs": [
      "Homework",
      "1 - Regression"
    ]
  },
  {
    "objectID": "homeworks/index_homework.html",
    "href": "homeworks/index_homework.html",
    "title": "Homeworks and evaluation",
    "section": "",
    "text": "Throughout the course, we will provide 4 mandatory homeworks and a written report about a scientific publication of your choice. Each of these five tasks count for 20% of the final mark. Additionally, we will provide an optional homework that will add up to an additional 10%.",
    "crumbs": [
      "Homework"
    ]
  },
  {
    "objectID": "course/deep_learning/regularization_techniques.html#weight-decay-l1l2-regularization",
    "href": "course/deep_learning/regularization_techniques.html#weight-decay-l1l2-regularization",
    "title": "Neural Networks regularization",
    "section": "3.1 Weight decay (L1/L2 regularization)",
    "text": "3.1 Weight decay (L1/L2 regularization)\nThis method involves adding a penalty to the cost function during training to discourage the model from learning excessively large weights. These regularization techniques are based on the idea that large weights can lead to overfitting, as they may allow the model to fit the training data too closely. L1 and L2 regularization are methods for adding a penalty term to the cost function during training to discourage the model from learning excessively large weights. L1 regularization:\n\n3.1.1 L1 regularization\nL1 regularization, also known as ${} regularization, adds a penalty term to the cost function that is proportional to the absolute value of the weights. The L1 regularization term has the form:\n\\[\\begin{equation}\nL_1 = \\lambda  \\sum |W|\n\\end{equation}\\] where λ is the regularization parameter, and w is the weight.\nThe effect of L1 regularization is to push the weights towards zero, which can help to reduce overfitting by decreasing the complexity of the model. L1 regularization can also be useful for feature selection, as it tends to drive the weights of unimportant features to zero, effectively removing them from the model.\n\n\n3.1.2 L2 regularization\nL2 regularization, also known as \\({\\it Ridge}\\) regularization, adds a penalty term to the cost function that is proportional to the square of the weights. The L2 regularization term has the form:\n\\[\\begin{equation}\nL_2 = \\lambda  \\sum W^2\n\\end{equation}\\]\nwhere \\(\\lambda\\) is the regularization parameter, and \\(W\\) are weights of the model.\nThe effect of L2 regularization is to shrink the weights towards zero, which can help to reduce overfitting by decreasing the complexity of the model.\nHowever, unlike L1 regularization, L2 regularization does not lead to the complete removal of weights, as it only shrinks the weights rather than setting them to zero.\nIn general, L2 regularization is more commonly used than L1 regularization, as it tends to be more stable and easier to optimize. However, L1 regularization can be useful in situations where it is important to select a subset of features, as it has the ability to drive some weights to zero.",
    "crumbs": [
      "Course",
      "Fundamentals of deep learning",
      "Neural Networks regularization"
    ]
  },
  {
    "objectID": "course/deep_learning/regularization_techniques.html#dropout",
    "href": "course/deep_learning/regularization_techniques.html#dropout",
    "title": "Neural Networks regularization",
    "section": "3.2 Dropout",
    "text": "3.2 Dropout\nThis is a regularization technique that randomly sets a fraction of the activations to zero during training. This helps to prevent overfitting by forcing the model to be more robust to the specific weights of individual units.",
    "crumbs": [
      "Course",
      "Fundamentals of deep learning",
      "Neural Networks regularization"
    ]
  },
  {
    "objectID": "course/deep_learning/regularization_techniques.html#batch-normalization",
    "href": "course/deep_learning/regularization_techniques.html#batch-normalization",
    "title": "Neural Networks regularization",
    "section": "3.3 Batch normalization:",
    "text": "3.3 Batch normalization:\nBatch normalization is a technique that is used to normalize the activations of a mini-batch in order to stabilize and accelerate the training of deep neural networks.\nIdeally, the normalization would be conducted over the entire training set, but to use this step jointly with stochastic optimization methods, it is impractical to use the global information. Thus, normalization is restrained to each mini-batch in the training process. This can help to prevent overfitting by ensuring that the activations of a layer have a consistent distribution, which makes it easier for the model to learn.\nDuring training, the batch normalization layer computes the mean and standard deviation of the activations of the current mini-batch and uses these statistics to normalize the activations. The normalized activations are then computed as follows:\nnormalized_activations = (activations - mean) / std\nwhere mean and std are the mean and standard deviation of the activations, respectively.\nThe batch normalization layer also stores the mean and standard deviation of the activations in a set of running statistics, which are updated at each training iteration by exponentially moving the mean and standard deviation of the mini-batch towards the mean and standard deviation of the running statistics.\nDuring evaluation of the model, the batch normalization layer uses the mean and standard deviation of the running statistics to normalize the activations. This helps to ensure that the model’s behavior is consistent during training and evaluation, and can improve the model’s generalization ability.\nIn PyTorch, batch normalization can be implemented by using the BatchNorm1d layer for fully-connected layers, or the BatchNorm2d layer for convolutional layers. These layers should be placed after the linear or convolutional layers, respectively, and before the non-linear activation function. For example:",
    "crumbs": [
      "Course",
      "Fundamentals of deep learning",
      "Neural Networks regularization"
    ]
  },
  {
    "objectID": "course/deep_learning/regularization_techniques.html#early-stopping",
    "href": "course/deep_learning/regularization_techniques.html#early-stopping",
    "title": "Neural Networks regularization",
    "section": "3.4 Early stopping",
    "text": "3.4 Early stopping\nThis method involves stopping the training process before the model has fully converged. This can be used to prevent overfitting by limiting the number of iterations that the model can use to learn the training data.\nData augmentation: This method involves generating additional training examples by applying random transformations to the existing training examples. This can help to prevent overfitting by providing the model with more diverse data to learn from.\n\n# import torch\n\n# Train the model for a maximum of 100 epochs\n#for epoch in range(100):\n  # Train the model for one epoch\n#  train(model, train_data, optimizer)\n\n  # Evaluate the model on the validation set\n#  val_loss = evaluate(model, val_data)\n\n  # If the validation loss has not improved in the last 10 epochs, stop training\n#  if val_loss &gt; best_val_loss:\n#    best_val_loss = val_loss\n#    patience = 0\n#  else:\n#    patience += 1\n#    if patience == 10:\n#      break",
    "crumbs": [
      "Course",
      "Fundamentals of deep learning",
      "Neural Networks regularization"
    ]
  },
  {
    "objectID": "course/deep_learning/regularization_techniques.html#data-augumentation",
    "href": "course/deep_learning/regularization_techniques.html#data-augumentation",
    "title": "Neural Networks regularization",
    "section": "3.5 Data augumentation",
    "text": "3.5 Data augumentation\nData augmentation: To implement data augmentation in PyTorch, you can use the torchvision.transforms module, which provides a number of pre-defined image transformations that can be applied to the training data. For example:\n\n# import torchvision.transforms as transforms\n\n# Define a transformation that randomly crops and flips the input images\n#transform = transforms.Compose([\n#    transforms.RandomCrop(32, padding=4),\n#    transforms.RandomHorizontalFlip()\n#])\n\n# Apply the transformation to the training data\n#train_data = torch.utils.data.DataLoader(\n#    dataset, batch_size=batch_size, shuffle=True, transform=transform)",
    "crumbs": [
      "Course",
      "Fundamentals of deep learning",
      "Neural Networks regularization"
    ]
  },
  {
    "objectID": "course/deep_learning/multiclass_classification_problem.html",
    "href": "course/deep_learning/multiclass_classification_problem.html",
    "title": "Multiclass classification",
    "section": "",
    "text": "1 One-hot encoding\nLet’s consider problem of data classification when each trainig sample \\(\\vec{x}\\) has a label \\(y\\) belonging to one class, where we have \\(J\\) classes in total. Next, we can enumerate each class by index \\(j \\in \\{1,\\dots, J\\}\\).\nMulticlass classification problems can be considered as task for which each input sample \\(\\vec{x}\\) is equiped with the discrete probability distribution\n\\[\\begin{equation}\n\\begin{split}\np = [p_1, p_2, \\dots, p_J],\\\\\n\\sum_{i=j}^{J} p_j = 1,\n\\end{split}\n\\end{equation}\\] providing information what is the probability that given imput data \\(\\vec{x}\\) belongs to given class \\(j\\).\nIn a particular scenario of a labeled data sample \\(\\vec{x}\\) with label \\(y\\) belonging to class with number \\(j = 3\\) the corresponding probability distribution is\n\\[\\begin{equation}\np = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0].\n\\end{equation}\\]\nAs such, training sample \\(\\vec{x}\\) is equiped with a new label \\(p\\).\nSuch a maping between labels \\(y\\) to discrete probability distribution \\(p\\) in the literature is so-called \\({\\it one-hot}\\) encoding.\nOne-hot encoding is often used as a way to represent categorical variables in machine learning models. It has the advantage of being able to represent any number of categories, and the labels are mutually exclusive, which can be useful for certain types of models.\n\n\n2 Categorical cross-entropy as a loss function\nFor multiclass classification task architecture of the Neural Network has output layer with \\(J\\) nodes, corresponding to number of all classes in the training dataset. We interpret output of a neural network \\(\\phi = [\\phi_1, \\phi_2, \\dots, \\phi_J]\\) as a predicted discrete probability distribution \\(q = [q_1, q_2, \\dots, q_J]\\), after applying softmax activation function\n\\[\\begin{equation}\n\\phi_j \\to q_j = softmax(\\phi_j) = \\frac{e^{\\phi_j}}{\\sum_l e^{\\phi_j}},\n\\end{equation}\\]\nwhich assures that \\(\\sum_j q_j = 1\\). As such, output \\(q = \\{q_0, q_2, \\dots, q_J\\}\\) can be interpreted as a discrete probability distribution, as well as \\(p\\).\nNow, the class prediction is taken as index \\(l\\) corresponding to the maximum value of the class probability, i.e. the model prediction is a index \\(l\\) for which \\(q_l\\) has maximal value - i.e. it provides digit which is most likely a proper label for the input data.\nDuring training the Neural Network we want to minimize distance between input class probability distribution \\(p\\) and predicted class probability distribution \\(q\\).\nTo compare two probability distributaion, i.e. to have a measure how \\(p\\) and \\(p\\) differ, we will use earlier introduced Kullback-Leibler divergence:\n\\[\\begin{equation}\nD_{KL}(p || q) = \\sum_{l} p_l\\log\\frac{p_l}{q_l}.\n\\end{equation}\\]\nWe can see that\n\\[\\begin{equation}\nD_{KL}(p || q) = \\sum_{l}p_l\\log{p_l} -\\sum_l p_l\\log{q_l} \\equiv -{\\cal S}(p) +   L_\\text{CE}(p,q),\n\\end{equation}\\]\nwhere \\({\\cal S}(p)\\) is Shannon entropy for discrete probability distribution \\(p\\), and\n\\[\\begin{equation}\nL_\\text{CE}(p,q) = \\sum_l p_l\\log{q_l}\n\\end{equation}\\] is, already introduced in previous lecture, categorical-cross entropy.\nBecause the Shannon entropy does not depends on the trainable parameters we can consider only categorical cross-entropy as a loss function.\n\n\n3 Example: Multiclass classification in MNIST dataset\nIn the context of the MNIST dataset our labels are digits assigned to \\(28\\times28\\) pixels images. In such a case, we can consider handwritten digits recognition as a classification problem with \\(J = 10\\) different classes - each for each digit. We assume that each class has assigned arbitrary index \\(l\\) enumerating classes.\nIn multiclass classification problem, one of the most popular techniqe for for data labels is so called \\({\\it one-hot encoding}\\). One-hot encoding is a way of representing each label as a \\(J\\)-dimensional vectors. Each vector has all elements set to \\(0\\) except one element, whose position corresponds to arbitrary class index \\(l\\).\nFor example, the digit \\(3\\) has class index \\(4\\) (we count from \\(0\\)), thus its label would be represented as\n\\[\\begin{equation}\np = y_{\\text{one-hot-encoded}} = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0] \\\\\n\\end{equation}\\]\nOne-hot encoding is often used as a way to represent categorical variables in machine learning models. It has the advantage of being able to represent any number of categories, and the labels are mutually exclusive, which can be useful for certain types of models.\nLet’ import training data: images and labels\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\n# Define a transform to normalize the data\ntransform = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize((0.5,), (0.5,))])\n# Download and load the training data\ntrainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n\nNx = 28   # number of pixels in x-direction\nNy = 28   # number of pixels in y-direction\nN_class = 10\n\nNow, we define simple feed-forward neural network with one hidden layers with \\(N_{h_1} = 21\\) nodes (as in our previous example) with ReLU activation function):\n\n# Define the model\nN_h_1 = 21\nmodel = nn.Sequential(nn.Linear(Nx*Ny, N_h_1),\n                      nn.ReLU(),\n                      nn.Linear(N_h_1, N_class)\n                     )\n\nNow, we define loss function \\(L\\) as cross-entropy, and Adam as a optimizer for calculating gradient of the loss function \\(L\\) with respect to trainable parameters.\n\n# Define the loss\ncriterion = nn.CrossEntropyLoss()\n# Define the optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nFinally, we will train our model for \\(N_{\\text{epoch}} = 10\\) epochs, and collect value of the loss function at each epoch.\n\nN_epoch = 30\n\n# Training loop\nfor epoch in range(0, N_epoch):\n    running_loss = 0.0\n    for images, labels in trainloader:\n        # Flatten MNIST images into a 784-dimensional vector\n        images = images.view(images.shape[0], -1)\n    \n        # Clear the gradients\n        optimizer.zero_grad()\n                \n        # One-hot encode the labels\n        one_hot_labels = torch.zeros(labels.size(0), 10)\n        one_hot_labels[torch.arange(labels.size(0)), labels] = 1\n                \n        # Forward pass\n        output = model(images)\n        \n        # Calculate the loss\n        loss = criterion(output, one_hot_labels)        \n\n        # Backward pass\n        loss.backward()  # automatic calculating the loss with respects to trainable parameters\n        \n        # Update the weights according to chosen optimization function. Here: Adam\n        optimizer.step()\n        \n        # Print statistics\n        running_loss += loss.item()\n    print(f\"Epoch {epoch} - Training loss: {running_loss/len(trainloader)}\")\n\nEpoch 0 - Training loss: 0.18809032182036434\nEpoch 1 - Training loss: 0.18602984545748436\nEpoch 2 - Training loss: 0.18213596730344078\nEpoch 3 - Training loss: 0.17800354234763047\nEpoch 4 - Training loss: 0.1751984041442336\nEpoch 5 - Training loss: 0.1730396168262783\nEpoch 6 - Training loss: 0.17177876067965397\nEpoch 7 - Training loss: 0.16887598526852726\nEpoch 8 - Training loss: 0.16749640748198671\nEpoch 9 - Training loss: 0.16485213802686569\nEpoch 10 - Training loss: 0.16440106320109513\nEpoch 11 - Training loss: 0.16213823579498002\nEpoch 12 - Training loss: 0.15957567634593164\nEpoch 13 - Training loss: 0.15976456029296937\nEpoch 14 - Training loss: 0.15756025875229507\nEpoch 15 - Training loss: 0.1583266692863567\nEpoch 16 - Training loss: 0.15594374585046825\nEpoch 17 - Training loss: 0.1546545083712\nEpoch 18 - Training loss: 0.15279026868056134\nEpoch 19 - Training loss: 0.1515156587383259\nEpoch 20 - Training loss: 0.15149353574842278\nEpoch 21 - Training loss: 0.14962262904513746\nEpoch 22 - Training loss: 0.15081732090153516\nEpoch 23 - Training loss: 0.14860467597254431\nEpoch 24 - Training loss: 0.14812911951592736\nEpoch 25 - Training loss: 0.14703498993402542\nEpoch 26 - Training loss: 0.14545652714234267\nEpoch 27 - Training loss: 0.14655765508815868\nEpoch 28 - Training loss: 0.14348608946828828\nEpoch 29 - Training loss: 0.14380284551598593\n\n\nNow, we can evaluate model on a test dataset and check the confusion matrix:\n\nimport numpy as np\n# Define a transform to normalize the data\ntransform = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize((0.5,), (0.5,))])\n# Download and load the test data\ntestset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n# Set the model to evaluation mode\nmodel.eval()\n\n# Make predictions on the test set\ncorrect = 0\ntotal = 0\nconfusion_matrix = np.zeros((10,10))\nwith torch.no_grad():\n    for images, labels in testloader:\n        # Flatten MNIST images into a 784-dimensional vector\n        images = images.view(images.shape[0], -1)\n        \n        # Forward pass\n        output = model(images)\n        \n        # Apply the softmax function to the output\n        probs = torch.softmax(output, dim=1)\n        \n        # Get the class with the highest probability\n        _, predicted = torch.max(probs, 1)\n        \n        # Update the correct and total counters\n        correct += (predicted == labels).sum().item()\n        total += labels.size(0)\n        \n        for i in range(0, predicted.shape[0]):           \n            confusion_matrix[predicted[i].item(),labels[i].item()] += 1\n\n# Calculate the accuracy\naccuracy = correct / total\n\nimport matplotlib.pyplot as plt\ncorrect = np.sum(np.diagonal(confusion_matrix))\naccuracy = correct/np.sum(confusion_matrix)\nconfusion_matrix = confusion_matrix/np.sum(confusion_matrix)*100\nplt.imshow(confusion_matrix)\nplt.xlabel(\"true\")\nplt.ylabel(\"predicted\")\nplt.title(\"Accuracy = \" + \"{:2.2f}\".format(accuracy*100) + \"%\")\n\nText(0.5, 1.0, 'Accuracy = 94.76%')\n\n\n\n\n\n\n\n\n\nAs we can see, we reach \\(\\sim 95 \\%\\) accuracy on test data, which is a huge improvement comparing to our previous neural network with only one output node (accuracy \\(\\sim 59 \\%\\)), where we were using MSE as a loss function.\n\n\n\n\n\n\nExercise\n\n\n\nAdd two, or three additinal layers to our network. How does it improve accuracy on a test data?",
    "crumbs": [
      "Course",
      "Fundamentals of deep learning",
      "Multiclass classification"
    ]
  },
  {
    "objectID": "course/deep_learning/pytorch_intro.html#task-and-data",
    "href": "course/deep_learning/pytorch_intro.html#task-and-data",
    "title": "Neural networks with PyTorch",
    "section": "2.1 Task and data",
    "text": "2.1 Task and data\nLet’s start by the task and the data. We will use the MNIST dataset, which is composed of hand-written digit images from 0 to 9. The task will be to classify those images into their respective digits.\n\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import DataLoader, random_split\n\n\nmnist_train = MNIST(root=\"data\", train=True, download=True, transform=ToTensor())\nmnist_test = MNIST(root=\"data\", train=False, download=True, transform=ToTensor())\n\nprint(mnist_train)\nprint(mnist_test)\n\nDataset MNIST\n    Number of datapoints: 60000\n    Root location: data\n    Split: Train\n    StandardTransform\nTransform: ToTensor()\nDataset MNIST\n    Number of datapoints: 10000\n    Root location: data\n    Split: Test\n    StandardTransform\nTransform: ToTensor()\n\n\nIn machine learning, it is very important that we become familiar with the data that we are dealing with. In this case, we may plot some example images.\n\n\nCode\nimport matplotlib.pyplot as plt\n\ndef plot_image(ax, image: torch.Tensor, label: int | None = None):\n    \"Plot a single image.\"\n    ax.imshow(image.squeeze(), cmap=\"gray\")\n    if label is not None:\n        ax.set_title(f\"Pred: {label}\")\n\ndef plot_examples(dataset):\n    \"Plot 5 examples from the MNIST dataset.\"\n    _, axes = plt.subplots(1, 5, figsize=(12, 3))\n    for i, ax in enumerate(axes):\n        image, label = dataset[i]\n        plot_image(ax, image)\n    plt.show()\n\nplot_examples(mnist_train)\n\n\n\n\n\n\n\n\n\nThe images are \\(28 \\times 28\\) pixels in grayscale, and the labels are a single scalar.\n\nimage, label = mnist_train[0]\nimage.shape, label\n\n(torch.Size([1, 28, 28]), 5)\n\n\nNow let’s split the training set into training and validation. This will allow us to evaluate the model’s generalization capabilities during training and tune its hyper-parameters.\n\ntrain_data, validation_data = random_split(mnist_train, [55000, 5000])\n\nFinally, we will create the data loaders for the training, validation, and testing data sets. These objects will take care of spliting the data into batches, given that 60000 images may be too much to process at once.\n\nbatch_size = 128\ntrain_loader = DataLoader(train_data, batch_size, shuffle=True)\nval_loader = DataLoader(validation_data, batch_size, shuffle=False)\ntest_loader = DataLoader(mnist_test, batch_size, shuffle=False)",
    "crumbs": [
      "Course",
      "Fundamentals of deep learning",
      "Neural networks with PyTorch"
    ]
  },
  {
    "objectID": "course/deep_learning/pytorch_intro.html#performance-measure",
    "href": "course/deep_learning/pytorch_intro.html#performance-measure",
    "title": "Neural networks with PyTorch",
    "section": "2.2 Performance measure",
    "text": "2.2 Performance measure\nIn order to train any model, we need to be able to evalute its performance on the given task. This performance measure must be a smooth function that is typically referred to as loss function. The learning process will aim to minimize the loss function, as introduced in the learning as an optimization task section from the introductory chapter.\nIn this case, we have a classification problem with ten classes (digits from 0 to 9). Therefore, we will use the cross-entropy loss function \\[\\mathcal{L}_{\\text{CE}} = -\\frac{1}{n}\\sum_i^n \\mathbf{y}_i^T\\log(f(\\mathbf{x}_i))\\,,\\] where \\(\\mathbf{y}_i\\) is the one-hot-encoding vector of the true label, and \\(f(\\mathbf{x}_i)\\) provides the predicted probability for sample \\(\\mathbf{x}_i\\) to belong to each of the classes.\n\ndef cross_entropy_loss(predictions, targets):\n    \"\"\"Compute the cross-entropy loss between predictions and targets for a given batch.\"\"\"\n    target_preds = predictions[torch.arange(len(predictions)), targets]\n    return -torch.mean(torch.log(target_preds))\n\nBesides the loss function, we can compute other performance indicators that may not need to be differentiable, like the accuracy or the error rate.\n\ndef accuracy(predictions, targets):\n    \"\"\"Compute the accuracy of predictions given the true targets.\"\"\"\n    return (predictions.argmax(dim=1) == targets).float().mean()",
    "crumbs": [
      "Course",
      "Fundamentals of deep learning",
      "Neural networks with PyTorch"
    ]
  },
  {
    "objectID": "course/deep_learning/pytorch_intro.html#model",
    "href": "course/deep_learning/pytorch_intro.html#model",
    "title": "Neural networks with PyTorch",
    "section": "2.3 Model",
    "text": "2.3 Model\nThe last ingredient for our learning task is a model that will encode the program to solve the task. In this case, we will start with a simple fully-connected neural network. In these networks, we distinguish between three types of layers:\n\nThe input layer contains the data values. In this case, it will be the pixel values.\nThe output layer contains the desired output. In this case, the probability for each class.\nThe hidden layers are all the layers between the input and output layers.\n\nIndividual neurons perform simple calculations based on the signal received from by the neurons from the preceding layer. Typically, they perform a linear transformation followed by a non-linear activation function \\(\\xi\\) of the form.\n\\[\\begin{split}\n    z &= \\mathbf{\\omega}^T \\mathbf{x} + b = \\sum_i \\omega_i x_i + b\\\\\n    x &= \\xi(z)\\,.\n\\end{split}\\]\nHere, \\(\\mathbf{x}\\) denotes the activations of the neurons in the preceding layer, and the connection strength between each of those neurons is encoded in the weight vector \\(\\mathbf{\\omega}\\). The neuron incorporates a bias \\(b\\), and the resulting value of the linear transformation \\(z\\) is known as the logit. Finally, the resulting activation of the neuron \\(x\\) is determined by applying the non-linear activation function \\(\\xi\\).\nWe will start by initializing the parameters for our linear operations.\n\ninput_size = 28 * 28\nhidden_size = 500\nn_classes = 10\n\n# Input to hidden\nW1 = torch.randn(input_size, hidden_size) / torch.sqrt(torch.tensor(input_size))\nW1.requires_grad_()\nb = torch.zeros(hidden_size, requires_grad=True)\n\n# Hidden to output\nW2 = torch.randn(hidden_size, n_classes) / torch.sqrt(torch.tensor(hidden_size))\nW2.requires_grad_();\n\nThe activation functions can take any form, so long as it is non-linear, and they can be used to obtain the desired output. In this case, we will use the rectified linear unit (ReLU) activation function in the hidden layer \\[\\text{ReLU}(z) = \\max(0, z)\\,,\\] and a softmax activation function in the output layer to normalize the logits as a probability distribution \\[\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_k e^{z_k}}\\,.\\]\n\ndef relu(x):\n    \"Rectified linear unit activation function.\"\n    return torch.maximum(x, torch.tensor(0.0))\n\ndef softmax(x):\n    \"Softmax activation function.\"\n    return torch.exp(x) / torch.exp(x).sum(axis=-1, keepdim=True)\n\nNow we can define our model.\n\ndef model(x):\n    \"Neural network model.\"\n    x = x.reshape(-1, 28 * 28)  # Flatten the image\n    z = x @ W1 + b  # First linear transformation\n    x = relu(z)  # Hidden layer activation\n    z = x @ W2  # Second linear transformation\n    return softmax(z)  # Output layer activation",
    "crumbs": [
      "Course",
      "Fundamentals of deep learning",
      "Neural networks with PyTorch"
    ]
  },
  {
    "objectID": "course/deep_learning/pytorch_intro.html#training",
    "href": "course/deep_learning/pytorch_intro.html#training",
    "title": "Neural networks with PyTorch",
    "section": "2.4 Training",
    "text": "2.4 Training\nWe have all the necessary ingredients to train a machine learning model for digit recognition. Let’s put everything together in a training loop.\nThe typical learning procedure is:\n\nFor every training batch\n\nEvaluate the model\nCompute the loss\nCompute the gradients of the parameters\nUpdate the parameters\n\nFor every validation batch\n\nEvaluate the model\nCompute the loss\n\nRepeat 1 and 2 for every training epoch\n\n\nlearning_rate = 0.1\nn_epochs = 40\n\ntraining_loss = []\nvalidation_loss = []\n\nfor _ in range(n_epochs):\n    epoch_loss = 0\n    for images, labels in train_loader:\n        preds = model(images)\n        loss = cross_entropy_loss(preds, labels)\n        loss.backward()\n\n        # Gradient descent step\n        with torch.no_grad():\n            # Update parameters\n            W1 -= W1.grad * learning_rate\n            b -= b.grad * learning_rate\n            W2 -= W2.grad * learning_rate\n            # Reset gradients\n            W1.grad.zero_()\n            b.grad.zero_()\n            W2.grad.zero_()\n\n        epoch_loss += loss.item()\n\n    training_loss.append(epoch_loss / len(train_loader))\n\n    with torch.no_grad():\n        epoch_loss = 0\n        val_preds, val_targets = [], []\n        for images, labels in val_loader:\n            preds = model(images)\n            loss = cross_entropy_loss(preds, labels)\n            \n            epoch_loss += loss.item()\n            val_preds.append(preds)\n            val_targets.append(labels)\n\n        val_acc = accuracy(torch.cat(val_preds), torch.cat(val_targets))\n        validation_loss.append(epoch_loss / len(val_loader))\n\n    print(f\"Training Loss: {training_loss[-1]:.4f}, Validation Loss: {validation_loss[-1]:.4f}, Accuracy: {val_acc:.4f}\")\n\nTraining Loss: 0.4941, Validation Loss: 0.3285, Accuracy: 0.9156\nTraining Loss: 0.2658, Validation Loss: 0.2582, Accuracy: 0.9346\nTraining Loss: 0.2140, Validation Loss: 0.2177, Accuracy: 0.9460\nTraining Loss: 0.1793, Validation Loss: 0.1871, Accuracy: 0.9528\nTraining Loss: 0.1548, Validation Loss: 0.1684, Accuracy: 0.9564\nTraining Loss: 0.1362, Validation Loss: 0.1526, Accuracy: 0.9622\nTraining Loss: 0.1211, Validation Loss: 0.1360, Accuracy: 0.9658\nTraining Loss: 0.1090, Validation Loss: 0.1302, Accuracy: 0.9664\nTraining Loss: 0.0988, Validation Loss: 0.1156, Accuracy: 0.9684\nTraining Loss: 0.0900, Validation Loss: 0.1139, Accuracy: 0.9700\nTraining Loss: 0.0831, Validation Loss: 0.1086, Accuracy: 0.9718\nTraining Loss: 0.0767, Validation Loss: 0.1035, Accuracy: 0.9722\nTraining Loss: 0.0712, Validation Loss: 0.1023, Accuracy: 0.9732\nTraining Loss: 0.0662, Validation Loss: 0.0955, Accuracy: 0.9742\nTraining Loss: 0.0618, Validation Loss: 0.0953, Accuracy: 0.9730\nTraining Loss: 0.0579, Validation Loss: 0.0902, Accuracy: 0.9760\nTraining Loss: 0.0542, Validation Loss: 0.0865, Accuracy: 0.9768\nTraining Loss: 0.0511, Validation Loss: 0.0864, Accuracy: 0.9762\nTraining Loss: 0.0480, Validation Loss: 0.0826, Accuracy: 0.9772\nTraining Loss: 0.0452, Validation Loss: 0.0815, Accuracy: 0.9768\nTraining Loss: 0.0428, Validation Loss: 0.0790, Accuracy: 0.9788\nTraining Loss: 0.0407, Validation Loss: 0.0760, Accuracy: 0.9764\nTraining Loss: 0.0385, Validation Loss: 0.0779, Accuracy: 0.9788\nTraining Loss: 0.0364, Validation Loss: 0.0745, Accuracy: 0.9790\nTraining Loss: 0.0345, Validation Loss: 0.0731, Accuracy: 0.9788\nTraining Loss: 0.0327, Validation Loss: 0.0719, Accuracy: 0.9790\nTraining Loss: 0.0311, Validation Loss: 0.0723, Accuracy: 0.9790\nTraining Loss: 0.0297, Validation Loss: 0.0707, Accuracy: 0.9798\nTraining Loss: 0.0283, Validation Loss: 0.0713, Accuracy: 0.9794\nTraining Loss: 0.0269, Validation Loss: 0.0709, Accuracy: 0.9794\nTraining Loss: 0.0257, Validation Loss: 0.0692, Accuracy: 0.9806\nTraining Loss: 0.0244, Validation Loss: 0.0713, Accuracy: 0.9792\nTraining Loss: 0.0235, Validation Loss: 0.0706, Accuracy: 0.9790\nTraining Loss: 0.0222, Validation Loss: 0.0684, Accuracy: 0.9794\nTraining Loss: 0.0212, Validation Loss: 0.0676, Accuracy: 0.9802\nTraining Loss: 0.0204, Validation Loss: 0.0680, Accuracy: 0.9798\nTraining Loss: 0.0195, Validation Loss: 0.0678, Accuracy: 0.9800\nTraining Loss: 0.0186, Validation Loss: 0.0677, Accuracy: 0.9798\nTraining Loss: 0.0180, Validation Loss: 0.0660, Accuracy: 0.9804\nTraining Loss: 0.0171, Validation Loss: 0.0662, Accuracy: 0.9796\n\n\n\nplt.plot(training_loss, label=\"Training Loss\")\nplt.plot(validation_loss, label=\"Validation Loss\")\nplt.xlabel(\"Training epoch\")\nplt.ylabel(\"Cross-entropy loss\")\nplt.grid()\nplt.legend()",
    "crumbs": [
      "Course",
      "Fundamentals of deep learning",
      "Neural networks with PyTorch"
    ]
  },
  {
    "objectID": "course/linear_models/perceptron.html",
    "href": "course/linear_models/perceptron.html",
    "title": "Perceptron",
    "section": "",
    "text": "1 Clasification task\nIn this section we discuss yet anotehr linear classifier, the perceptron. The perceptron is a machinea learning algorithm that allows one to separate with an hyperplane a linearly separable dataset.\nTo this end, we use the function make_blobs of scikit learn to generate a dataset with two labelled cluters in two dimensions.\n\nx, y =make_blobs(centers=np.array([[3,3],[0,0]]), cluster_std=0.5)\ny[y==0]=-1\nprint(x[:4])\nprint(y[:4])\n\n[[-1.18004026 -0.61965393]\n [-0.57032133  0.96112062]\n [ 3.67974571  2.09263418]\n [ 0.16344673 -0.22585435]]\n[ 1  1 -1  1]\n\n\nFigure 1 shows the auto generated dataset.\n\n\nCode\nfig =px.scatter(x=x[:,0],y=x[:,1],color=y.astype(str))\nfig.update_layout(xaxis_title='x1',yaxis_title='x2')\n\n\n\n\n                                                \n\n\nFigure 1: Auto generated dataset with two clusters\n\n\n\n\nThe goal of the perceptron algorithm is to find an hyperplane that separates these two clusters. Let us therefore consider the line\n\\[f(x) = w_0+w_1x_1+w_2 x_2=0\\]\nThe normal vector to this line is given by \\(\\mathbf{w}^*=\\mathbf{w}/\\parallel \\mathbf{w} \\parallel\\).\n\n\n\n\n\n\nExercise\n\n\n\nShow that\n\nfor any point on the line \\[ {\\mathbf{w}^*}^T \\mathbf{x}_1 = -w_0.\\]\nfor any two points \\(\\mathbf{x_1}\\) and \\(\\mathbf{x}_2\\) one the line \\[ {\\mathbf{w}^*}^T (\\mathbf{x_1}-\\mathbf{x}_2)=0.\\]\n\n\n\nThe signed distance between the line and a point is given by\n\\[{\\mathbf{w}^*}^T(\\mathbf{x}-\\mathbf{x}_0)=\\frac{\\mathbf{w}^T\\mathbf{x}+w_0}{\\parallel \\mathbf{w}\\parallel}=\\frac{f(x)}{\\parallel \\mathbf{w}\\parallel}\\]\n\n\n2 Loss function\nThe signed distance between the line and a point discussed in the previsou section offers a natural way to define a loss function. For each datapoint\\({x_i,y_i}\\), we would like to maximize the product\n\\[y_i (\\mathbf{w}^T\\mathbf{x}+w_0).\\]\nIndeed when \\(y_i\\) and \\(\\mathbf{w}^T\\mathbf{x}+w_0\\), this quantity is positive.\nTherefore, we define the loss function to minimize the mean over the missclassified examples\n\\[L =-\\sum_{i=1}^{N_\\text{misclassified}} y_i(\\mathbf{w}^T\\mathbf{x}_i+w_0)\\]\n\ndef ff(w,x):\n    return -(w[1]*x+w[0])/w[2]\nx1 = np.array([x[:,0].min()-1,x[:,0].max()+1])\n\nvec = np.zeros((df.shape[0],2))\nfor i,w in enumerate(df['w']):\n    vec[i,:] = ff(w,x1)\n    \nloss = df['value'].to_numpy()\n\n\n\nCode\ncolor = y.astype(str)\ncolor[y&gt;0] = \"blue\"\ncolor[y&lt;0] = \"red\"\n\nframes = [go.Frame(data=[go.Scatter(x=x1, y=vec[i,:],mode='lines')],layout=go.Layout(title_text=f'step:{i}, Loss:{loss[i]:.2f}')) for i in range(loss.size)]\n\nbuttons = [dict(label=\"Play\",method=\"animate\",\n                args=[None, {\"frame\": {\"duration\": 100, \"redraw\": True},\n                             \"fromcurrent\": True, \n                             \"transition\": {\"duration\": 300,\"easing\": \"quadratic-in-out\"}}]),\n           dict(label=\"Pause\",method=\"animate\",\n                args=[[None], {\"frame\": {\"duration\": 0, \"redraw\": False},\"mode\": \"immediate\",\"transition\": {\"duration\": 0}}]),\n          dict(label=\"Restart\",method=\"animate\",\n                args=[None])]\n\nFig = go.Figure(\n    data=[go.Scatter(x=x1, y= vec[0,:],mode='lines',name = 'line'),\n          go.Scatter(x=x[:,0], y=x[:,1], mode=\"markers\", marker_color=color,name='data',\n                hovertemplate='x:%{x:.2f}'\n                +'&lt;br&gt;y:%{y:.2f}&lt;/br&gt;&lt;extra&gt;&lt;/extra&gt;')],\n    layout=go.Layout(\n        xaxis=dict(range=[x[:,0].min()-2, x[:,0].max()+2], autorange=False),       \n        yaxis=dict(range=[x[:,1].min()-2, x[:,1].max()+2], autorange=False),\n        updatemenus=[dict(\n            type=\"buttons\",\n            buttons=buttons)]\n    ),\n    frames= frames\n)\n\nFig.show()\n\n\n\n\n                                                \n\n\nFigure 2: Animation of t",
    "crumbs": [
      "Course",
      "Linear models",
      "Perceptron"
    ]
  },
  {
    "objectID": "course/linear_models/logistic_regression.html",
    "href": "course/linear_models/logistic_regression.html",
    "title": "Logistic regression",
    "section": "",
    "text": "1 Binary classification\nIn this section, we consider the classification task. In particular, we focus here on the binary classification, whcih means that a datapoint can either be in class \\(0\\) or in class \\(1\\).\nLet us consider a concrete dataset, the toxity dataset. In this dataset, scientists were studying the toxity of a substance to a population of insects. They therefore administrated a serie of toxic doses and measured the number of deaths in the population of \\(250\\) individuals. The results of the study are shown in Figure 1.\n\nn_ind = 250\ndata = dict(doses=range(1,7),deaths=[28,53,93,126,172,197])\ndf = pd.DataFrame.from_dict(data)\ndf['p_dying'] = df['deaths']/n_ind\n\n\n\nCode\npx.bar(data_frame=df, x='doses', y='deaths')\n\n\n\n\n                                                \n\n\nFigure 1: Number of deaths with respect to the number of doses.\n\n\n\n\nHere, the goal of the classification would be to predict, given a number of dosis whether an individual would be death (class \\(0\\)) or alive (class \\(1\\)).\n\n\n2 Logistic regression\nWe can here define a probability distribution of dying for each value of the doses by dividing by the total number of individuals (here \\(250\\)). We can therefore see the classification task as fitting this probability distribution. A simple trial function is the sigmoid function\n\\[ \\sigma(x) =\\frac{1}{1+e^{-(ax+b)}}\\] and the goal is to find the values of \\(a\\) and \\(b\\) that fit the best the data.\n\n\n\n\n\n\nBinomial distribution\n\n\n\nWhen we divide the number of deaths by the number of individuals, we are effectively evaluating the estimator of the parameter \\(p\\) of the Binomial distribution. As the reminder, the Binomial distribution charatcerizes the probability of having \\(k\\) successes over \\(n\\) independent samples of a Bernouilli distribution with succes probability \\(p\\).\nNotice how we transformed our classification problem into a regression problem. We will come back to this probabilisitc view of machine lerning in the next lecture.\n\n\n\ndef sigmoid(x,a,b):\n    return 1/(1 + np.exp(-(a*x+b)))\n\nFigure 2 shows a trial of the fit for the values of \\(a=0.5\\) and \\(b=-2\\). You can play around with the values of \\(a\\) and \\(b\\) to find a more decent fit in the next figure (only in the notebook).\n\n\nCode\na, b = 0.5, -2\n\nvecd = np.arange(0,7,0.1)\n\nFig = go.Figure()\n\n\nFig.add_bar(x=df['doses'], y=df['p_dying'], name='toxity dataset')\n\nFig.add_trace(go.Scatter(name=f'sigmoid: a={a}, b={b}',x=vecd,y=sigmoid(vecd,a,b)))\nFig.update_layout(xaxis_title='doses',yaxis_title='p')\n\n\n\n\n                                                \n\n\nFigure 2: Probability of deaths with respect to the number of doses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nShow that, for the logistic regression,\n\\[\\log(\\frac{p}{1-p})=ax+b\\]\n\n\n\n\n3 The dataset\nUntil now, we have considered the viewpoint of the whole population. But generally, the dataset is built from the data of each individual to which is associated a tuple \\((x,y)\\), where \\(x\\) is the number of dosis and \\(y\\) is the class (\\(0\\): dead, \\(1\\): alive). To this end, we construct a dataset with \\(n_\\text{ind}\\times n_\\text{doses}\\) individuals.\n\nfor i, d in zip(df['doses'], df['deaths']):\n    vec1 = np.zeros((n_ind,2))\n    vec1[:,0], vec1[:d,1] = i, 1\n    vec = vec1 if i ==1 else np.concatenate((vec,vec1))\n    \nnp.random.shuffle(vec)\nx, y = vec[:,0], vec[:,1]\n\n\nprint(x[:20])\nprint(y[:20])\n\n[5. 2. 5. 2. 2. 3. 1. 1. 1. 6. 6. 3. 3. 6. 6. 6. 2. 6. 3. 4.]\n[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1.]\n\n\n\n\n4 Loss function: Binary Cross Entropy\nWe now want to automatically find the values of \\(a\\) and \\(b\\). To this end, we have to define a loss function. We will here use the binary cross entropy\n\\[ BCE = -\\frac{1}{N}\\sum_{i=1}^N y_i \\log [\\sigma(x_i,a,b)] +(1-y_i)\\log[1-\\sigma(x_i,a,b)].\\]\nLet us have a look of the loss landscape of the binary cross entropy with respect to the paramters \\(a\\) and \\(b\\), as depicted in Figure 3. We actually observe that the landscape is convex for this choice of the Loss function. In this particular case, the latter can proven.\n\n\nCode to generate the data for the plot\nvec_a = np.arange(-5,5,0.1)\nvec_b = np.arange(-5,5,0.1)\nmatz = np.zeros((vec_a.size,vec_b.size))\n\nfor i, a1 in enumerate(vec_a):\n    for j, b1 in enumerate(vec_b):\n        p = dict(a=a1, b=b1)\n        matz[i,j] = BCE(x, y, sigmoid, params=p)\n\n\n\n\nCode\nfig = go.Figure()\n\nfig.add_contour(z=matz,x=vec_b, y=vec_a,hovertemplate=\n                    'a:%{y:.2f}'\n                    +'&lt;br&gt;b:%{x:.2f}&lt;/br&gt;'\n                    +'f:%{z:.2f}&lt;extra&gt;&lt;/extra&gt;')\n\n\nd = dict(width=600,\n         height=600,\n         xaxis={'title':'b'},\n         yaxis={'title':'a'}\n       )\n\nfig.update_layout(d)\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 3: Binary Cross Entropy\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWhat is the value loss for your guess? Is it close to the minimum?\n\n\n\n\n5 Gradient of the Loss function\nLet us compute the gradient with respect to \\(a\\) and \\(b\\).\n\n\n\n\n\n\nExercise\n\n\n\nWhat is the gradient of the sigmoid function \\(\\sigma(x)\\)?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\(\\sigma'(x) = \\sigma(x) (1-\\sigma(x))\\)\n\n\n\nWe find\n\\[\\begin{aligned}\n\\partial_a BCE &= \\frac{1}{N}\\sum_{i=1}^N (\\sigma_i -y_i)x_i\\\\\n\\partial_b BCE &= \\frac{1}{N}\\sum_{i=1}^N (\\sigma_i -y_i),\\\\\n\\end{aligned}\\]\nwhere we have defined \\(\\sigma_i\\equiv \\sigma(ax_i+b)\\).\nAlthough the problem is convex, there is no easy way to derive an analytical closed form. This comes from the non-linearity introduced by the sigmoid. Libraries like scikit-learnpropose to use different algorithm such as e.g. conjugate gradient. In the next section, we will consider the stochastic gradient descent approach.\n\n\n6 Stochastic gradient descent\n\n\n\n\n\n\nExercise\n\n\n\nImplement the function grad_BCE(x, y, p) where p=dict(a=a, b=b) is a dictionary containing the the parameters \\(a\\) and \\(b\\). The function should return an array of the form np.array([grad_a, grad_b]).\n\n\nHaving computed the gradients, we can now apply the stochastic gradient descent algorithm.\n\na0, b0 = 2, 1 \n\npini = dict(a=a0, b=b0)\nll = dict(loss=BCE, grads=grad_BCE, fun=sigmoid)\n\ntrackers = sgd(x, y, pini, ll, niter=int(1.5E2))\n\nFigure 4 shows the trajectory of the stochastic gradient descent algorithm.\n\n\nCode\namin, amax = np.min(trackers['a']), np.max(trackers['a'])\nbmin, bmax = np.min(trackers['b']), np.max(trackers['b'])\n\nn = 100\nstepa, stepb =(amax-amin)/n, (bmax-bmin)/n\n\nvec_a = np.arange(amin-19*stepa, amax+20*stepa, stepa)\nvec_b = np.arange(bmin-19*stepb, bmax+20*stepb,stepb)\nmatz = np.zeros((vec_a.size,vec_b.size))\n\nfor i, a1 in enumerate(vec_a):\n    for j, b1 in enumerate(vec_b):\n        p = dict(a=a1, b=b1)\n        matz[i,j] = BCE(x, y, sigmoid, params=p)\n        \nfig = go.Figure()\nfig.add_contour(z=matz,x=vec_b, y=vec_a,\n                hovertemplate=\n                    'a:%{y:.2f}'\n                    +'&lt;br&gt;b:%{x:.2f}&lt;/br&gt;'\n                    +'f:%{z:.2f}&lt;extra&gt;&lt;/extra&gt;')\nmask = np.arange(0,len(trackers['a']),100)\nveca, vecb, vecl = np.array(trackers['a'])[mask],np.array(trackers['b'])[mask], np.array(trackers['loss'])[mask]\nfig.add_scatter(x=vecb, y=veca, name=f'SGD',text=vecl, mode='lines+markers',\n                hovertemplate=\n                'a:%{y:.2f}'\n                +'&lt;br&gt;b:%{x:.2f}&lt;/br&gt;'\n                +'f:%{text:.2f}&lt;extra&gt;&lt;/extra&gt;')\n\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 4: Trajectory in the parameter space of the stochastic gradient descent algorithm.\n\n\n\n\n\n\n7 Accuracy and other metrics\nSo far, we did not discuss about the choice of the metric. The first metric that comes into mind is the accuracy, i.e.\n\\[ \\text{acc}= \\frac{\\# \\text{correct predictions}}{\\#\\text{dataset}},\\]\nwhich considers the ratio between the number of correct predictions and the number of elements in our training set.\nlet us compute the accuracuy in terms of the training.\n\nveca, vecb, vecl = np.array(trackers['a']), np.array(trackers['b']), np.array(trackers['loss'])\nmask = np.arange(0,len(veca),100)\nveca, vecb, vecl = veca [mask], vecb[mask], vecl[mask]\n\n\n\n\n\n\n\nExercise\n\n\n\nWrite the function accuracy(x,y,a,b), which returns the accuracy for a given dataset and for the parameters \\(a\\) and \\(b\\). Choose the label with the following rule: \\(0\\) if \\(\\sigma_i&lt;0.5\\) else \\(1\\).\n\n\n\n\nCode\ndef accuracy(x,y,a,b):\n    yp = sigmoid(x, a, b)\n    yp[yp&gt;0.5] = 1\n    yp[yp&lt;=0.5] = 0\n    return np.sum(yp==y)/y.size\n\n\nWe then compute the accuracy during the training.\n\nvec_acc = np.zeros_like(veca)\n\nfor i,(a,b) in enumerate(zip(veca,vecb)):\n    vec_acc[i] = accuracy(x,y,a,b)\n\nFigure 5 shows the value of the Loss function and the accuracy during the training. It is interesting to note that while the Loss function is a smooth function, the accuracy is not smooth and that the variations of Loss function do not directly correspond to the variations of the accuracy. It is therefore important to check not only the Loss function but also the figure of merit of the problem, that can be for example the accuracy.\n\n\nCode\n# Create figure with secondary y-axis\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n# Add traces\nfig.add_trace(\n    go.Scatter(y=vecl, name=\"Loss\"),\n    secondary_y=False,\n)\n\nfig.add_trace(\n    go.Scatter(y=vec_acc, name=\"Accuracy\"),\n    secondary_y=True,\n)\n\n\n# Set x-axis title\nfig.update_xaxes(title_text=\"iterations\")\n\n# Set y-axes titles\nfig.update_yaxes(title_text=\"Loss\", secondary_y=False)\nfig.update_yaxes(title_text=\"Accuracy\", secondary_y=True)\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 5: Loss function vs Accuracy\n\n\n\n\nDepending on the applications, it might be advantageous to define different figures of merits. The later can be done with the help of the confusion matrix\n\n\n\n\nPositive (Prediction)\nNegative (Prediction)\n\n\n\n\nPositive (Ground Truth)\nTrue Positive\nFalse negative\n\n\nNegative (Ground Truth)\nFalse positive\nTrue Negative\n\n\n\nLet us now construct the confusion matrix for our model.\n\nyp = sigmoid(x, a, b)\nyp[yp&gt;0.5] = 1\nyp[yp&lt;=0.5] = 0\ncm = confusion_matrix(y,yp)\n\nFigure 6 shows the confustion matrix for the logistic regression.\n\n\nCode\nX, Y = [\"Alive(P)\", \"Dead(P)\"], [\"Alive(GT)\", \"Dead(GT)\"]\nfig = px.imshow(cm, x=X, y=Y, text_auto=True,color_continuous_scale='Blues')\n\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 6: Confusion matrix for the logistic regression\n\n\n\n\nWe readily observe that the accuracy correponds to the trace of this matrix normalized by the number of individuals.\n\nprint(f'{np.trace(cm)/np.sum(cm):.3f}')\n\n0.714\n\n\nThere are other metrics that one can use for the predictions\n\n\n\n\n\n\n\n\nMetric\nDescription\nFormula\n\n\n\n\nPrecision\nCorrect positive predictions over the total of positive predictions\n\\(\\frac{TP}{TP+FP}\\)\n\n\nRecall\nCorrect positive predictions over the total of actual positives\n\\(\\frac{TP}{TP+FN}\\)\n\n\nF1 score\nHarmonic mean of precision and recall\n\\(\\frac{2\\text{Recall}\\,\\text{Precision}}{\\text{Recall}+\\text{Precision}}\\)\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCompute the different metrics for the trained logistic regression model.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nprecision = cm[0,0]/(cm[0,0]+cm[1,0])\nrecall = cm[0,0]/(cm[0,0]+cm[0,1])\nF1_score = 2*recall*precision/(precision+recall)\nprint(f'Precision: {precision:.3f}')\nprint(f'Recall: {recall:.3f}')\nprint(f'F1 score: {F1_score:.3f}')\n\n\n\nLet us perform the same analysis on another dataset, the brest cancer dataset of scikit learn.\n\nx1, y1 =load_breast_cancer(return_X_y=True)\nx1 = x1[:,3]\n\nprint(x1[:10])\nprint(y1[:10])\n\n[1001.  1326.  1203.   386.1 1297.   477.1 1040.   577.9  519.8  475.9]\n[0 0 0 0 0 0 0 0 0 0]\n\n\n\nx1 = x1.reshape([np.size(x1),1])\n\nclf = LogisticRegression().fit(x1,y1)\nyp1 = clf.predict(x1)\n\ncm = confusion_matrix(y1,yp1)\n\n\n\nCode\nX1, Y1 = [\"Malignous(P)\", \"Benign(P)\"], [\"Malignous(GT)\", \"Benign(GT)\"]\nfig = px.imshow(cm, x=X1, y=Y1, text_auto=True,color_continuous_scale='Blues')\n\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 7: Confusion matrix for the logistic regression of the brest cancer\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCompute the different metrics for the trained logistic regression model.",
    "crumbs": [
      "Course",
      "Linear models",
      "Logistic regression"
    ]
  },
  {
    "objectID": "course/quantum/vqe.html#finding-the-ground-state-energy-on-nisq-devices",
    "href": "course/quantum/vqe.html#finding-the-ground-state-energy-on-nisq-devices",
    "title": "Variational Quantum Eigensolver (VQE) from scratch",
    "section": "1 Finding the ground state energy on NISQ devices",
    "text": "1 Finding the ground state energy on NISQ devices\nFinding a ground state energy of a quantum many-body system is one of the main objectives of quantum physics. However, the exact diagonalization method fails due to the exponential increase of the Hilbert space with the number of particles. One possible way to overcome such a numerical restriction is to use Variational Quantum Eigensolver (VQE) on Near-Intermediate-Scale-Quantum (NISQ) devices.\nThe VQE is a variational method for finding a ground state in the quantum many-body system on quantum computers with the help of automatic differentiation. It was first proposed in A. Peruzzo, et. al. A variational eigenvalue solver on a photonic quantum processor. The idea behind the VQE is to implement a many-body wave-function ansatz as a parametrized quantum circuit. Next, the variational parameters optimization is prepared as a minimization of an expectation value of the considered Hamiltonian. For more details, see our book Modern applications of machine learning in quantum sciences p. 220. The schematic VQE protocol you can find below:\n\n\n\nVQE_image.png\n\n\nLet’s implement a simple Variational Quantum Eigensolver for system containing \\(L=4\\) spins-1/2 from scratch, using PyTorch automatic differentiation. Let us consider \\(1D\\) Hamiltonian:\n\\[\\begin{equation}\n\\hat{H} = \\sum_{i}\\sum_{\\tau = x,y,z} J_\\tau\\hat{\\sigma}^\\tau_i\\hat{\\sigma}^\\tau_{i+1} + \\sum_i\\sum_{\\tau = x,y,z}h^\\tau \\hat{\\sigma}^{\\tau}_i,\n\\end{equation}\\] where \\(J_\\tau\\) and \\(h_\\tau\\) are Hamiltonian’s parameters, and \\(\\tau = x,y,z\\).\nNext, we have propose an ansatz in the form of a quantum circuit, schematically depicted on the figure below: \nThe above pictorial representation of the ansatz we denote as \\(|\\psi(\\vec{\\theta})\\rangle\\). Our ansatz contains \\(3\\) CNOT gates, and depends on \\(16\\) angles \\(\\theta_i\\), \\(i \\in (1,\\dots,16)\\) which parametrize the rotational gates defined as \\[\\begin{equation}\n\\begin{split}\n  R_y(\\theta_i) & = e^{-i\\theta_i \\hat{\\sigma}^y_i} \\\\\n  R_z(\\theta_i) & = e^{-i\\theta_i \\hat{\\sigma}^z_i},\n\\end{split}\n\\end{equation}\\] where \\(\\hat{\\sigma}^{\\tau}\\) are Pauli operators defined on a spin chain, i.e. \\[\\begin{equation}\n\\hat{\\sigma}^{\\tau}_i = \\mathbb{1}^{i-1}\\otimes\\hat{\\sigma}^\\tau\\otimes\\mathbb{1}^{N-i}.\n\\end{equation}\\] Here, the \\(\\mathbb{1}\\) is a \\(2\\times2\\) identity matrix, and \\(\\hat{\\sigma}^{\\tau}\\) are standard \\(2\\times2\\) Pauli matrices.\nTo find a ground state energy, or more precisiley, a set of parameters for which the expectation value of the Hamiltonian is smallest, we will minimize the energy of the system \\[\\begin{equation}\nE(\\vec{\\theta}) = \\langle \\psi(\\vec{\\theta})|\\hat{H}|\\psi(\\vec{\\theta})\\rangle\n\\end{equation}\\] via optimizing parameters \\(\\vec{\\theta}\\). Optimization will be done via one of the optimization algorithms, such as SGD or Adam.",
    "crumbs": [
      "Course",
      "Automatic differentiation for quantum computing",
      "Variational Quantum Eigensolver (VQE) from scratch"
    ]
  },
  {
    "objectID": "course/quantum/vqe.html#implementation",
    "href": "course/quantum/vqe.html#implementation",
    "title": "Variational Quantum Eigensolver (VQE) from scratch",
    "section": "2 Implementation:",
    "text": "2 Implementation:\nLet’s import all necessary libraries:\n\nimport torch as pt\nfrom torch import matrix_exp as expm\nfrom torch.linalg import eigh as eigh\n\nNow, we implement spin-chain Pauli operators, rotational gates, and CNOT gates:\n\ndef get_Identity(k):  # returns k-tensor product of the identity operator, ie. Id^k\n    Id = id_local\n    for i in range(0, k-1):\n        Id = pt.kron(Id, id_local)\n    return Id\n         \ndef get_string_operator(A, L, i):\n    Op = A\n    if(i == 1):\n        Op = pt.kron(A,get_Identity(L-1))\n        return Op\n    if(i == L):\n        Op = pt.kron(get_Identity(L-1),A)\n        return Op\n    if(i&gt;0 and i&lt;L):\n        Op = pt.kron(get_Identity(i-1), pt.kron(Op, get_Identity(L-i)))\n        return Op\n    \ndef Rx(theta, j):\n    return expm(-1j*theta*sigma_x[j])\n\ndef Ry(theta, j):\n    return expm(-1j*theta*sigma_y[j])\n\ndef Rz(theta, j):\n    return expm(-1j*theta*sigma_z[j])    \n\ndef CNOT(i,j):\n    return  expm(pt.pi/4*(Id - sigma_z[i])@(Id - sigma_x[j])*1j)\n\nNow, let’s define system size and prepare necessary operators:\n\nL = 6   # Number of spins\nD = 2**L # Size of the Hilbert space\n# Pauli operators\nid_local = pt.tensor([[1.,0],[0,1.]])  \nsigma_x_local = pt.tensor([[0,1.],[1.,0]])  \nsigma_y_local = 1j*pt.tensor([[0,-1.],[1.,0]]) \nsigma_z_local = pt.tensor([[1.,0],[0,-1.]])  \n\n# Operators acting on j-th spin in spin chain\nId = get_string_operator(id_local, L, 1)\nsigma_x = {}\nsigma_y = {}\nsigma_z = {}\n\nfor j in range(1,L+1):\n    sigma_x[j] = get_string_operator(sigma_x_local, L, j)\n    sigma_y[j] = get_string_operator(sigma_y_local, L, j)\n    sigma_z[j] = get_string_operator(sigma_z_local, L, j)\n\nNow, let us define our Hamiltonian and calculate its ground state energy with numerical diagonalization:\n\n# Hamiltonian parameters\nJ = {\"x\": 1.,\n     \"y\": 1.,\n     \"z\": -1.}\nh = {\"x\": 1.,\n     \"y\": 1.5,\n     \"z\": 3.}\n\nH = pt.zeros((D,D))\nfor i in range(1,L):\n    H = H + J[\"x\"]*sigma_x[i]@sigma_x[i+1] + J[\"y\"]*sigma_y[i]@sigma_y[i+1] + J[\"z\"]*sigma_z[i]@sigma_z[i+1]\nfor i in range(1,L+1):\n    H = H + h[\"x\"]*sigma_x[i] + h[\"y\"]*sigma_y[i] + h[\"z\"]*sigma_z[i]\n    \n    \nE, P = eigh(H)\nE_GS = E[0]\nprint('Exact ground state energy E_{GS} = ' + \"{:2.2f}\".format(E_GS))\n\nExact ground state energy E_{GS} = -24.58\n\n\nWe define our ansatz for the Hamiltonian ground state:\n\ndef psi_ansatz(theta):   \n    psi_re = pt.zeros(D)\n    psi_im = pt.zeros(D)\n    \n    psi_re[D-1] = 1   # In computational basis this vector \n                      # corresponds to all spins down\n    \n    psi_tmp = pt.complex(psi_re,psi_im)    \n    \n    psi_tmp = Ry(theta[0],1)@psi_tmp\n    psi_tmp = Ry(theta[1],2)@psi_tmp\n    psi_tmp = Ry(theta[2],3)@psi_tmp\n    psi_tmp = Ry(theta[3],4)@psi_tmp\n \n    \n    psi_tmp = Rz(theta[4],1)@psi_tmp\n    psi_tmp = Rz(theta[5],2)@psi_tmp\n    psi_tmp = Rz(theta[6],3)@psi_tmp\n    psi_tmp = Rz(theta[7],4)@psi_tmp    \n  \n    \n\n    psi_tmp = CNOT(1,2)@psi_tmp\n    psi_tmp = CNOT(2,3)@psi_tmp\n    psi_tmp = CNOT(3,4)@psi_tmp\n    \n\n    psi_tmp = Rz(theta[8],1)@psi_tmp\n    psi_tmp = Rz(theta[9],2)@psi_tmp\n    psi_tmp = Rz(theta[10],3)@psi_tmp\n    psi_tmp = Rz(theta[11],4)@psi_tmp       \n   \n    \n    psi_tmp = Ry(theta[12],1)@psi_tmp\n    psi_tmp = Ry(theta[13],2)@psi_tmp\n    psi_tmp = Ry(theta[14],3)@psi_tmp\n    psi_tmp = Ry(theta[15],4)@psi_tmp\n    \n    psi_tmp = CNOT(1,2)@psi_tmp\n    psi_tmp = CNOT(2,3)@psi_tmp\n    psi_tmp = CNOT(3,4)@psi_tmp\n\n    \n    return psi_tmp\n\nFinally, we define our loss function as the expecation value of the Hamiltonian - function which we will minimize with the help of automatic differentiation, and optimization algorithms.\n\ndef get_E(theta):\n    psi_tmp = psi_ansatz(theta)\n    E = pt.vdot(psi_tmp, H@psi_tmp)\n    return E\n\nIn the last step, we initialize initial values of the theta as a torch tensor with trainable entries, and iteratively find optimal parameters \\(\\vec{\\theta}\\):\n\ntheta = pt.zeros(16,requires_grad=True)\noptimizer = pt.optim.Adam([theta],lr = 1e-1)\nE_variational_vs_epochs = []\nfor i in range(0,200):\n    loss = get_E(theta)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    E_variational_vs_epochs.append(get_E(theta).item().real)\n    \nprint(\"Exact diagonalization provides E_GS = \" + \"{:2.2f}\".format(E_GS))\nprint(\"VQE                   provides E_GS = \" + \"{:2.2f}\".format(E_variational_vs_epochs[-1]))\n\nExact diagonalization provides E_GS = -24.58\nVQE                   provides E_GS = -24.05\n\n\nAs we can see, a very simple ansatz for VQE provides a quite ground state accurate energy, close to value obtained via the Exact Diagonalization method.",
    "crumbs": [
      "Course",
      "Automatic differentiation for quantum computing",
      "Variational Quantum Eigensolver (VQE) from scratch"
    ]
  },
  {
    "objectID": "course/index.html",
    "href": "course/index.html",
    "title": "Structure of the lectures",
    "section": "",
    "text": "flowchart TD\nsubgraph one[ ]\n A(linear models)--&gt; A1(Linear regression)\n A --&gt; A2(Polynomial Regression)\n A --&gt; A3(Logistic Regression)\n A --&gt; A4(Perceptron)\nend\nstyle one fill:#82c4c3,stroke-width:0px;\nclassDef boxes stroke-width:0px;\nclass A,A1,A2,A3,A4 boxes;\n\n\n\n\n\n\n\n\n\n\n\nflowchart TD\nsubgraph three[ ]\n C(A Probabilistic View on Machine Learning)--&gt; C1(Review Probability)\n C--&gt;C2(Likelihood)\n C--&gt;C3(Kullback Leibler Divergence)\n C--&gt;C4(Linear Regression) \nend\nstyle three fill:#f6d887,stroke-width:0px;\nclassDef boxes stroke-width:0px;\nclass C,C1,C2,C3,C4 boxes;\n\n\n\n\n\n\n\n\n\n\n\nflowchart TD\nsubgraph two[ ]\n B(Neural Networks)--&gt; B1(Perceptron)\n B--&gt;B2(Deep Neural Networks)\n B--&gt;B3(Automatic Differentiation)\nend\nstyle two fill:#82c4c3,stroke-width:0px;\nclassDef boxes stroke-width:0px;\nclass B,B1,B2,B3,B4 boxes;",
    "crumbs": [
      "Course"
    ]
  },
  {
    "objectID": "course/generative/language_models.html#example-task",
    "href": "course/generative/language_models.html#example-task",
    "title": "Language models",
    "section": "1.1 Example task",
    "text": "1.1 Example task\nWe will introduce language models with an example task of training a model to count numbers “1,2,3,4,…,8765,8766,8767,…”. This seems like a rather simple task that could be easily achieved numerically with a single line of code. However, we will consider the digits as strings that conform sentences.\nThis toy example will allow us to understand the main concepts behind language models. We will use it as a running example and implement the main ideas as we see them.\nHere, we will build our data set, which is nothing more than a text document containing the numbers.\n\nmax_num = 1_000_000\ntext = \",\".join([str(i) for i in range(max_num)])\n\nLet’s see the first and last few digits of our data set.\n\n\nCode\nprint(text[:20])\nprint(text[-20:])\n\n\n0,1,2,3,4,5,6,7,8,9,\n999997,999998,999999",
    "crumbs": [
      "Course",
      "Generative models",
      "Language models"
    ]
  },
  {
    "objectID": "course/generative/language_models.html#giving-numerical-meaning-to-text",
    "href": "course/generative/language_models.html#giving-numerical-meaning-to-text",
    "title": "Language models",
    "section": "1.2 Giving numerical meaning to text",
    "text": "1.2 Giving numerical meaning to text\nWe can communicate very deep concepts with words, but how does a machine understand them?\nWhen we work with text, we split it into elementary pieces called tokens. This is known as tokenization and there is quite a lot of freedom on how to do it. For example, we can take from full sentences, to words, to single characters. The most common practice is to use sub-word tokens that are between single characters to full words, such as SentencePiece. We can also have special tokens to account for additional grammatical information. For example, we can use special tokens to indicate the beggining and ending of a sequence, or to indicate that the words start with capital letters.\nLet’s see a simple tokenization example. We would take the following sentence:\nMy cat won't stop purring.\nAnd transform it into the tokens:\n&lt;BoS&gt;&lt;x_maj&gt;&lt;my&gt; &lt;cat&gt; &lt;wo&gt;&lt;n't&gt; &lt;stop&gt; &lt;purr&gt;&lt;ing&gt;&lt;.&gt;&lt;EoS&gt;\n\n\n\n\n\n\nNote\n\n\n\nI just made up this tokenization, this is just to provide an idea.\n\n\nWith this, we define a vocabulary of tokens. To provide them with “meaning”, we assign a trainable parameter vector to each of them, which are known as embedding vectors. The larger the embedding, the richer the information we can associate to every individual token. We typically store these vectors in a so-called embedding matrix, where every row provides the associated embedding vector to a token. This way, we identify the tokens by an integer index that corresponds to their row in the embedding matrix.\nTaking long tokens results into large vocabularies and, therefore, we need more memory. However, we can generate a piece of text with just a few inference steps. Conversely, short tokens require much less memory at the cost of more inference steps to write. Thus, this presents a trade-off between memory and computational time. You can get some intuition about it by comparing the number of letters in the alphabet (shortest possible tokens) with the number of entries in a dictionary (every word is a token).\nTo process a piece of text, we first split it into the tokens of our vocabulary (tokenization), and replace the tokens by their corresponding indices (numericalization).\nLet’s see how this works in our example task. First of all, we build the token vocabulary. In this simple case, every digit is a token together with the separator “,”.\n\nvocab = sorted(list(set(text)))\nvocab_size = len(vocab)\nvocab\n\n[',', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n\n\nNow we can build a Tokenizer class to encode raw text into tokens, and decode tokens to actual text.\n\nclass Tokenizer:\n    def __init__(self, vocab):\n        self.s2i = {char: i for i, char in enumerate(vocab)}\n        self.i2s = {i: char for char, i in self.s2i.items()}\n    \n    def encode(self, string):\n        \"Tokenize an input string\"\n        return [self.s2i[char] for char in string]\n    \n    def decode(self, ints):\n        \"Transform a list of integers to a string of characters\"\n        return ''.join([self.i2s[i] for i in ints])\n\n\ntkn = Tokenizer(vocab)\n\nLet’s see the map from tokens to integer.\n\ntkn.s2i\n\n{',': 0,\n '0': 1,\n '1': 2,\n '2': 3,\n '3': 4,\n '4': 5,\n '5': 6,\n '6': 7,\n '7': 8,\n '8': 9,\n '9': 10}\n\n\nWe can try our tokenizer with a text example.\n\npre_tkn = text[:10]\npre_tkn, tkn.encode(pre_tkn)\n\n('0,1,2,3,4,', [1, 0, 2, 0, 3, 0, 4, 0, 5, 0])\n\n\nWe can also test the decoding function by encoding and decoding.\n\ntkn.decode(tkn.encode(pre_tkn))\n\n'0,1,2,3,4,'\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere we only perform the text pre-processing. The embedding belongs to the machine learning model.",
    "crumbs": [
      "Course",
      "Generative models",
      "Language models"
    ]
  },
  {
    "objectID": "course/generative/language_models.html#learning-the-data-probability-distribution",
    "href": "course/generative/language_models.html#learning-the-data-probability-distribution",
    "title": "Language models",
    "section": "1.3 Learning the data probability distribution",
    "text": "1.3 Learning the data probability distribution\nTo learn how to generate text, we need to learn the underlying distribution of the data we wish to replicate \\(p_{\\text{data}}(\\mathbf{x})\\). We model text as a sequence of tokens \\(\\mathbf{x}=\\left[x_1, x_2, \\dots, x_{T-1}\\right]\\), and the goal is to predict the next token \\(x_T\\). This way, we can recursively generate text:\n\nWe start with some initial context \\(x_1, x_2, \\dots, x_{T-1}\\).\nWe predict the next token \\(x_T\\), given the context.\nWe append the prediction to the existing text and repeat the process taking \\(x_1,\\dots,x_T\\) as context.\n\nWe typically do this defining a parametrized model to approximate the probability distribution, \\(p_\\theta(\\mathbf{x})\\approx p_{\\text{data}}(\\mathbf{x})\\). The parameters \\(\\theta\\) can represent from the weights of a neural network, to the coefficients of a gaussian mixture model.\nA standard technique in the machine learning field is to use the chain rule of probability to model sequential data. This way, the probability to observe a sequence of tokens can be described as \\[p_{\\theta}(\\mathbf{x})=p_\\theta(x_1)\\prod_{t=2}^{T}p_\\theta(x_t|x_1\\dots x_{t-1})\\,.\\]\nWe optimize our model parameters to obtain the maximum likelihood estimator, which is the most statistically efficient estimator. In this tutorial, we do not want to dive too deep in the details. The main intuition behind it is that we try to maximize the likelihood of observing the training data under our parametrized model. As such, we wish to minimize the negative log-likelihood loss or cross-entropy loss: \\[\\theta^* = \\text{arg}\\,\\text{min}_\\theta - \\frac{1}{N}\\sum_{i=1}^N \\log p_\\theta\\left(\\mathbf{x}^{(i)}\\right) = \\text{arg}\\,\\text{min}_\\theta - \\frac{1}{N}\\sum_{i=1}^N\\sum_{t=1}^T \\log p_\\theta\\left(x_t^{(i)}|x_{&lt;t}^{(i)}\\right)\\]\nWe can understand the task as a classification problem at every time-step where the goal is to predict the token that follows. Thus, we can build our self-supervised classification task by simply taking the text shifted by one position as target for our prediction. For example, consider the tokenized sentence\n&lt;this&gt; &lt;language&gt; &lt;model&gt; &lt;rocks&gt;&lt;!&gt;\nGiven the tokens\n&lt;this&gt; &lt;language&gt;\nwe wish to predict\n&lt;model&gt;\namong all the tokens in the vocabulary.\nAs we typically do in machine learning, we find the optimal parameters \\(\\theta^*\\), i.e., train our model, with gradient-based optimization.",
    "crumbs": [
      "Course",
      "Generative models",
      "Language models"
    ]
  },
  {
    "objectID": "course/generative/language_models.html#data-processing",
    "href": "course/generative/language_models.html#data-processing",
    "title": "Language models",
    "section": "2.1 Data processing",
    "text": "2.1 Data processing\nFirst of all, we need to properly arrange our data. We will start by tokenizing the whole text piece.\n\ndata = torch.tensor(tkn.encode(text))\ndata[:20]\n\ntensor([ 1,  0,  2,  0,  3,  0,  4,  0,  5,  0,  6,  0,  7,  0,  8,  0,  9,  0,\n        10,  0])\n\n\nNow we need to save a part of the data for validation and keep the rest for training. In generative models, we do not tend to use too much data for validation because it is just to get a rough idea of how it is working. In the end, we will evaluate the performance ourselves asking the model to generate samples.\nTo keep this simple, we will save the last numbers as validation data.\n\n\n\n\n\n\nNote\n\n\n\nGiven the nature of our data, it would be best to save chunks of the data sampled at different points along the whole text piece.\n\n\n\nval_pct = 0.1\nsplit_idx = int(len(data)*val_pct)\ndata_train = data[:-split_idx]\ndata_val = data[-split_idx:]\n\n\ndata_train.shape, data_val.shape\n\n(torch.Size([6200001]), torch.Size([688888]))\n\n\nTo train machine learning models, we take advantage of parallelization to process several samples at once. To do so, we will split the text in sub-sequences from which we will build our training batches.\n\ndef get_batch(data, batch_size, seq_len):\n    idx = torch.randint(len(data)-seq_len, (batch_size,))\n    x = torch.stack([data[i:i+seq_len] for i in idx])\n    y = torch.stack([data[i:i+seq_len] for i in idx+1])\n    return x.to(device), y.to(device)\n\n\nbatch_size = 64\nseq_len = 8\nxb, yb = get_batch(data_train, batch_size, seq_len)\n\n\nxb.shape\n\ntorch.Size([64, 8])\n\n\n\nxb[0], yb[0]\n\n(tensor([ 3,  0,  8,  8, 10,  1,  2,  4], device='cuda:0'),\n tensor([ 0,  8,  8, 10,  1,  2,  4,  0], device='cuda:0'))",
    "crumbs": [
      "Course",
      "Generative models",
      "Language models"
    ]
  },
  {
    "objectID": "course/generative/language_models.html#model-definition",
    "href": "course/generative/language_models.html#model-definition",
    "title": "Language models",
    "section": "2.2 Model definition",
    "text": "2.2 Model definition\nWe will make a bigram model that predicts the following character based on the previous one. These models are stochastic and, therefore, the output of the model is a probability distribution over our vocabulary. We can easily achieve this by making the embedding size as large as the vocabulary. This way, when we index into the embedding matrix with a token, we immediately obtain the probability distribution over the possible next tokens.\n\nclass BigramLanguageModel(nn.Module):\n    \"Language model that predicts text based on the previous character.\"\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, vocab_size)\n        \n    def forward(self, x):\n        logits = self.embedding(x)\n        return logits\n        \n    @torch.no_grad()\n    def generate(self, x, new_tkn):\n        for _ in range(new_tkn):\n            logits = self(x)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            new_tkn = torch.multinomial(probs, 1)\n            x = torch.cat((x, new_tkn), dim=1)\n        return x\n\n\nbigram_model = BigramLanguageModel(vocab_size).to(device)\n\n\nxb.shape, bigram_model(xb).shape\n\n(torch.Size([64, 8]), torch.Size([64, 8, 11]))\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe logits we define here are the unnormalized probability scores for each token. To transform them in a normalized probability distribution, we use a SoftMax function. We will see below that pytorch takes the logits directly to compute the loss function instead of the probabilities.\n\n\nLet’s try generating some text with our model.\n\ncontext = torch.zeros((1, 1), dtype=torch.long).to(device)\ntkn.decode(bigram_model.generate(context, 20)[0].tolist())\n\n',,779031473,20590,877'",
    "crumbs": [
      "Course",
      "Generative models",
      "Language models"
    ]
  },
  {
    "objectID": "course/generative/language_models.html#training-loop",
    "href": "course/generative/language_models.html#training-loop",
    "title": "Language models",
    "section": "2.3 Training loop",
    "text": "2.3 Training loop\nWith the data and the model, we’re almost ready to do the training. We need to define a loss function and an optimiziation algorithm to update our model parameters.\nAs we have mentioned before, we wish to minimize the negative log-likelihood of the data with respect to the model. To do so, we use pytorch’s cross entropy loss.\n\ndef cross_entropy_loss(logits, targets):\n    \"Cross entropy loss flattening tensors\"\n    BS, T, H = logits.shape\n    loss = F.cross_entropy(logits.view(BS*T, H), targets.view(-1))\n    return loss\n\nThen, as optimizer, we will use Adam.\n\noptimizer = torch.optim.AdamW(bigram_model.parameters(), lr=1e-3)\n\nNow let’s define the training loop.\n\nbatch_size = 32\nseq_len = 24\ntrain_steps = 3000\n\nfor _ in range(train_steps):\n    xb, yb = get_batch(data_train, batch_size, seq_len)\n    \n    optimizer.zero_grad()\n    logits = bigram_model(xb)\n    loss = cross_entropy_loss(logits, yb)\n    loss.backward()\n    optimizer.step()\n    \nprint(loss.item())\n\n2.370857000350952\n\n\nWe will plug this into a function for later usage and estimate the loss on the validation set.\n\ndef train_model(steps, model, lr, batch_sz, seq_len):\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    for i in range(steps):\n        xb, yb = get_batch(data_train, batch_sz, seq_len)\n        optimizer.zero_grad()\n        logits = model(xb)\n        loss = cross_entropy_loss(logits, yb)\n        loss.backward()\n        optimizer.step()\n        if i % 200 == 0 or i == steps - 1:\n            losses = estimate_loss(model, batch_sz, seq_len)\n            print(f\"Step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n    return model\n\n@torch.no_grad()\ndef estimate_loss(model, batch_sz, seq_len, eval_iters=50):\n    \"\"\"Estimate losses for train and validation data sets.\n    Adapted from https://github.com/karpathy/nanoGPT\"\"\"\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(data_train if split == 'train' else data_val,\n                             batch_sz, seq_len)\n            logits = model(X)\n            loss = cross_entropy_loss(logits, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nBigram models can’t accomplish this example task. After every digit, all the other digits are equally likely to happen if we do not consider any further context. This model can only take advantage of the separator ,. For instance, we know there will not be two consecutive separators and that the following number won’t start with 0.\nWe can see this in the first row of the embedding matrix.\n\n\nCode\nembedding_matrix = list(bigram_model.parameters())[0] \nembedding_matrix.softmax(-1)[0]\n\n\ntensor([0.0007, 0.0007, 0.1393, 0.0884, 0.1212, 0.1808, 0.1448, 0.1381, 0.0857,\n        0.0989, 0.0012], device='cuda:0', grad_fn=&lt;SelectBackward0&gt;)\n\n\nLet’s generate some text.\n\ncontext = torch.zeros((1, 1), dtype=torch.long).to(device)\ntkn.decode(bigram_model.generate(context, 20)[0].tolist())\n\n',69,475,8423,1207856,'\n\n\nIn contrast to the previous example, we see the model has learned to not add consecutive separators, but the digits are still random. GPT time!",
    "crumbs": [
      "Course",
      "Generative models",
      "Language models"
    ]
  },
  {
    "objectID": "course/generative/language_models.html#transformer",
    "href": "course/generative/language_models.html#transformer",
    "title": "Language models",
    "section": "3.1 Transformer",
    "text": "3.1 Transformer\nThe architecture behind the GPT language models is based on the transformer, depicted in Figure 2.\n\n\n\n\n\n\nFigure 2: Transformer schematic representation.\n\n\n\nThe transformer was introduced as an architecture for translation tasks with two main parts: the encoder (left) and the decoder (right). The decoder is the responsible part for generating the translated text and, thus, it is the language model bit of the whole architecture.\nThe transformer architecture relies heavily on self-attention mechanisms. Indeed, the original paper is called “Attention is all you need”. Unlike the bigram model, the transformer decoder can account for all the possible relationships between tokens in the past text to generate the new tokens.",
    "crumbs": [
      "Course",
      "Generative models",
      "Language models"
    ]
  },
  {
    "objectID": "course/generative/language_models.html#causal-self-attention",
    "href": "course/generative/language_models.html#causal-self-attention",
    "title": "Language models",
    "section": "3.2 Causal self-attention",
    "text": "3.2 Causal self-attention\nThe key element in the transformer architecture is the self-attention layer. This allows our tokens in our text piece to “communicate with each other” in a fixed way:\n\nFor every token, we compute three quantities: a key \\(\\mathbf{k}\\), a query \\(\\mathbf{q}\\) and a value \\(\\mathbf{v}\\).\nThen, tokens compare their query to the other tokens’ keys.\nThe resulting value for each token is the weighted average of all the values according to the query-key similarity.\n\nWe compute the similarity between keys and queries doing the dot product between the vectors. Then, to ensure the similarity weights are normalized, we apply the softmax activation function to all the dot products of the query of interest with all the keys. We can efficiently compute all of these with matrix multiplications: \\[\\text{Attention}(Q,K,V) = \\text{SoftMax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\\,\\] where \\(Q,K,V\\) are the matrices containing the query, key and value vectors of every token in the text as rows. \\(d_k\\) denotes the size of the key vector, and the normalization ensures the model is numerically stable. Without this normalization, even if \\(Q\\) and \\(K\\) have unit variance, the variance of their product is of the order of the head size \\(d_k\\).\nSo far, we have not mentioned how to get the queries, keys and values from our tokens. We can choose any differentiable function. In the GPT architecture, they use a dense linear layer.\n\nclass AttentionHead(nn.Module):\n    \"Self-attention head.\"\n    def __init__(self, emb_sz, head_sz):\n        super().__init__()\n        self.key = nn.Linear(emb_sz, head_sz, bias=False)\n        self.query = nn.Linear(emb_sz, head_sz, bias=False)\n        self.value = nn.Linear(emb_sz, head_sz, bias=False)\n    \n    def forward(self, x):\n        q = self.query(x) # (BS, T, H)\n        k = self.key(x)\n        v = self.value(x)\n        \n        w = q @ k.transpose(-2, -1) * k.shape[-1]**(-0.5) # (BS, T, T)\n        return w.softmax(-1) @ v # (BS, T, H)\n\nThis attention mechanism on its own, allows all the tokens to “see” each other at all times. This is what we would see in the transformer encoder, as all the source text in a translation task already exists. However, the transformer decoder can only attend to text as it is being generated. This means that, while we train it, we need to ensure that tokens cannot attend to what would be future innexistent ones.\nThis seems obvious because, at inference time, we clearly only have the text that is already generated. Nonetheless, during training, we sample full sequence chunks of a fixed sequence length. We can take the maximum advantage of this by training our model to generate new tokens for all the possible contexts available in this chunk, from a single initial token to all.\n\nbatch_size, seq_len = 1, 8 \nxb, yb = get_batch(data_train, batch_size, seq_len)\n\n\nclass CausalAttentionHead(nn.Module):\n    \"Masked self-attention head.\"\n    def __init__(self, emb_sz, head_sz, seq_len, dropout=0.2):\n        super().__init__()\n        self.key = nn.Linear(emb_sz, head_sz, bias=False)\n        self.query = nn.Linear(emb_sz, head_sz, bias=False)\n        self.value = nn.Linear(emb_sz, head_sz, bias=False)\n        self.register_buffer('mask', torch.tril(torch.ones(seq_len, seq_len)))\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        q = self.query(x) # (BS, T, H)\n        k = self.key(x)\n        v = self.value(x)\n        \n        _, T, _ = x.shape\n        w = q @ k.transpose(-2, -1) * k.shape[-1]**(-0.5)    # (BS, T, T)\n        w = w.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n        return self.dropout(w.softmax(-1)) @ v # (BS, T, H)\n\n\n\n\n\n\n\nNote\n\n\n\nThis implementation works well. However, pytorch provides a torch.nn.functional.scaled_dot_product_attention that uses specialized CUDA kernels.\n\n\nNow that we have implemented the self-attention attention mechanism, let’s make a first version of our GPT model. The model will have an embedding, an attention layer and a fully connected layer.\n\nclass GPT(nn.Module):\n    \"GPT-like model\"\n    def __init__(self, vocab_sz, emb_sz, head_sz, seq_len):\n        super().__init__()\n        self.seq_len = seq_len\n        self.embedding = nn.Embedding(vocab_sz, emb_sz)\n        self.attn = CausalAttentionHead(emb_sz, head_sz, seq_len)\n        self.linear = nn.Linear(head_sz, vocab_sz)\n        \n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.attn(x)\n        return self.linear(x)\n    \n    @torch.no_grad()\n    def generate(self, x, new_tkn):\n        for _ in range(new_tkn):\n            context = x[:, -self.seq_len:]\n            logits = self(context)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            new_tkn = torch.multinomial(probs, 1)\n            x = torch.cat((x, new_tkn), dim=1)\n        return x\n\n\ntorch.manual_seed(7)\nbatch_size, seq_len = 64, 60\nemb_sz, head_sz = 16, 16\ngpt = GPT(vocab_size, emb_sz, head_sz, seq_len).to(device)\n\n\ntrain_model(400, gpt, 1e-3, batch_size, seq_len)\n\nStep 0: train loss 2.4356, val loss 2.4098\nStep 200: train loss 2.2399, val loss 2.3747\nStep 399: train loss 2.2106, val loss 2.3604\n\n\n\ncontext = torch.zeros((1, 1), dtype=torch.long).to(device)\ntkn.decode(gpt.generate(context, 20)[0].tolist())\n\n',9,56863316,5276562,5'\n\n\nAlready with this basic transformer decoder, we reach a loss that is lower than the bigram model, but it is still not completing the task appropiately. Let’s keep the work up!",
    "crumbs": [
      "Course",
      "Generative models",
      "Language models"
    ]
  },
  {
    "objectID": "course/generative/language_models.html#positional-encoding",
    "href": "course/generative/language_models.html#positional-encoding",
    "title": "Language models",
    "section": "3.3 Positional encoding",
    "text": "3.3 Positional encoding\nWith self-attention, our model can combine the information between all the tokens, but it has no notion about the relative distances between them. To solve this, we can provide our model with a positional encoding, as it is illustrated in Figure 2.\nThere are many different ways to provide the model with information about the token positions. In GPT, they use a positional embedding. This is the same as the vocabulary embedding with the difference that we will have as many rows in the embedding matrix as the maximum sequence length that we allow our model to process.\nLet’s implement it!\n\nclass GPT(nn.Module):\n    \"GPT-like model\"\n    def __init__(self, vocab_sz, emb_sz, head_sz, seq_len):\n        super().__init__()\n        self.seq_len = seq_len\n        self.embedding_tkn = nn.Embedding(vocab_sz, emb_sz)\n        self.embedding_pos = nn.Embedding(seq_len, emb_sz)\n        self.attn = CausalAttentionHead(emb_sz, head_sz, seq_len)\n        self.linear = nn.Linear(head_sz, vocab_sz)\n        \n    def forward(self, x):\n        emb_tkn = self.embedding_tkn(x)\n        emb_pos = self.embedding_pos(torch.arange(x.shape[1], device=x.device))\n        x = emb_tkn + emb_pos\n        x = self.attn(x)\n        return self.linear(x)\n    \n    @torch.no_grad()\n    def generate(self, x, new_tkn):\n        for _ in range(new_tkn):\n            context = x[:, -self.seq_len:]\n            logits = self(context)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            new_tkn = torch.multinomial(probs, 1)\n            x = torch.cat((x, new_tkn), dim=1)\n        return x\n\n\ntorch.manual_seed(7)\nbatch_size, seq_len = 64, 60\nemb_sz, head_sz = 16, 16\ngpt = GPT(vocab_size, emb_sz, head_sz, seq_len).to(device)\n\n\ntrain_model(1000, gpt, 5e-4, batch_size, seq_len)\n\nStep 0: train loss 2.3120, val loss 2.0298\nStep 200: train loss 1.6010, val loss 1.3704\nStep 400: train loss 1.5358, val loss 1.3347\nStep 600: train loss 1.4745, val loss 1.2902\nStep 800: train loss 1.4576, val loss 1.2444\nStep 999: train loss 1.4536, val loss 1.2777\n\n\n\ncontext = torch.ones((1, 1), dtype=torch.long).to(device)\ngpt.eval()\ntkn.decode(gpt.generate(context, 20)[0].tolist())\n\n'01100,,91100,,02101,,'\n\n\nWe have significantly reduced the loss, but it actually seems to do worse!",
    "crumbs": [
      "Course",
      "Generative models",
      "Language models"
    ]
  },
  {
    "objectID": "course/generative/language_models.html#multi-head-attention",
    "href": "course/generative/language_models.html#multi-head-attention",
    "title": "Language models",
    "section": "3.4 Multi-head attention",
    "text": "3.4 Multi-head attention\nSo far, we have only used a single attention head in our model. In the GPT architecture, we use multi-head attention which consists of running various independent. Then, we concatenate the output of the different heads and project the resulting feature vectors to the original embedding size.\n\nclass MultiHeadAttention(nn.Module):\n    \"Multiple parallel self-attention heads.\"\n\n    def __init__(self, num_heads, emb_sz, head_sz, seq_len):\n        super().__init__()\n        self.heads = nn.ModuleList([CausalAttentionHead(emb_sz, head_sz, seq_len)\n                                    for _ in range(num_heads)])\n        self.linear = nn.Linear(head_sz*num_heads, emb_sz)\n\n    def forward(self, x):\n        x = torch.cat([head(x) for head in self.heads], dim=-1)\n        x = self.linear(x)\n        return x\n\nUsually, we take the embedding size and divide it by the number of heads to have better control of the matrix sizes within our model.\n\n\n\n\n\n\nNote\n\n\n\nHere, we have implemented the heads sequentially instead of in parallel. There is a much faster way to compute all the attention heads at once. The tensor dimensions in the self-attention module are [BS, T, E], where E denotes the embedding size. Since all the opperations are carried over the last two dimensions, if we reshape the Q, K, V tensors to [BS, NH, T, HS], where NH and HS denote the number of heads and head size, respectively, we can compute the self-attention for all the heads at once.\n\n\n\nclass GPT(nn.Module):\n    \"GPT-like model\"\n    def __init__(self, vocab_sz, emb_sz, n_head, seq_len):\n        super().__init__()\n        self.seq_len = seq_len\n        self.embedding_tkn = nn.Embedding(vocab_sz, emb_sz)\n        self.embedding_pos = nn.Embedding(seq_len, emb_sz)\n        head_sz = emb_sz // n_head\n        self.attn = MultiHeadAttention(n_head, emb_sz, head_sz, seq_len)\n        self.linear = nn.Linear(emb_sz, vocab_sz)\n        \n    def forward(self, x):\n        emb_tkn = self.embedding_tkn(x)\n        emb_pos = self.embedding_pos(torch.arange(x.shape[1], device=x.device))\n        x = emb_tkn + emb_pos\n        x = self.attn(x)\n        return self.linear(x)\n    \n    def generate(self, x, new_tkn):\n        for _ in range(new_tkn):\n            context = x[:, -self.seq_len:]\n            logits = self(context)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            new_tkn = torch.multinomial(probs, 1)\n            x = torch.cat((x, new_tkn), dim=1)\n        return x\n\n\ntorch.manual_seed(7)\nbatch_size, seq_len = 64, 60\nemb_sz, n_head = 16, 2\ngpt = GPT(vocab_size, emb_sz, n_head, seq_len).to(device)\n\n\ngpt = train_model(5000, gpt, 2e-4, batch_size, seq_len)\n\nStep 0: train loss 2.4114, val loss 2.4379\nStep 200: train loss 2.3326, val loss 2.3935\nStep 400: train loss 2.2342, val loss 2.3961\nStep 600: train loss 2.1180, val loss 2.3374\nStep 800: train loss 1.9681, val loss 2.1082\nStep 1000: train loss 1.7882, val loss 1.8349\nStep 1200: train loss 1.5916, val loss 1.5813\nStep 1400: train loss 1.4464, val loss 1.3999\nStep 1600: train loss 1.3529, val loss 1.2617\nStep 1800: train loss 1.3099, val loss 1.1754\nStep 2000: train loss 1.2533, val loss 1.1168\nStep 2200: train loss 1.2490, val loss 1.0802\nStep 2400: train loss 1.2136, val loss 1.0455\nStep 2600: train loss 1.2242, val loss 1.0329\nStep 2800: train loss 1.2253, val loss 1.1018\nStep 3000: train loss 1.2024, val loss 0.9837\nStep 3200: train loss 1.1999, val loss 1.0419\nStep 3400: train loss 1.2010, val loss 1.0247\nStep 3600: train loss 1.2008, val loss 0.9965\nStep 3800: train loss 1.1920, val loss 1.0156\nStep 4000: train loss 1.2110, val loss 0.9998\nStep 4200: train loss 1.1858, val loss 1.0006\nStep 4400: train loss 1.1929, val loss 1.0027\nStep 4600: train loss 1.1902, val loss 1.0175\nStep 4800: train loss 1.1692, val loss 0.9676\nStep 4999: train loss 1.1663, val loss 1.0324\n\n\n\ncontext = torch.ones((1, 1), dtype=torch.long).to(device)\ngpt.eval()\ntkn.decode(gpt.generate(context, 20)[0].tolist())\n\n'081111218121121812,12'",
    "crumbs": [
      "Course",
      "Generative models",
      "Language models"
    ]
  },
  {
    "objectID": "course/generative/language_models.html#feedforward",
    "href": "course/generative/language_models.html#feedforward",
    "title": "Language models",
    "section": "3.5 Feedforward",
    "text": "3.5 Feedforward\nIn the transformer architecture, we find multi-head attention layers that are followed by feedforward parts. These two main parts constitute the main body of a repeating block that we can then stack several times.\nWith the self-attention, we had tokens exchanging information. With the feedforward part, we let the tokens elaborate on this information.\nLet’s implement the feedforward bit of the network. It is a multi-layer perceptron with a single hidden layer.\n\nclass FeedForward(nn.Module):\n    def __init__(self, emb_sz, dropout=0.2):\n        super().__init__()\n        self.layers = nn.Sequential(nn.Linear(emb_sz, 4*emb_sz),\n                                    nn.GELU(),\n                                    nn.Linear(4*emb_sz, emb_sz),\n                                    nn.Dropout(dropout))\n\n    def forward(self, x):\n        return self.layers(x)",
    "crumbs": [
      "Course",
      "Generative models",
      "Language models"
    ]
  },
  {
    "objectID": "course/generative/language_models.html#decoder-block",
    "href": "course/generative/language_models.html#decoder-block",
    "title": "Language models",
    "section": "3.6 Decoder block",
    "text": "3.6 Decoder block\nWe grow our network by stacking decoder blocks. These have an initial self-attention part followed by a feedforward part. Concatenating blocks, we alternate between both, resulting in a combination of token communication and local computation.\nThere are two main key elements in the decoder block that we have not implemented yet. These are the residual paths and the layer normalization.\n\n\n\n\n\n\nNote\n\n\n\nBeware that in the GPT architecture, the normalization layers go before the self-attention and feedforward layers. This is an enhancement with respect to the original transformer architecture from Figure 2.\n\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, emb_sz, n_heads, seq_len):\n        super().__init__()\n        self.norm_1 = nn.LayerNorm(emb_sz)\n        head_sz = emb_sz // n_heads\n        self.heads = MultiHeadAttention(n_heads, emb_sz, head_sz, seq_len)\n        self.norm_2 = nn.LayerNorm(emb_sz)\n        self.ffw = FeedForward(emb_sz)\n        \n    def forward(self, x):\n        x = x + self.heads(self.norm_1(x))\n        x = x + self.ffw(self.norm_2(x))\n        return x\n\nNow we can rewrite our GPT models stacking a few blocks together.\n\nclass GPT(nn.Module):\n    \"GPT-like model\"\n    def __init__(self, vocab_sz, emb_sz, n_blocks, n_heads, seq_len):\n        super().__init__()\n        self.seq_len = seq_len\n        self.embedding_tkn = nn.Embedding(vocab_sz, emb_sz)\n        self.embedding_pos = nn.Embedding(seq_len, emb_sz)\n        self.blocks = nn.Sequential(*[DecoderBlock(emb_sz, n_heads, seq_len)\n                                      for _ in range(n_blocks)])\n        self.layer_norm = nn.LayerNorm(emb_sz)\n        self.linear = nn.Linear(emb_sz, vocab_sz)\n        \n    def forward(self, x):\n        emb_tkn = self.embedding_tkn(x)\n        emb_pos = self.embedding_pos(torch.arange(x.shape[1], device=x.device))\n        x = emb_tkn + emb_pos\n        x = self.blocks(x)\n        return self.linear(self.layer_norm(x))\n    \n    def generate(self, x, new_tkn):\n        for _ in range(new_tkn):\n            context = x[:, -self.seq_len:]\n            logits = self(context)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            new_tkn = torch.multinomial(probs, 1)\n            x = torch.cat((x, new_tkn), dim=1)\n        return x\n\n\ntorch.manual_seed(7)\nbatch_size, seq_len = 64, 60\nemb_sz, n_blocks, n_heads = 16, 3, 2\ngpt = GPT(vocab_size, emb_sz, n_blocks, n_heads, seq_len).to(device)\n\n\ngpt = train_model(5000, gpt, 2e-4, batch_size, seq_len)\n\nStep 0: train loss 2.5447, val loss 2.5251\nStep 200: train loss 2.3506, val loss 2.4647\nStep 400: train loss 2.3077, val loss 2.5329\nStep 600: train loss 2.2680, val loss 2.5167\nStep 800: train loss 2.2413, val loss 2.5738\nStep 1000: train loss 2.2003, val loss 2.5733\nStep 1200: train loss 2.1812, val loss 2.5701\nStep 1400: train loss 2.1544, val loss 2.4687\nStep 1600: train loss 2.1205, val loss 2.5322\nStep 1800: train loss 2.0167, val loss 2.2486\nStep 2000: train loss 1.8641, val loss 2.1164\nStep 2200: train loss 1.6780, val loss 1.7356\nStep 2400: train loss 1.4727, val loss 1.4668\nStep 2600: train loss 1.3467, val loss 1.2514\nStep 2800: train loss 1.2467, val loss 1.1161\nStep 3000: train loss 1.2174, val loss 1.0631\nStep 3200: train loss 1.1694, val loss 1.0006\nStep 3400: train loss 1.1376, val loss 0.9736\nStep 3600: train loss 1.1015, val loss 0.9086\nStep 3800: train loss 1.0782, val loss 0.9290\nStep 4000: train loss 1.0529, val loss 0.8534\nStep 4200: train loss 1.0305, val loss 0.8555\nStep 4400: train loss 0.9912, val loss 0.8366\nStep 4600: train loss 0.9993, val loss 0.8055\nStep 4800: train loss 0.9802, val loss 0.8178\nStep 4999: train loss 0.9659, val loss 0.7985\n\n\n\ncontext = torch.ones((1, 1), dtype=torch.long).to(device)\ngpt.eval()\ntkn.decode(gpt.generate(context, 20)[0].tolist())\n\n'050547561056756105675'\n\n\nTechnically, this generation is not wrong. Let’s provide it with a bit of extra context.\n\ncontext = torch.tensor([[3, 4, 4, 4, 0, 3, 4, 4, 5, 0]], dtype=torch.long).to(device)\ntkn.decode(gpt.generate(context, 40).tolist()[0])\n\n'2333,2334,3,236433,232433,232433,232477,236433,23,'\n\n\nIt’s not perfect, but we’re getting there.",
    "crumbs": [
      "Course",
      "Generative models",
      "Language models"
    ]
  },
  {
    "objectID": "course/generative/language_models.html#go-big-or-go-home",
    "href": "course/generative/language_models.html#go-big-or-go-home",
    "title": "Language models",
    "section": "3.7 Go big or go home",
    "text": "3.7 Go big or go home\nLet’s see how far we can push the model. As we grow the newtork, it is essential that we add some regularization, such as dropout.\n\ntorch.manual_seed(7)\nbatch_size, seq_len = 64, 60\nemb_sz, n_blocks, n_heads = 64, 4, 8\ngpt = GPT(vocab_size, emb_sz, n_blocks, n_heads, seq_len).to(device)\n\n\ngpt = train_model(10000, gpt, 1e-4, batch_size, seq_len)\n\nStep 0: train loss 2.5046, val loss 2.4860\nStep 200: train loss 2.1752, val loss 2.4264\nStep 400: train loss 2.0528, val loss 2.3285\nStep 600: train loss 1.9620, val loss 2.2053\nStep 800: train loss 1.6613, val loss 1.9140\nStep 1000: train loss 1.2678, val loss 1.3279\nStep 1200: train loss 1.0419, val loss 0.9195\nStep 1400: train loss 0.9266, val loss 0.7967\nStep 1600: train loss 0.8495, val loss 0.7225\nStep 1800: train loss 0.8086, val loss 0.6540\nStep 2000: train loss 0.7846, val loss 0.6082\nStep 2200: train loss 0.7407, val loss 0.5484\nStep 2400: train loss 0.7069, val loss 0.5276\nStep 2600: train loss 0.6790, val loss 0.5213\nStep 2800: train loss 0.6550, val loss 0.4604\nStep 3000: train loss 0.6206, val loss 0.4564\nStep 3200: train loss 0.6230, val loss 0.4313\nStep 3400: train loss 0.5819, val loss 0.4089\nStep 3600: train loss 0.5919, val loss 0.3990\nStep 3800: train loss 0.5422, val loss 0.3756\nStep 4000: train loss 0.5757, val loss 0.3539\nStep 4200: train loss 0.5493, val loss 0.3613\nStep 4400: train loss 0.5248, val loss 0.3461\nStep 4600: train loss 0.5180, val loss 0.3421\nStep 4800: train loss 0.5198, val loss 0.3184\nStep 5000: train loss 0.4806, val loss 0.3184\nStep 5200: train loss 0.4996, val loss 0.3353\nStep 5400: train loss 0.5133, val loss 0.3156\nStep 5600: train loss 0.4976, val loss 0.3038\nStep 5800: train loss 0.5066, val loss 0.3003\nStep 6000: train loss 0.4901, val loss 0.2954\nStep 6200: train loss 0.4883, val loss 0.2951\nStep 6400: train loss 0.4717, val loss 0.2944\nStep 6600: train loss 0.4752, val loss 0.2763\nStep 6800: train loss 0.4771, val loss 0.2869\nStep 7000: train loss 0.4656, val loss 0.2769\nStep 7200: train loss 0.4768, val loss 0.2656\nStep 7400: train loss 0.4678, val loss 0.2896\nStep 7600: train loss 0.4505, val loss 0.2976\nStep 7800: train loss 0.4683, val loss 0.2885\nStep 8000: train loss 0.4828, val loss 0.2718\nStep 8200: train loss 0.4449, val loss 0.2778\nStep 8400: train loss 0.4472, val loss 0.2672\nStep 8600: train loss 0.4702, val loss 0.2790\nStep 8800: train loss 0.4432, val loss 0.2778\nStep 9000: train loss 0.4936, val loss 0.2839\nStep 9200: train loss 0.4809, val loss 0.2610\nStep 9400: train loss 0.4890, val loss 0.2844\nStep 9600: train loss 0.4797, val loss 0.2951\nStep 9800: train loss 0.4548, val loss 0.2792\nStep 9999: train loss 0.4566, val loss 0.2632\n\n\n\ncontext = torch.ones((1, 1), dtype=torch.long).to(device)\ngpt.eval()\ntkn.decode(gpt.generate(context, 20)[0].tolist())\n\n'0,383421,383422,38342'\n\n\nThis model seems to know what it’s doing. Let’s try with a different context.\n\ncontext = torch.tensor([[5, 5, 0, 4, 9, 5, 6]], dtype=torch.long).to(device)\ntkn.decode(gpt.generate(context, 20)[0].tolist())\n\n'44,384545,384546,384547,384'\n\n\n\ncontext = torch.tensor([[0]], dtype=torch.long).to(device)\ntkn.decode(gpt.generate(context, 20)[0].tolist())\n\n',853803,853804,853805'\n\n\nPromising. Let’s see more!\n\ncontext = torch.tensor([[0]], dtype=torch.long).to(device)\ntkn.decode(gpt.generate(context, 80)[0].tolist())\n\n',686570,686571,686572,686574,686575,686576,686577,686577,686578,686579,686580,686'\n\n\nIn this sequence we see a couple of artifacts: it skips the 686573 and it repeats the 686577. However, it has learned how to change from 79 to 80. Let’s try again.\n\ncontext = torch.tensor([[0]], dtype=torch.long).to(device)\ntkn.decode(gpt.generate(context, 80)[0].tolist())\n\n',149120,149121,149122,149123,149124,149125,149126,149127,149128,149129,149130,149'\n\n\nFlawless. This model rocks!",
    "crumbs": [
      "Course",
      "Generative models",
      "Language models"
    ]
  },
  {
    "objectID": "course/applications/applications-index.html",
    "href": "course/applications/applications-index.html",
    "title": "Typical machine learning applications",
    "section": "",
    "text": "1 Recap\nSo far, we have visited the fundamentals of machine learning. We have tackled both regression and classification tasks building the algorithms from scratch. This has allowed us to introduce key machine learning concepts such as loss function, stochastic gradient descent, overfitting or regularization, which are transferrable to any machine learning task and architecture.\nBuilding a polynomial regression from scratch, we have gained intuition about what are the model parameters, how to compute their gradients and how to update them to obtain a better model. Then, with the logistic regression, we have learned the difference between a loss function and a metric. Finally, with the perceptron, we have mastered a fundamental building block of many complex machine learning architectures, as well as developed further intuition about classification tasks in multiple dimensions.\n\n\n2 Next steps\nFrom now on, we will take a more applied approach. We will use neural networks to tackle more challenging problems than what we have done so far. However, the basic principles remain the same.\nIn this lecture, we provide an overview of various prototypical machine learning tasks over different kinds of data. This will provide context for the upcomming lessons as well as (hopefully) some motivation! We will focus on three main types of data: images, text and structured data. These give raise to computer vision, natural language processing and tabular data tasks, respectively.\nSeat back and enjoy the ride!",
    "crumbs": [
      "Course",
      "Machine learning application overview"
    ]
  },
  {
    "objectID": "course/applications/applications-tabular.html",
    "href": "course/applications/applications-tabular.html",
    "title": "Typical tasks with structured data",
    "section": "",
    "text": "Note\n\n\n\nIn this notebook we use random forests, which is a machine learning technique built upon decision trees. Furthermore, we use the fastai (Howard and Gugger 2020) library to download the data for the different tasks and easily train our models.\n\n\n\n1 Introduction\nTabular data or structured data problems are pretty common in the field of machine learning. It is the prototypical problem in which each sample is described by a certain set of features and, thus, the dataset can be layed out in a table (hence the name). The goal, then, is to predict the value of one of the columns based on the rest. Up until quite recently, tabular data problems where generally addressed with classical models based on decision trees, be it ensembles or gradient boosted machines. However, deep learning has proven quite successful on these tasks in the past years.\nWithin this field, we encounter problems of all kinds, from telling flower types apart given a feature list, to assessing whether to give a loan to a bank client. Unfortunately, tabular data problems are much less nicer to show than computer vision tasks and so this part will be less flashy than the others. In order to illustrate the process, we will address a regression problem to infer the auction prices of bulldozers that was a kaggle competition. We will solve the same problem with random forests and neural networks in order to see what differences we find with them.\n\n\n\n\n\n\nNote\n\n\n\nWe take a regression example here, but tabular data problems can also be classification tasks and all the processes shown may be applied indistinctively.\n\n\nLet’s have a look at the data.\n\n\nCode\npath = URLs.path('bluebook')\n\ndf = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)\ndf_test = pd.read_csv(path/'Test.csv', low_memory=False)\ndf.head()\n\n\n\n\n\n\n\n\n\nSalesID\nSalePrice\nMachineID\nModelID\ndatasource\nauctioneerID\nYearMade\nMachineHoursCurrentMeter\nUsageBand\nsaledate\n...\nUndercarriage_Pad_Width\nStick_Length\nThumb\nPattern_Changer\nGrouser_Type\nBackhoe_Mounting\nBlade_Type\nTravel_Controls\nDifferential_Type\nSteering_Controls\n\n\n\n\n0\n1139246\n66000.0\n999089\n3157\n121\n3.0\n2004\n68.0\nLow\n11/16/2006 0:00\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nStandard\nConventional\n\n\n1\n1139248\n57000.0\n117657\n77\n121\n3.0\n1996\n4640.0\nLow\n3/26/2004 0:00\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nStandard\nConventional\n\n\n2\n1139249\n10000.0\n434808\n7009\n121\n3.0\n2001\n2838.0\nHigh\n2/26/2004 0:00\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n1139251\n38500.0\n1026470\n332\n121\n3.0\n2001\n3486.0\nHigh\n5/19/2011 0:00\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n1139253\n11000.0\n1057373\n17311\n121\n3.0\n2007\n722.0\nMedium\n7/23/2009 0:00\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 53 columns\n\n\n\nEach bulldozer is described by 53 features that constitute the columns of the dataset.\n\n\nCode\ndf.columns\n\n\nIndex(['SalesID', 'SalePrice', 'MachineID', 'ModelID', 'datasource',\n       'auctioneerID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',\n       'saledate', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc',\n       'fiModelSeries', 'fiModelDescriptor', 'ProductSize',\n       'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc',\n       'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control',\n       'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension',\n       'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics',\n       'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size',\n       'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow',\n       'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb',\n       'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type',\n       'Travel_Controls', 'Differential_Type', 'Steering_Controls'],\n      dtype='object')\n\n\nThe first thing to do is to identify our target value. In this case, it is the SalePrice column and, in fact, we want to predict the logarithm of the price, as stated in the competition. Then, these problems heavily rely on feature engineering, which consists on adding additional (smart) features that may be informative for the task. For instance, from a single date we can extract the day of the week, whether it was weekend or holidays, beginning or end of the month, etc. We could even figure out the weather if needed!\nCompetitions such as this one are won, in general, by those who can come up with the best relevant features for the task at hand.\n\n\nCode\ndep_var = 'SalePrice'\ndf[dep_var] = np.log(df[dep_var])\n\n\n\n\n\n\n\n\nExpand to learn about the training details\n\n\n\n\n\nGenerally, besides feature engineering, one of the key points in this kind of problems is properly handling categorical and numerical values as well as missing values. For instance, ProductSize is a categorical feature which takes values ‘Large’, ‘Large / Medium’, ‘Medium’, ‘Small’, ‘Mini’ and ‘Compact’. The model does not konw how to process these strings and so we convert them into numerical values assigning a number to each category. These numbers have essentially no meaning. However, given the nature of decision trees, it is convenient that ordinal categories, such as this one, are ordered so that increasing numbers, for example, represent increasing categorical sizes. Numerical values, in turn, should be properly normalized (for neural networks) and, finally, missing values are filled with the mean value of the column and a new column indicating wether it was filled or not is added.\nChoosing the right validation set is also extremely important. Given that this is a price forecasting task, we will take the latest sales within the training dataset to be our validation set.\n\n\n\n\n\nCode\nsizes = 'Large','Large / Medium','Medium','Small','Mini','Compact'\ndf['ProductSize'] = df['ProductSize'].astype('category')\ndf['ProductSize'].cat.set_categories(sizes, ordered=True, inplace=True)\n\ndf = add_datepart(df, 'saledate')\ndf_test = add_datepart(df_test, 'saledate')\n\n# Split train/validation\ncond = (df.saleYear&lt;2011) | (df.saleMonth&lt;10)\ntrain_idx, val_idx = np.where(cond)[0], np.where(~cond)[0]\nsplits = (list(train_idx), list(val_idx))\n\n# Handle continuous and categorical variables\nprocs = [Categorify, FillMissing]\ncont, cat = cont_cat_split(df, 1, dep_var=dep_var)\nto = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)\n\n\n\n\n2 Random forests\nRandom forests are the go-to technique to deal with tabular data. They are extremely powerful and extremely easy to set up and train thanks to libraries like sci-kit learn.\nLet’s fit a random forest regressor to the dataset and evaluate its performance. We evaluate the root mean square error (RMSE) of the price prediction on the validation set.\n\n\nCode\ndef rf(xs, y, n_estimators=40, max_samples=200_000, max_features=0.5,\n       min_samples_leaf=5, **kwargs):\n    \"Builds and fits a `RandomForestRegressor`.\"\n    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators,\n        max_samples=max_samples, max_features=max_features,\n        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)\n\ndef r_mse(pred, y):    return round(math.sqrt(((pred-y)**2).mean()), 6)\ndef m_rmse(m, xs, y): return r_mse(m.predict(xs), y)\n\n\nx, y = to.train.xs, to.train.y\nvalid_x, valid_y = to.valid.xs, to.valid.y\n\nm = rf(x, y)\nm_rmse(m, valid_x, valid_y)\n\n\n0.232313\n\n\nThe RMSE is 0.23 in the logarithm of the price. Let’s see how to improve on this. Random forests are quite easy to interpret and we can see, for instance, what are the most relevant features as well as those that are redundant.\nLet’s have a look at the feature importances of the most significant ones (top 30).\n\n\nCode\ndef plot_feature_importances(m, df, top=30):\n    fi = pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}).sort_values('imp', ascending=False)\n    fi[:top].plot('cols', 'imp', 'barh', figsize=(12, 8), legend=False)\n    return fi\n\nfi = plot_feature_importances(m, x);\n\n\n\n\n\n\n\n\n\nWe can see that some features are much more relevant than others. For instance, the year in which the bulldozer was made and its size seem to be the most significant aspects when it comes to determining its selling price, while things such as the transmission mechanism or the day it is being sold barely have an impact.\nWe will remove the least relevant features and retrain our model, leading to a simpler regressor. Therefore, if the performance is similar, it means that it will be able to generalize better. Evaluating the RMSE of the retrained model in the validation set we see that it is not only similar but, actually, a little bit better.\n\n\nCode\nto_keep = fi[fi.imp&gt;0.005].cols\nx_i, valid_x_i = x[to_keep], valid_x[to_keep]\nm = rf(x_i, y)\nm_rmse(m, valid_x_i, valid_y)\n\n\n0.231334\n\n\nBesides feature importance, we can also see which of these features are redundant or provide similar information. Removing redundant features makes our model simpler and more robust, meaning that it will generalize better to unseen data.\n\n\nCode\ndef cluster_columns(df, figsize=(10,6), font_size=12):\n    corr = np.round(scipy.stats.spearmanr(df).correlation, 4)\n    corr_condensed = hc.distance.squareform(1-corr)\n    z = hc.linkage(corr_condensed, method='average')\n    fig = plt.figure(figsize=figsize)\n    hc.dendrogram(z, labels=df.columns, orientation='left', leaf_font_size=font_size)\n    plt.show()\n    \ncluster_columns(x_i)\n\n\n\n\n\n\n\n\n\nThose features that are merged together at the rightmost part of the plot are the ones that are the most similar. For instance, ‘SaleYear’ and ‘SaleElapsed’ provide the same information but in different formats: the first states the year it was sold and the second tells us how many years have passed since it was sold. Just like with irrelevant features, we can remove some of these redudant ones and re-evaluate our model.\n\n\nCode\nto_drop = ['saleYear', 'ProductGroupDesc', 'fiBaseModel', 'Grouser_Tracks']\nx_ic, valid_x_ic = x_i.drop(to_drop, axis=1), valid_x_i.drop(to_drop, axis=1)\nm = rf(x_ic, y)\nm_rmse(m, valid_x_ic, valid_y)\n\n\n0.232922\n\n\nDropping the least informative features and some of the redundant ones, we have greatly simplified our model while keeping the same performance. This will allow the model to generalize much, much better. We could keep up with the model interpretation and feature engineering, but it is beyond the scope of this lesson. Some other features that we can drop are time-stamp variables, such as MachineID and SalesID, as well as some model identification ones. This is because, with the model in production, when we want to infer the price of a bulldozer that is currently being sold, the time-stamp-related features do not provide any significant information to the random forest, provided that it is completely unable to generalize beyond what it has seen during training. For an in-depth explanation, check the lesson 7 of fastai’s 2020 course.\nWe will proceed now to do the prediction by training a neural network.\n\n\n3 Neural networks\nWhile random forests do great work, they are completely unable to extrapolate to regions beyond the limits of the training data. It may not be the end of the world for some tasks, but it is definitely terrible for some others.\nHowever, as we have seen, those models can be extremely helpful to understand the data and get an idea of the most important features, as they are very easily interpretable. Therefore, we will combine both approaches and take advantage of the feature analysis that we have performed with the random forest. This way, we will get rid of some of the meaningless features straight away before training the network.\n\n\n\n\n\n\nExpand to learn about the training details\n\n\n\n\n\nThe neural network will have to deal with continuous and categorical variables in a completely different way. We will create an embdedding for each categorical variable, while the numerical ones are just input into a fully connected layer. Then, everything is brought together in a dense classifier at the end. Therefore, it is importnat that we split the variables into numerical and categorical and, in fact, categorical variables with high cardinality, like saleElapsed, may be dealt with as numerical ones to prevent massive embeddings.\n\n\n\nLet’s train!\n\n\nCode\nx_ic = x_ic.drop(['SalesID', 'MachineID', 'fiModelDescriptor'], axis=1)\ndf_nn = df[list(x_ic.columns) + [dep_var]] # Keep only useful features\n\ncont_nn, cat_nn = cont_cat_split(df_nn, max_card=9000, dep_var=dep_var)\n\ncont_nn.append('saleElapsed')\ncat_nn.remove('saleElapsed')\ndf_nn.saleElapsed.dtype = int\n\nprocs_nn = [Categorify, FillMissing, Normalize]\nto_nn = TabularPandas(df_nn, procs_nn, cat_nn, cont_nn, splits=splits, y_names=dep_var)\ndls = to_nn.dataloaders(1024)\n\nlearn = tabular_learner(dls, y_range=(8,12), layers=[500,250],\n                        n_out=1, loss_func=F.mse_loss)\n\nlearn.fit_one_cycle(5, 1e-2)\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.070896\n0.063198\n00:07\n\n\n1\n0.056112\n0.067255\n00:07\n\n\n2\n0.049322\n0.054010\n00:07\n\n\n3\n0.043438\n0.051197\n00:07\n\n\n4\n0.040356\n0.051439\n00:07\n\n\n\n\n\nIn order to compare the random forest with the neural network we have to check what the RMSE is.\n\n\nCode\npreds,targs = learn.get_preds()\nr_mse(preds,targs)\n\n\n\n\n\n0.226801\n\n\nThe neural network provides a much better result than the random forest predicting the sales price of bulldozers. This is, mainly, due to the hard limitation in extrapolation of random forests, which make them struggle in forecasting tasks such as this one where prices evolve through time and we have to make inferences in the future.\nThis has been only one example of how to apply machine learning to tabular data. As you can see, these kind of problems offer a much more engaging relationship in the feature engineering part, provided that we feed the data straight into the classifier.\n\n\n\n\n\nReferences\n\nHoward, Jeremy, and Sylvain Gugger. 2020. “Fastai: A Layered API for Deep Learning.” Information 11 (2). https://doi.org/10.3390/info11020108.",
    "crumbs": [
      "Course",
      "Machine learning application overview",
      "Typical tasks with structured data"
    ]
  },
  {
    "objectID": "course/introduction.html",
    "href": "course/introduction.html",
    "title": "Introduction to Machine Learning",
    "section": "",
    "text": "Note\n\n\n\nThis introduction has been adapted from Borja’s PhD thesis (Requena 2024).",
    "crumbs": [
      "Course",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "course/introduction.html#supervised-learning",
    "href": "course/introduction.html#supervised-learning",
    "title": "Introduction to Machine Learning",
    "section": "3.1 Supervised learning",
    "text": "3.1 Supervised learning\nSupervised learning refers to ML algorithms that learn from labeled data \\(\\mathcal{D} = \\{(\\mathbf{x}_i,\\mathbf{y}_i)\\}\\). There exist various approaches to supervised learning, spanning from statistical methods to classical ML and deep learning. In most cases, substantial amounts of data are required for the training process, which entails the accurate labeling of the data. This is usually considered one of the most significant drawbacks of supervised learning, as obtaining perfectly matched labels is not always feasible or may require extensive manual annotation by humans.",
    "crumbs": [
      "Course",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "course/introduction.html#unsupervised-learning",
    "href": "course/introduction.html#unsupervised-learning",
    "title": "Introduction to Machine Learning",
    "section": "3.2 Unsupervised learning",
    "text": "3.2 Unsupervised learning\nWhile labeled data may be scarce, we often have access to large amounts of raw unlabeled data \\(\\mathcal{D} = \\{\\mathbf{x}_i\\}\\). In this case, we can employ unsupervised learning, which refers to ML algorithms that learn from unlabeled data. Unsupervised learning can either be used for preliminary pre-processing steps, such as dimensionality reduction, or for representation learning, as in data clustering. Furthermore, it can be used to learn the underlying probability distribution of the data and generate entirely new examples.",
    "crumbs": [
      "Course",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "course/introduction.html#reinforcement-learning",
    "href": "course/introduction.html#reinforcement-learning",
    "title": "Introduction to Machine Learning",
    "section": "3.3 Reinforcement learning",
    "text": "3.3 Reinforcement learning\nIn contrast to the two previous types of learning, some ML do not rely on a fixed data set \\(\\mathcal{D}\\). For instance, in reinforcement learning, we usually do not even have a data set at all from the beginning. Instead, the learning algorithm interacts with an environment in a feedback loop to accomplish a given task. The data set is gradually shaped as the learning system collects experiences derived from these interactions. Different reinforcement learning algorithms manage the collected data differently, although it is always leveraged to achieve the objective task.",
    "crumbs": [
      "Course",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "lib_nbs/losses.html",
    "href": "lib_nbs/losses.html",
    "title": "Loss functions and gradients",
    "section": "",
    "text": "source\n\n\n\n MSE (x:numpy.ndarray, y:numpy.ndarray, fun:&lt;built-infunctioncallable&gt;,\n      params=None)\n\nGiven the data \\(x\\) and \\(y\\), this function computes the mean square error between \\(y\\) and \\(y'=f(x)\\)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nndarray\n\nx data of N elements\n\n\ny\nndarray\n\ny data of N elements\n\n\nfun\ncallable\n\nfunction \\(y=f(x)\\)\n\n\nparams\nNoneType\nNone\nParameters of the function in the form of a dictionary\n\n\nReturns\nfloat\n\n\n\n\n\nThe mean square error is defined as \\[ MSE(y,y')=\\frac{1}{N}\\sum_{i=1}^{N}(y'_i-y_i)^2.\\]\n\nsource\n\n\n\n\n grad_MSE_lr (x:numpy.ndarray, y:numpy.ndarray, params:dict)\n\nComputes the gradient of the mean square error loss function with respect to \\(a\\) and \\(b\\) and returns np.array([\\(\\partial_a\\) MSE,\\(\\partial_b\\) MSE])\n\n\n\n\nType\nDetails\n\n\n\n\nx\nndarray\nx data of N elements\n\n\ny\nndarray\ny data of N elements\n\n\nparams\ndict\nParameters of the function\n\n\nReturns\nndarray\ngradients\n\n\n\n\nsource\n\n\n\n\n grad_MSE_pr (x:numpy.ndarray, y:numpy.ndarray, params:dict)\n\nComputes the gradient of the mean square error loss function with respect to \\(a\\) and \\(b\\) and returns np.array([\\(\\partial_a\\) MSE,\\(\\partial_b\\) MSE])\n\n\n\n\nType\nDetails\n\n\n\n\nx\nndarray\nx data of N elements\n\n\ny\nndarray\ny data of N elements\n\n\nparams\ndict\nparameters of the function\n\n\nReturns\nndarray\ngradients\n\n\n\n\nsource\n\n\n\n\n BCE (x:numpy.ndarray, y:numpy.ndarray, fun:&lt;built-infunctioncallable&gt;,\n      params:dict)\n\nGiven the data \\(x\\) and \\(y\\), this function computes the mean binary cross entropy \\(y\\) and \\(y'=f(x)\\)\n\n\n\n\nType\nDetails\n\n\n\n\nx\nndarray\nx data of N elements\n\n\ny\nndarray\ny data of N elements\n\n\nfun\ncallable\nfunction \\(y=f(x)\\)\n\n\nparams\ndict\nParameters of the function\n\n\nReturns\nfloat\n\n\n\n\n\nsource\n\n\n\n\n grad_BCE (x:numpy.ndarray, y:numpy.ndarray, params:dict)\n\nComputes the gradient of the binary cross entropy loss function with respect to \\(a\\) and \\(b\\) and returns np.array([\\(\\partial_a\\) BCE,\\(\\partial_b\\) BCE])\n\n\n\n\nType\nDetails\n\n\n\n\nx\nndarray\nx data of N elements\n\n\ny\nndarray\ny data of N elements\n\n\nparams\ndict\nParameters of the function\n\n\nReturns\nndarray\ngradients\n\n\n\n\nsource\n\n\n\n\n L_per (x:numpy.ndarray, y:numpy.ndarray, fun:&lt;built-infunctioncallable&gt;,\n        params:dict)\n\nGiven the data \\(x\\) and \\(y\\), this function computes the Loss of the perceptron algorithm\n\n\n\n\nType\nDetails\n\n\n\n\nx\nndarray\nx data of N elements\n\n\ny\nndarray\ny data of N elements\n\n\nfun\ncallable\nfunction \\(y=f(x)\\)\n\n\nparams\ndict\nParameters of the function\n\n\nReturns\nfloat\n\n\n\n\n\nsource\n\n\n\n\n grad_per (x:numpy.ndarray, y:numpy.ndarray, params:dict)\n\nComputes the gradient of the perceptron loss function and returns np.array(grad_w)\n\n\n\n\nType\nDetails\n\n\n\n\nx\nndarray\nx data of N elements\n\n\ny\nndarray\ny data of N elements\n\n\nparams\ndict\nParameters of the function\n\n\nReturns\nndarray\ngradients",
    "crumbs": [
      "Library",
      "Loss functions and gradients"
    ]
  },
  {
    "objectID": "lib_nbs/losses.html#loss-function",
    "href": "lib_nbs/losses.html#loss-function",
    "title": "Loss functions and gradients",
    "section": "",
    "text": "source\n\n\n\n MSE (x:numpy.ndarray, y:numpy.ndarray, fun:&lt;built-infunctioncallable&gt;,\n      params=None)\n\nGiven the data \\(x\\) and \\(y\\), this function computes the mean square error between \\(y\\) and \\(y'=f(x)\\)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nndarray\n\nx data of N elements\n\n\ny\nndarray\n\ny data of N elements\n\n\nfun\ncallable\n\nfunction \\(y=f(x)\\)\n\n\nparams\nNoneType\nNone\nParameters of the function in the form of a dictionary\n\n\nReturns\nfloat\n\n\n\n\n\nThe mean square error is defined as \\[ MSE(y,y')=\\frac{1}{N}\\sum_{i=1}^{N}(y'_i-y_i)^2.\\]\n\nsource\n\n\n\n\n grad_MSE_lr (x:numpy.ndarray, y:numpy.ndarray, params:dict)\n\nComputes the gradient of the mean square error loss function with respect to \\(a\\) and \\(b\\) and returns np.array([\\(\\partial_a\\) MSE,\\(\\partial_b\\) MSE])\n\n\n\n\nType\nDetails\n\n\n\n\nx\nndarray\nx data of N elements\n\n\ny\nndarray\ny data of N elements\n\n\nparams\ndict\nParameters of the function\n\n\nReturns\nndarray\ngradients\n\n\n\n\nsource\n\n\n\n\n grad_MSE_pr (x:numpy.ndarray, y:numpy.ndarray, params:dict)\n\nComputes the gradient of the mean square error loss function with respect to \\(a\\) and \\(b\\) and returns np.array([\\(\\partial_a\\) MSE,\\(\\partial_b\\) MSE])\n\n\n\n\nType\nDetails\n\n\n\n\nx\nndarray\nx data of N elements\n\n\ny\nndarray\ny data of N elements\n\n\nparams\ndict\nparameters of the function\n\n\nReturns\nndarray\ngradients\n\n\n\n\nsource\n\n\n\n\n BCE (x:numpy.ndarray, y:numpy.ndarray, fun:&lt;built-infunctioncallable&gt;,\n      params:dict)\n\nGiven the data \\(x\\) and \\(y\\), this function computes the mean binary cross entropy \\(y\\) and \\(y'=f(x)\\)\n\n\n\n\nType\nDetails\n\n\n\n\nx\nndarray\nx data of N elements\n\n\ny\nndarray\ny data of N elements\n\n\nfun\ncallable\nfunction \\(y=f(x)\\)\n\n\nparams\ndict\nParameters of the function\n\n\nReturns\nfloat\n\n\n\n\n\nsource\n\n\n\n\n grad_BCE (x:numpy.ndarray, y:numpy.ndarray, params:dict)\n\nComputes the gradient of the binary cross entropy loss function with respect to \\(a\\) and \\(b\\) and returns np.array([\\(\\partial_a\\) BCE,\\(\\partial_b\\) BCE])\n\n\n\n\nType\nDetails\n\n\n\n\nx\nndarray\nx data of N elements\n\n\ny\nndarray\ny data of N elements\n\n\nparams\ndict\nParameters of the function\n\n\nReturns\nndarray\ngradients\n\n\n\n\nsource\n\n\n\n\n L_per (x:numpy.ndarray, y:numpy.ndarray, fun:&lt;built-infunctioncallable&gt;,\n        params:dict)\n\nGiven the data \\(x\\) and \\(y\\), this function computes the Loss of the perceptron algorithm\n\n\n\n\nType\nDetails\n\n\n\n\nx\nndarray\nx data of N elements\n\n\ny\nndarray\ny data of N elements\n\n\nfun\ncallable\nfunction \\(y=f(x)\\)\n\n\nparams\ndict\nParameters of the function\n\n\nReturns\nfloat\n\n\n\n\n\nsource\n\n\n\n\n grad_per (x:numpy.ndarray, y:numpy.ndarray, params:dict)\n\nComputes the gradient of the perceptron loss function and returns np.array(grad_w)\n\n\n\n\nType\nDetails\n\n\n\n\nx\nndarray\nx data of N elements\n\n\ny\nndarray\ny data of N elements\n\n\nparams\ndict\nParameters of the function\n\n\nReturns\nndarray\ngradients",
    "crumbs": [
      "Library",
      "Loss functions and gradients"
    ]
  },
  {
    "objectID": "lib_nbs/data_gen.html",
    "href": "lib_nbs/data_gen.html",
    "title": "Data Generation",
    "section": "",
    "text": "source\n\nline\n\n line (x:numpy.ndarray, a=1.0, b=0.5, interval=[-10.0, 10.0], noise=[0,\n       1e-05], nsamples=100)\n\nCreate a dataset of nsamples in the interval following the linear regression \\(y=a x+b\\).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nndarray\n\n\n\n\na\nfloat\n1.0\nSlope\n\n\nb\nfloat\n0.5\nIntercept\n\n\ninterval\nlist\n[-10.0, 10.0]\nInterval for x.\n\n\nnoise\nlist\n[0, 1e-05]\nNoise [\\(\\mu\\),\\(\\sigma\\)] with mean \\(\\mu\\) and standard deviation \\(\\sigma\\)\n\n\nnsamples\nint\n100\nNumber of samples\n\n\nReturns\nndarray\n\nthe array \\(y=ax+b\\)\n\n\n\n\nsource\n\n\nnoisy_line\n\n noisy_line (a=1.0, b=0.5, interval=[-10.0, 10.0], noise=[0, 1e-05],\n             nsamples=100)\n\nCreate a dataset of nsamples in the interval following the linear regression \\(y=a x+b\\) and adds a gaussian noise on y.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\na\nfloat\n1.0\nSlope\n\n\nb\nfloat\n0.5\nIntercept\n\n\ninterval\nlist\n[-10.0, 10.0]\nInterval for x.\n\n\nnoise\nlist\n[0, 1e-05]\nNoise [\\(\\mu\\),\\(\\sigma\\)] with mean \\(\\mu\\) and standard deviation \\(\\sigma\\)\n\n\nnsamples\nint\n100\nNumber of samples\n\n\nReturns\ntuple\n\n- a random x vector in the interval of size nsamples- the noisy vector following \\(y= ax+b\\)\n\n\n\n\nsource\n\n\ncurve\n\n curve (x, coeffs)\n\nCreate a vector following the polynomial curve \\(y=w^Tx\\), where \\(x=(x^0...x^d)\\) and \\(x=(w^0...w^d)\\).\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nx\n\ndataset to be imputed\n\n\ncoeffs\n\narray of the weights of the polynomial of degree d-1, where d is the size of the array.\n\n\nReturns\nndarray\nthe vector \\(y=w \\cdot x\\)\n\n\n\n\nsource\n\n\nnoisy_curve\n\n noisy_curve (coeffs, x=None, interval=[-2, 2], noise=None, nsamples=100)\n\nCreate a dataset of nsamples in the interval following the polynomial curve \\(y=w^Tx\\), where \\(x=(x^0...x^d)\\) and \\(x=(w^0...w^d)\\) and adds a gaussian noise on y.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncoeffs\n\n\narray of the weights of the polynomial of degree d-1, where d is the size of the array.\n\n\nx\nNoneType\nNone\ndataset to be imputed. if x is None, then the dataset is constructed with nsamples from a uniform distribution\n\n\ninterval\nlist\n[-2, 2]\ninterval for the sampling of x\n\n\nnoise\nNoneType\nNone\ntuple contining \\(\\mu\\) and \\(\\sigma\\). If noise is None, then there is no noise\n\n\nnsamples\nint\n100\nnumber of samples for x\n\n\nReturns\ntuple\n\n- a random x vector in the interval of size nsamples- the noisy vector following \\(y=w \\cdot x\\)",
    "crumbs": [
      "Library",
      "Data Generation"
    ]
  }
]